{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdfHuXemyCjA"
   },
   "source": [
    "# Data Analysis\n",
    "### 2025\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "1. Zavtra\n",
    "data_eastview/zavtra_soros_final_01102025.csv\n",
    "Link: https://drive.google.com/file/d/1wPyHPmYYfPkKeXa7eS6D5fnp0snbjgwd/view?usp=drive_link\n",
    "\n",
    "2. Tsargrad\n",
    "data_eastview/\n",
    "tsargrad_soros_translated_01102025_final.csv\n",
    "Link: https://drive.google.com/file/d/1KWhHCw3KUyTVJ5MivGWSDOcyzcWDXHuo/view?usp=drive_link\n",
    "\n",
    "3. Eastlink\n",
    "data_eastview/\n",
    "eastlink_soros_final_01102025.csv\n",
    "Link: https://drive.google.com/file/d/1coS3_R5uHDVHlJbs-LdD3DnG9h7O5bjW/view?usp=drive_link\n",
    "Note: Eastlink contains 11 publications.\n",
    "\n",
    "4. RT data\n",
    "`data/final_dataset_updated.csv `\n",
    "Link: https://drive.google.com/file/d/1-DJTaPm1QJ-orCQ5RPRfm9sJ1wFfMVFq/view?usp=drive_link\n",
    "Note: This is from previous work\n",
    "Renamed here as `final_dataset_updated_er.csv`\n",
    "```\n",
    "\n",
    "\n",
    "### Publications (3. Eastlink)\n",
    "data_eastview/\n",
    "eastlink_soros_final_01102025.csv\n",
    "```\n",
    "Publications = [\n",
    "    \"Kommersant\",\n",
    "    \"Literaturnaia gazeta\",\n",
    "    \"Nezavisimaia gazeta\",\n",
    "    \"Novaia gazeta\",\n",
    "    \"Pravda\",\n",
    "    \"Slovo\",\n",
    "    \"Sovetskaia Rossiia\",\n",
    "    \"Trud\",\n",
    "    \"Vedomosti\",\n",
    "    \"Время MH\",\n",
    "    \"Общая газета\"\n",
    "]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sentiment Analysis\n",
    "Ner Models: en_core_web_sm (SpaCy) and dslim/bert-large-NER (Transformers)\n",
    "https://github.com/fhamborg/NewsMTSC/blob/main/READMEpypi.md\n",
    "https://aclanthology.org/2021.eacl-main.142.pdf\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgBbEiACAXy_"
   },
   "source": [
    "# Set Up (1) Jupyter Notebook or (2) Google Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1759290480051,
     "user": {
      "displayName": "Te' Claire",
      "userId": "06350349370332120081"
     },
     "user_tz": -600
    },
    "id": "iAc4fzEDhJoj"
   },
   "outputs": [],
   "source": [
    "# Import libraries for all (1) and (2)\n",
    "import csv\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas.api.types as ptypes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas.api.types as ptypes\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not infer format, .*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Set Up (1) Jupyter Notebook (uncomment to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q scikit-learn\n",
    "\n",
    "import sklearn\n",
    "import sys\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"python exe:\", sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Import data (locally)\n",
    "tsargrad_data = pd.read_csv(\"tsargrad_soros_final_01102025.csv\", encoding=\"utf-8-sig\")\n",
    "zavtra_data   = pd.read_csv(\"zavtra_soros_final_01102025.csv\",   encoding=\"utf-8-sig\")\n",
    "eastview_data = pd.read_csv(\"eastlink_soros_final_01102025.csv\", encoding=\"utf-8-sig\")\n",
    "eir_data = pd.read_csv(\"combined_eir_output_data.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "print(tsargrad_data.shape, zavtra_data.shape, eastview_data.shape, eir_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 'WARNING' Statment at start of `eastview_data`\n",
    "1. Check lengths\n",
    "2. View ArticleTextEnglish\n",
    "3. Check rows before and after celan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = eastview_data[\"ArticleTextEnglish\"].astype(\"string\")\n",
    "\n",
    "print(\"rows:\", len(s))\n",
    "print(\"non-empty:\", s.fillna(\"\").str.strip().ne(\"\").sum())\n",
    "print(\"median length:\", int(s.fillna(\"\").str.len().median()))\n",
    "print(\"max length:\", int(s.fillna(\"\").str.len().max()))\n",
    "\n",
    "# how many look like boilerplate-only (very short)?\n",
    "print(\"very short (<120 chars):\", int(s.fillna(\"\").str.len().lt(120).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_meta = [c for c in [\"ArticleID\",\"ArticleTitle\",\"Publication\",\"Year\",\"Month\",\"ArticleLink\"]\n",
    "             if c in eastview_data.columns]\n",
    "\n",
    "def sample_preview(df, n=5, seed=42, text_col=\"ArticleTextEnglish\", head_chars=1000):\n",
    "    sub = df.sample(n=n, random_state=seed)\n",
    "    for i, (_, r) in enumerate(sub.iterrows(), 1):\n",
    "        print(f\"\\n#{i} — index={r.name}\")\n",
    "        print(\" | \".join([f\"{c}={r.get(c, '')}\" for c in cols_meta]))\n",
    "        txt = str(r[text_col] or \"\")\n",
    "        print(\"-\" * 60)\n",
    "        print(txt[:head_chars].strip())\n",
    "        if len(txt) > head_chars:\n",
    "            print(\"… [truncated]\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "sample_preview(eastview_data, n=5, seed=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 1) Preferred: from ATTENTION…COPYRIGHT up to *and including* \"Welcome To East View\"\n",
    "EV_PREFIX_TO_WELCOME_RE = re.compile(\n",
    "    r\"\"\"^\\s*ATTENTION:.*?COPYRIGHT      # ATTENTION … COPYRIGHT …\n",
    "        [\\s\\S]*?                        # anything (non-greedy)\n",
    "        Welcome\\W+To\\W+East\\W*View\\s+   # … up to & including 'Welcome To East View'\n",
    "    \"\"\",\n",
    "    flags=re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "# 2) Smart alternative endings when 'Welcome...' is absent\n",
    "EV_PREFIX_SMART_RE = re.compile(\n",
    "    r\"\"\"^\\s*ATTENTION:.*?COPYRIGHT.*?        # ATTENTION … COPYRIGHT …\n",
    "        (?:East\\W*View.*?\\.\\s+               # … up to a sentence ending that mentions East View\n",
    "         |https?://[^\\s)]+[^.]*\\.\\s+         # … or a URL sentence end\n",
    "        )\n",
    "    \"\"\",\n",
    "    flags=re.IGNORECASE | re.DOTALL | re.VERBOSE,\n",
    ")\n",
    "\n",
    "# 3) Fallback: to the first period that follows COPYRIGHT\n",
    "EV_PREFIX_FALLBACK_RE = re.compile(\n",
    "    r\"\"\"^\\s*ATTENTION:.*?COPYRIGHT.*?\\.\\s+\"\"\",\n",
    "    flags=re.IGNORECASE | re.DOTALL,\n",
    ")\n",
    "\n",
    "def strip_ev_prefix_inline(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # normalize odd spaces/newlines\n",
    "    t = (text.replace(\"\\u00A0\",\" \").replace(\"\\u2007\",\" \").replace(\"\\u202F\",\" \")\n",
    "              .replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\"))\n",
    "    head = t[:1000].lower()\n",
    "    if \"attention:\" in head and \"copyright\" in head:\n",
    "        # Try Welcome… inclusive cut first\n",
    "        t2 = EV_PREFIX_TO_WELCOME_RE.sub(\"\", t, count=1)\n",
    "        if t2 == t:\n",
    "            # Then try other smart endings\n",
    "            t2 = EV_PREFIX_SMART_RE.sub(\"\", t, count=1)\n",
    "            if t2 == t:\n",
    "                # Last resort: first full stop after COPYRIGHT\n",
    "                t2 = EV_PREFIX_FALLBACK_RE.sub(\"\", t, count=1)\n",
    "        return t2.lstrip()\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check before and run function clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = eastview_data[\"ArticleTextEnglish\"].astype(str)\n",
    "starts_with_disclaimer = s.str.match(r\"^\\s*ATTENTION:.*COPYRIGHT\", case=False)\n",
    "print(\"Rows starting with EV preface:\", int(starts_with_disclaimer.sum()))\n",
    "eastview_data[\"ArticleTextEnglish\"] = s.map(strip_ev_prefix_inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check after function clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = eastview_data[\"ArticleTextEnglish\"].astype(str)\n",
    "still_has_prefix = s2.str.match(r\"^\\s*ATTENTION:.*COPYRIGHT\", case=False)\n",
    "print(\"Rows still starting with preface after clean:\", int(still_has_prefix.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check same indices before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_idx(idx, col=\"ArticleTextEnglish\", head=500):\n",
    "    r = eastview_data.loc[idx]\n",
    "    meta = \" | \".join([f\"{k}={r.get(k,'')}\" for k in [\"ArticleID\",\"ArticleTitle\",\"Publication\",\"Year\",\"Month\",\"ArticleLink\"] if k in eastview_data.columns])\n",
    "    print(f\"\\nindex={idx} :: {meta}\\n\" + \"-\"*70)\n",
    "    print(str(r[col])[:head])\n",
    "    if len(str(r[col])) > head:\n",
    "        print(\"… [truncated]\")\n",
    "\n",
    "for i in [1930, 2360, 1225, 640, 1347]:\n",
    "    preview_idx(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_idx = s2[still_has_prefix].head(5).index.tolist()\n",
    "for i in problem_idx:\n",
    "    print(\"\\n--- stubborn start @ index\", i, \"---\")\n",
    "    print(s2.loc[i][:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Set Up (2) Google Notebook (uncomment to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21871,
     "status": "ok",
     "timestamp": 1759290501940,
     "user": {
      "displayName": "Te' Claire",
      "userId": "06350349370332120081"
     },
     "user_tz": -600
    },
    "id": "YQW9oWvhTAAb",
    "outputId": "a992f012-03e4-4199-e969-e239ca16c7d9"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "# from google.colab import files\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# from psutil import virtual_memory\n",
    "# ram_gb = virtual_memory().total / 1e9\n",
    "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "# if ram_gb < 20:\n",
    "#   print('Not using a high-RAM runtime')\n",
    "# else:\n",
    "#   print('You are using a high-RAM runtime!')\n",
    "\n",
    "\n",
    "# SVD imports for google Collab\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Google drive load data\n",
    "# # Load EIR Data (eir_data)\n",
    "# eir_path = '/content/drive/MyDrive/george_soros/notebooks/data/combined_eir_output_data.csv'\n",
    "# eir_data = pd.read_csv(eir_path)\n",
    "# # eir_data.head()\n",
    "\n",
    "# # ------------\n",
    "# # Load Eastview\n",
    "# eastview_path = '/content/drive/MyDrive/george_soros/notebooks/data_eastview/eastlink_soros_final_01102025.csv'\n",
    "# eastview_data = pd.read_csv(eastview_path)\n",
    "# # eastview_data.head()\n",
    "\n",
    "# # ------------\n",
    "# # Load Zavtra\n",
    "# # data_eastview/zavtra_soros_final_01102025.csv\n",
    "# zavtra_path = '/content/drive/MyDrive/george_soros/notebooks/data_eastview/zavtra_soros_final_01102025.csv'\n",
    "# zavtra_data = pd.read_csv(zavtra_path)\n",
    "# # zavtra_data.head()\n",
    "\n",
    "# # ------------\n",
    "# # Load Tsargrad\n",
    "# tsargrad_path = '/content/drive/MyDrive/george_soros/notebooks/data_eastview/tsargrad_soros_translated_01102025_final.csv'\n",
    "# tsargrad_data = pd.read_csv(tsargrad_path)\n",
    "# # tsargrad_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- END OF SET UP --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RT data\n",
    "# Set nullable string dtype\n",
    "STR_COLS = [\"content\", \"soros_sentence\", \"uid\", \"source\"]\n",
    "dtype_map = {c: \"string\" for c in STR_COLS}\n",
    "\n",
    "combined_df = pd.read_csv(\n",
    "    \"final_dataset_updated_rt.csv\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    dtype=dtype_map,   # avoid mixed types in these\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Coerce numeric columns (anything non-numeric becomes NaN)\n",
    "for c in [\"score_soros_sentence\", \"soros\", \"year\"]:\n",
    "    if c in combined_df.columns:\n",
    "        combined_df[c] = pd.to_numeric(combined_df[c], errors=\"coerce\")\n",
    "\n",
    "# Parse date if present (looks like YYYYMMDD as int/string)\n",
    "if \"date\" in combined_df.columns:\n",
    "    # keep original in case you need it\n",
    "    combined_df[\"_date_raw\"] = combined_df[\"date\"].astype(\"string\")\n",
    "    combined_df[\"date\"] = pd.to_datetime(combined_df[\"_date_raw\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# Normalize source to lowercase for reliable filtering\n",
    "if \"source\" in combined_df.columns:\n",
    "    combined_df[\"source\"] = combined_df[\"source\"].str.strip().str.lower()\n",
    "\n",
    "# keep only rows with a positive score\n",
    "mask_scored = combined_df[\"score_soros_sentence\"].notna() & (combined_df[\"score_soros_sentence\"] > 0)\n",
    "\n",
    "# source == 'rt' (case-insensitive because we normalized)\n",
    "mask_rt = combined_df[\"source\"].eq(\"rt\")\n",
    "\n",
    "rt_data = combined_df.loc[mask_scored & mask_rt].copy()\n",
    "\n",
    "print(rt_data.shape)\n",
    "display(rt_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITICAL_KEYS = [\"ArticleID\", \"ArticleLink\", \"uid\"]\n",
    "TEXT_COL_HINTS = [\"ArticleTextEnglish\", \"article_text\", \"translated_article_excerpt\", \"content\", \"soros_sentence\"]\n",
    "DATE_COL_HINTS = [\"PublicationDate\", \"PublishedDate\", \"publication_date\", \"translated_date\", \"Date\", \"date\"]\n",
    "\n",
    "def first_present(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def parse_best_date(df):\n",
    "    \"\"\"Try to parse a reasonable date column to datetime; return (colname, parsed_series).\"\"\"\n",
    "    # prioritize our hints\n",
    "    for col in [c for c in DATE_COL_HINTS if c in df.columns]:\n",
    "        dt = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        if dt.notna().any(): \n",
    "            return col, dt\n",
    "    # otherwise try any object-like column\n",
    "    for col in df.select_dtypes(include=[\"object\", \"string\"]).columns:\n",
    "        dt = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        if dt.notna().any():\n",
    "            return col, dt\n",
    "    return None, pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "def audit(df: pd.DataFrame, name: str):\n",
    "    print(f\"\\n=== {name}: shape {df.shape} ===\")\n",
    "    display(df.head(3))\n",
    "\n",
    "    # dtypes (compact)\n",
    "    dtypes = df.dtypes.to_frame(\"dtype\")\n",
    "    display(dtypes.T)\n",
    "\n",
    "    # missingness\n",
    "    miss = df.isna().sum().to_frame(\"missing\")\n",
    "    miss[\"missing_pct\"] = (miss[\"missing\"] / len(df) * 100).round(2)\n",
    "    miss_sorted = miss.sort_values(\"missing_pct\", ascending=False)\n",
    "    print(\"Missing values (top 10):\")\n",
    "    display(miss_sorted.head(10))\n",
    "\n",
    "    # uniques\n",
    "    nunique = df.nunique(dropna=True).sort_values(ascending=False)\n",
    "    print(\"Top uniques (top 10):\")\n",
    "    display(nunique.head(10).to_frame(\"nunique\"))\n",
    "\n",
    "    # duplicates on common keys\n",
    "    for key in CRITICAL_KEYS:\n",
    "        if key in df.columns:\n",
    "            dup = df.duplicated(subset=[key]).sum()\n",
    "            if dup:\n",
    "                print(f\"WARNING: {dup} duplicate rows on key '{key}'\")\n",
    "    \n",
    "    # text length quick peek\n",
    "    txt_col = first_present(df, TEXT_COL_HINTS)\n",
    "    if txt_col:\n",
    "        lens = df[txt_col].dropna().astype(str).str.len()\n",
    "        print(f\"Text column: '{txt_col}' | non-null: {lens.size} | len[min/median/max]:\",\n",
    "              int(lens.min()) if not lens.empty else None,\n",
    "              int(lens.median()) if not lens.empty else None,\n",
    "              int(lens.max()) if not lens.empty else None)\n",
    "\n",
    "    # numeric range checks commonly used here\n",
    "    for col in [\"probability\", \"score_soros_sentence\", \"soros_count\", \"year\"]:\n",
    "        if col in df.columns:\n",
    "            coln = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            valid = coln.notna()\n",
    "            if valid.any():\n",
    "                print(f\"Numeric '{col}': min={coln[valid].min()}, median={coln[valid].median()}, max={coln[valid].max()}\")\n",
    "\n",
    "    # date coverage\n",
    "    date_col, dt = parse_best_date(df)\n",
    "    if date_col:\n",
    "        dt_nm = dt.dropna()\n",
    "        if not dt_nm.empty:\n",
    "            print(f\"Date column used: '{date_col}' | range: {dt_nm.min().date()} → {dt_nm.max().date()}\")\n",
    "        else:\n",
    "            print(f\"Date column candidate '{date_col}' could not be parsed to any non-null datetimes.\")\n",
    "    else:\n",
    "        print(\"No parseable date column found.\")\n",
    "\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit(tsargrad_data, \"Tsargrad\")\n",
    "audit(zavtra_data,   \"Zavtra\")\n",
    "audit(eastview_data, \"Eastlink/EastView\")\n",
    "audit(eir_data,      \"EIR\")\n",
    "audit(rt_data,       \"RT (filtered)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review notes\n",
    "- Zavtra dates from PublicationDate / ArticleDate, which include Russian month names (e.g., “29 марта 2025”) and/or English (“23 July 2024”).\n",
    "- EastView dates Year + Month (or ExtractedDate) instead of “Date collected”.\n",
    "- EIR dates 20220701 must be parsed with a fixed format (%Y%m%d)—generic parsing turns it into 1970-01-01.\n",
    "- Tsargrad: translated_date already parses fine.\n",
    "- Drop NaN columns (in “Unnamed” columns), standardise a year_month columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, df in [(\"Tsargrad\", tsargrad_data), (\"Zavtra\", zavtra_data), (\"EastView\", eastview_data), (\"EIR\", eir_data)]:\n",
    "#     s = df[\"year_month\"].value_counts().sort_index()\n",
    "#     print(f\"\\n{name} months: {len(s)} distinct\")\n",
    "#     display(s.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "tsargrad_data.to_csv(\"tsargrad_soros_final_01102025_clean.csv\", index=False)\n",
    "zavtra_data.to_csv(\"zavtra_soros_final_01102025_clean.csv\", index=False)\n",
    "eastview_data.to_csv(\"eastlink_soros_final_01102025_clean.csv\", index=False)\n",
    "eir_data.to_csv(\"eir_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zavtra_data.head()\n",
    "# zavtra_data.columns.tolist()\n",
    "# zavtra_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsargrad_data.head()\n",
    "# tsargrad_data.columns.tolist()\n",
    "# tsargrad_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eastview_data.head()\n",
    "# eastview_data.columns.tolist()\n",
    "# eastview_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eir_data.head()\n",
    "# eir_data.columns.tolist()\n",
    "# eir_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_data.head()\n",
    "# rt_data.columns.tolist()\n",
    "# rt_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0QX7rJ2hf8m"
   },
   "source": [
    "## Set Colors & publication maps\n",
    "- https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3\n",
    "- https://www.simplifiedsciencepublishing.com/resources/best-color-palettes-for-scientific-figures-and-data-visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1759290501969,
     "user": {
      "displayName": "Te' Claire",
      "userId": "06350349370332120081"
     },
     "user_tz": -600
    },
    "id": "9EXH9TiZUI0a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your palettes\n",
    "custom_palette = {\n",
    "    \"kommersant\": \"#ff7f0e\",      # vivid orange\n",
    "    \"literaturnaia_gazeta\": \"#1b9e77\",  # teal-green\n",
    "    \"nezavisimaia_gazeta\": \"#d62728\",   # red\n",
    "    \"novaia_gazeta\": \"#9467bd\",   # purple\n",
    "    \"pravda\": \"#8c564b\",          # brown\n",
    "    \"slovo\": \"#e377c2\",           # pink\n",
    "    \"sovetskaia_rossiia\": \"#7f7f7f\",    # grey\n",
    "    \"trud\": \"#bcbd22\",            # yellow-green\n",
    "    \"vedomosti\": \"#17becf\",       # cyan\n",
    "    \"b_mh\": \"#377eb8\",            # blue\n",
    "    \"о_г\": \"#ffae00\",             # yellow\n",
    "}\n",
    "eir_colors = {\"eir\": \"#1f77b4\"}       # keep blue\n",
    "rt_colors  = {\"rt\":  \"#4daf4a\"}       # dark leaf green\n",
    "ztra       = {\"Zavtra\": \"#984ea3\"}   # violet\n",
    "tsar_colors= {\"Tsargrad\": \"#a65628\"} # chestnut brown\n",
    "\n",
    "\n",
    "# Master palette used in plots (only keys that appear will be used)\n",
    "master_palette = {}\n",
    "master_palette.update(custom_palette)\n",
    "master_palette.update(eir_colors)\n",
    "master_palette.update(rt_colors)\n",
    "master_palette.update(ztra)\n",
    "master_palette.update(tsar_colors)\n",
    "\n",
    "# EastView -> palette keys (lowercase/ASCII)\n",
    "eastview_map = {\n",
    "    \"Kommersant\": \"kommersant\",\n",
    "    \"Literaturnaia gazeta\": \"literaturnaia_gazeta\",\n",
    "    \"Nezavisimaia gazeta\": \"nezavisimaia_gazeta\",\n",
    "    \"Novaia gazeta\": \"novaia_gazeta\",\n",
    "    \"Pravda\": \"pravda\",\n",
    "    \"Slovo\": \"slovo\",\n",
    "    \"Sovetskaia Rossiia\": \"sovetskaia_rossiia\",\n",
    "    \"Trud\": \"trud\",\n",
    "    \"Vedomosti\": \"vedomosti\",\n",
    "    \"Время MH\": \"b_mh\",   # latin H\n",
    "    \"Время МН\": \"b_mh\",   # cyrillic Н (just in case)\n",
    "    \"Общая газета\": \"о_г\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_neg_prob(df):\n",
    "    \"\"\"Return a Series with negativity probability derived from score+probability.\"\"\"\n",
    "    if \"probability\" not in df.columns or \"score\" not in df.columns:\n",
    "        return pd.Series([np.nan]*len(df), index=df.index)\n",
    "    p = pd.to_numeric(df[\"probability\"], errors=\"coerce\")\n",
    "    s = df[\"score\"].astype(str).str.lower()\n",
    "    # map: negative -> p; positive -> 1-p; neutral -> 0.5; else -> NaN\n",
    "    out = np.where(s.eq(\"negative\"), p,\n",
    "          np.where(s.eq(\"positive\"), 1 - p,\n",
    "          np.where(s.eq(\"neutral\"), 0.5, np.nan)))\n",
    "    return pd.Series(out, index=df.index)\n",
    "\n",
    "def ensure_year_month(df):\n",
    "    \"\"\"Use existing year_month (from your earlier step). If missing, try to build from a date column.\"\"\"\n",
    "    if \"year_month\" in df.columns:\n",
    "        return pd.to_datetime(df[\"year_month\"], errors=\"coerce\")\n",
    "    # cheap fallback for one-off cases\n",
    "    for c in [\"PublicationDate\",\"PublishedDate\",\"publication_date\",\"translated_date\",\"ExtractedDate\",\"Date\",\"date\"]:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                return dt.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    return pd.Series(pd.NaT, index=df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build article-level frames for each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- EastView (11 pubs) ----\n",
    "ev = eastview_data.copy()\n",
    "\n",
    "# publication key for colors + a nice display name\n",
    "ev[\"Publication_key\"] = ev[\"Publication\"].map(eastview_map).fillna(\n",
    "    ev[\"Publication\"].astype(str).str.lower().str.replace(\" \", \"_\", regex=False)\n",
    ")\n",
    "ev[\"Publication_display\"] = ev[\"Publication\"]  # keep original pretty label\n",
    "\n",
    "# mentions and negativity\n",
    "ev[\"mentions\"] = pd.to_numeric(ev.get(\"soros_count\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "ev[\"neg_prob\"] = derive_neg_prob(ev)\n",
    "\n",
    "# time\n",
    "ev[\"year_month\"] = ensure_year_month(ev)\n",
    "ev[\"year\"] = ev[\"year_month\"].dt.year\n",
    "\n",
    "# Use ArticleID if present, else ArticleLink as article key\n",
    "ev[\"article_key\"] = ev[\"ArticleID\"] if \"ArticleID\" in ev.columns else ev[\"ArticleLink\"]\n",
    "ev_articles = ev.dropna(subset=[\"article_key\"]).copy()\n",
    "\n",
    "# ---- Zavtra ----\n",
    "zv = zavtra_data.copy()\n",
    "zv[\"Publication_key\"] = \"Zavtra\"          # use display label as key (palette has it)\n",
    "zv[\"Publication_display\"] = \"Zavtra\"\n",
    "zv[\"mentions\"] = pd.to_numeric(zv.get(\"soros_count\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "zv[\"neg_prob\"] = derive_neg_prob(zv)\n",
    "zv[\"year_month\"] = ensure_year_month(zv)\n",
    "zv[\"year\"] = zv[\"year_month\"].dt.year\n",
    "zv[\"article_key\"] = zv[\"ArticleLink\"] if \"ArticleLink\" in zv.columns else zv.index\n",
    "zv_articles = zv.dropna(subset=[\"article_key\"]).copy()\n",
    "\n",
    "# ---- Tsargrad ----\n",
    "ts = tsargrad_data.copy()\n",
    "ts[\"Publication_key\"] = \"Tsargrad\"\n",
    "ts[\"Publication_display\"] = \"Tsargrad\"\n",
    "ts[\"mentions\"] = pd.to_numeric(ts.get(\"soros_count\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "ts[\"neg_prob\"] = derive_neg_prob(ts)\n",
    "ts[\"year_month\"] = ensure_year_month(ts)\n",
    "ts[\"year\"] = ts[\"year_month\"].dt.year\n",
    "ts[\"article_key\"] = ts[\"url\"] if \"url\" in ts.columns else ts.index\n",
    "ts_articles = ts.dropna(subset=[\"article_key\"]).copy()\n",
    "\n",
    "# ---- RT (group sentences by article uid) ----\n",
    "rt = rt_data.copy()\n",
    "rt[\"Publication_key\"] = \"rt\"\n",
    "rt[\"Publication_display\"] = \"RT\"\n",
    "rt[\"year_month\"] = ensure_year_month(rt)\n",
    "rt[\"year\"] = rt[\"year_month\"].dt.year\n",
    "\n",
    "rt_grouped = (\n",
    "    rt.groupby([\"uid\",\"Publication_key\",\"Publication_display\",\"year_month\",\"year\"], as_index=False)\n",
    "      .agg(mentions=(\"soros\", \"sum\"),\n",
    "           rt_prob=(\"score_soros_sentence\",\"mean\"))   # sentence-level average per article\n",
    ")\n",
    "rt_grouped[\"article_key\"] = rt_grouped[\"uid\"]\n",
    "\n",
    "# ---- EIR (one row = one article) ----\n",
    "eir = eir_data.copy()\n",
    "eir[\"Publication_key\"] = \"eir\"\n",
    "eir[\"Publication_display\"] = \"EIR\"\n",
    "eir[\"mentions\"] = pd.to_numeric(eir.get(\"soros\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "eir[\"year_month\"] = ensure_year_month(eir)\n",
    "eir[\"year\"] = eir[\"year_month\"].dt.year\n",
    "# set rt_prob/neg_prob to NaN (no sentiment model here)\n",
    "eir[\"neg_prob\"] = np.nan\n",
    "eir[\"rt_prob\"] = np.nan\n",
    "eir[\"article_key\"] = eir.index\n",
    "eir_articles = eir[[\"Publication_key\",\"Publication_display\",\"year_month\",\"year\",\"mentions\",\"neg_prob\",\"rt_prob\",\"article_key\"]].copy()\n",
    "\n",
    "# Harmonize eastview/zavtra/tsargrad columns to match\n",
    "keep_cols = [\"Publication_key\",\"Publication_display\",\"year_month\",\"year\",\"mentions\",\"neg_prob\",\"article_key\"]\n",
    "ev_articles  = ev_articles[keep_cols].copy()\n",
    "zv_articles  = zv_articles[keep_cols].copy()\n",
    "ts_articles  = ts_articles[keep_cols].copy()\n",
    "rt_articles  = rt_grouped.rename(columns={\"rt_prob\":\"neg_prob\"})[[\"Publication_key\",\"Publication_display\",\"year_month\",\"year\",\"mentions\",\"neg_prob\",\"article_key\"]].copy()\n",
    "# NOTE: we relabeled RT mean sentence score into 'neg_prob' column to reuse code below,\n",
    "# but we will *not* mix RT with others in the same \"negativity\" chart; we'll plot RT separately.\n",
    "\n",
    "# Combine everything\n",
    "articles_all = pd.concat([ev_articles, zv_articles, ts_articles, rt_articles, eir_articles], ignore_index=True)\n",
    "# Clean time\n",
    "articles_all[\"year_month\"] = pd.to_datetime(articles_all[\"year_month\"], errors=\"coerce\")\n",
    "articles_all[\"year\"] = pd.to_numeric(articles_all[\"year\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a palette keyed by the *display* names visible in legends\n",
    "def palette_by_display(df, master_palette):\n",
    "    # df must have Publication_display + Publication_key\n",
    "    m = (df[[\"Publication_display\",\"Publication_key\"]]\n",
    "         .drop_duplicates()\n",
    "         .set_index(\"Publication_display\")[\"Publication_key\"])\n",
    "    return {disp: master_palette.get(key, \"#999999\") for disp, key in m.items()}\n",
    "\n",
    "def rotate_year_ticks(ax, rotation=45):\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "    plt.setp(ax.get_xticklabels(), rotation=rotation, ha=\"right\")\n",
    "\n",
    "# These rely on your prebuilt articles_all with:\n",
    "# ['Publication_display','Publication_key','year_month','year','article_key','mentions','neg_prob']\n",
    "# If any are missing, run your previous “assembly” cell first.\n",
    "assert {\"Publication_display\",\"Publication_key\",\"year_month\",\"year\",\"article_key\",\"mentions\"}.issubset(articles_all.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average ratio of mentioned 'soros' per article for each publication\n",
    "- mentions_per_article = total_mentions / number_of_articles\n",
    "- Both total_mentions and number_of_articles are integers, but their ratio is the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mentions per article\n",
    "per_pub_mentions = (\n",
    "    articles_all\n",
    "    .groupby([\"Publication_display\",\"Publication_key\"], as_index=False)\n",
    "    .agg(articles=(\"article_key\",\"nunique\"),\n",
    "         total_mentions=(\"mentions\",\"sum\"))\n",
    "    .assign(mentions_per_article=lambda d: d[\"total_mentions\"] / d[\"articles\"])\n",
    "    .sort_values(\"mentions_per_article\", ascending=False)\n",
    ")\n",
    "\n",
    "# Negativity (exclude sources without comparable model, i.e., keep EastView/Zavtra/Tsargrad)\n",
    "comp_sources = articles_all[\"Publication_display\"].isin([\"Zavtra\",\"Tsargrad\"]) | articles_all[\"Publication_key\"].isin(custom_palette.keys())\n",
    "per_pub_neg = (\n",
    "    articles_all.loc[comp_sources & articles_all[\"neg_prob\"].notna()]\n",
    "    .groupby([\"Publication_display\",\"Publication_key\"], as_index=False)\n",
    "    .agg(mean_neg_prob=(\"neg_prob\",\"mean\"),\n",
    "         n_articles=(\"article_key\",\"nunique\"))\n",
    "    .sort_values(\"mean_neg_prob\", ascending=False)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all rows, sorted\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "display(\n",
    "    per_pub_mentions\n",
    "      .sort_values([\"mentions_per_article\",\"Publication_display\"], ascending=[False, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "display(\n",
    "    per_pub_neg\n",
    "      .sort_values([\"mean_neg_prob\",\"Publication_display\"], ascending=[False, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EastView only (11 pubs)\n",
    "ev_mentions = per_pub_mentions[per_pub_mentions[\"Publication_key\"].isin(custom_palette.keys())]\n",
    "ev_neg      = per_pub_neg[per_pub_neg[\"Publication_key\"].isin(custom_palette.keys())]\n",
    "\n",
    "# Singles (Zavtra, Tsargrad, RT, EIR)\n",
    "singles_mentions = per_pub_mentions[per_pub_mentions[\"Publication_display\"].isin([\"Zavtra\",\"Tsargrad\",\"RT\",\"EIR\"])]\n",
    "singles_neg      = per_pub_neg[per_pub_neg[\"Publication_display\"].isin([\"Zavtra\",\"Tsargrad\"])]\n",
    "\n",
    "print(\"EastView mentions (all 11):\")\n",
    "display(ev_mentions.sort_values(\"mentions_per_article\", ascending=False))\n",
    "\n",
    "print(\"Singles mentions (Zavtra, Tsargrad, RT, EIR):\")\n",
    "display(singles_mentions.sort_values(\"mentions_per_article\", ascending=False))\n",
    "\n",
    "print(\"EastView negativity (all with sentiment):\")\n",
    "display(ev_neg.sort_values(\"mean_neg_prob\", ascending=False))\n",
    "\n",
    "print(\"Singles negativity (comparable sentiment only):\")\n",
    "display(singles_neg.sort_values(\"mean_neg_prob\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage — number of records per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute yearly counts\n",
    "yearly_counts = (\n",
    "    articles_all.dropna(subset=[\"year\"])\n",
    "      .assign(year=lambda d: d[\"year\"].astype(int))\n",
    "      .groupby([\"Publication_display\",\"Publication_key\",\"year\"], as_index=False)\n",
    "      .agg(articles=(\"article_key\",\"nunique\"))\n",
    ")\n",
    "\n",
    "# --- Palette: Publication_display -> color from your master palette ---\n",
    "def palette_by_display(df, master_palette):\n",
    "    m = {}\n",
    "    for _, r in df[[\"Publication_display\",\"Publication_key\"]].drop_duplicates().iterrows():\n",
    "        disp, key = r[\"Publication_display\"], r[\"Publication_key\"]\n",
    "        m[disp] = master_palette.get(key, \"#999999\")\n",
    "    return m\n",
    "\n",
    "pal = palette_by_display(yearly_counts, master_palette)\n",
    "\n",
    "# ---- Build a complete grid (years × pubs) so missing combos are zero ----\n",
    "def complete_grid(df, pubs, year_order):\n",
    "    # collapse to year_str + pub\n",
    "    tmp = (df.assign(year_str=df[\"year\"].astype(str))\n",
    "             .groupby([\"year_str\",\"Publication_display\"], as_index=False)[\"articles\"].sum())\n",
    "\n",
    "    # full cartesian product of all years × pubs\n",
    "    grid = pd.MultiIndex.from_product([year_order, pubs],\n",
    "                                      names=[\"year_str\",\"Publication_display\"])\n",
    "\n",
    "    out = (tmp.set_index([\"year_str\",\"Publication_display\"])\n",
    "              .reindex(grid, fill_value=0)   # fill missing with zero\n",
    "              .reset_index())\n",
    "\n",
    "    # keep ordered categorical for x\n",
    "    out[\"year_str\"] = pd.Categorical(out[\"year_str\"],\n",
    "                                     categories=year_order, ordered=True)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage — number of records per year EIR 1984 to 1995 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Window & x-axis setup (EVERY year tick 1995..2025) ---\n",
    "start_year = 1984\n",
    "end_year   = 1995 \n",
    "year_order = [str(y) for y in range(start_year, end_year + 1)]\n",
    "\n",
    "df_win = yearly_counts[\n",
    "    (yearly_counts[\"year\"] >= start_year) & (yearly_counts[\"year\"] <= end_year)\n",
    "].copy()\n",
    "\n",
    "# pubs to show\n",
    "pubs_all = sorted(df_win[\"Publication_display\"].unique().tolist())\n",
    "\n",
    "# completed grid for ALL pubs (for consistent y-limit)\n",
    "df_all_full = complete_grid(df_win, pubs_all, year_order)\n",
    "\n",
    "# Shared y-limit (use all-pubs grid so both charts are comparable)\n",
    "ymax = int(np.ceil(df_all_full[\"articles\"].max() * 1.10)) if not df_all_full.empty else 1\n",
    "\n",
    "# --- Global font sizing (bigger) ---\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 13,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "def plot_grouped_bars_bottom_legend(df, pubs_to_show, title, palette_map, year_order, ymax, width=0.9):\n",
    "    # complete grid for the chosen pubs\n",
    "    df_full = complete_grid(df, pubs_to_show, year_order)\n",
    "\n",
    "    # build palette only for pubs shown\n",
    "    pal_local = {p: palette_map.get(p, \"#999999\") for p in pubs_to_show}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(28, 12))\n",
    "\n",
    "    sns.barplot(\n",
    "        data=df_full.sort_values([\"year_str\",\"Publication_display\"]),\n",
    "        x=\"year_str\", y=\"articles\",\n",
    "        hue=\"Publication_display\",\n",
    "        palette=pal_local,\n",
    "        width=width,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_ylim(0, ymax)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Articles\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # FORCE every year label (do NOT call MaxNLocator afterwards)\n",
    "    ax.set_xticks(np.arange(len(year_order)), labels=year_order)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "    # Legend below the chart (centered)\n",
    "    ncols = min(8, max(2, int(np.ceil(len(pubs_to_show)/3))))\n",
    "    ax.legend(\n",
    "        title=\"Publication\",\n",
    "        bbox_to_anchor=(0.5, -0.18),\n",
    "        loc=\"upper center\",\n",
    "        ncol=ncols,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.24)  # extra space for legend\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 1995–2025\n",
    "plot_grouped_bars_bottom_legend(\n",
    "    df=df_win,\n",
    "    pubs_to_show=pubs_all,\n",
    "    title=\"Number of articles per year (all publications): 1984–1995\",\n",
    "    palette_map=pal,\n",
    "    year_order=year_order,\n",
    "    ymax=ymax,\n",
    "    width=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window & “complete grid” helper\n",
    "start_year = 1995\n",
    "end_year   = 2025   # keep 2025 so labels show through end even if last data < 2025\n",
    "year_order = [str(y) for y in range(start_year, end_year + 1)]\n",
    "\n",
    "df_win = yearly_counts[\n",
    "    (yearly_counts[\"year\"] >= start_year) & (yearly_counts[\"year\"] <= end_year)\n",
    "].copy()\n",
    "\n",
    "def complete_grid(df, pubs, year_order):\n",
    "    \"\"\"Return a DataFrame with all (year, pub) combos; missing → 0.\"\"\"\n",
    "    tmp = (df.assign(year_str=df[\"year\"].astype(str))\n",
    "             .groupby([\"year_str\",\"Publication_display\"], as_index=False)[\"articles\"].sum())\n",
    "    grid = pd.MultiIndex.from_product([year_order, pubs], names=[\"year_str\",\"Publication_display\"])\n",
    "    out = (tmp.set_index([\"year_str\",\"Publication_display\"])\n",
    "              .reindex(grid, fill_value=0)\n",
    "              .reset_index())\n",
    "    out[\"year_str\"] = pd.Categorical(out[\"year_str\"], categories=year_order, ordered=True)\n",
    "    return out\n",
    "\n",
    "pubs_all = sorted(df_win[\"Publication_display\"].unique().tolist())\n",
    "df_all_full = complete_grid(df_win, pubs_all, year_order)\n",
    "\n",
    "# y-limit for line charts (non-stacked): use max single-series value\n",
    "ymax_lines = int(np.ceil(df_all_full[\"articles\"].max() * 1.10)) if not df_all_full.empty else 1\n",
    "\n",
    "# 2) Styling — larger fonts\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 13,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# Helper: multi-series LINE plot with legend below\n",
    "def plot_grouped_lines_bottom_legend(df, pubs_to_show, title, palette_map, year_order, ymax, linewidth=2.5, markersize=6):\n",
    "    df_full = complete_grid(df, pubs_to_show, year_order)\n",
    "    pal_local = {p: palette_map.get(p, \"#999999\") for p in pubs_to_show}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(28, 22))\n",
    "    sns.lineplot(\n",
    "        data=df_full.sort_values([\"Publication_display\",\"year_str\"]),\n",
    "        x=\"year_str\", y=\"articles\",\n",
    "        hue=\"Publication_display\",\n",
    "        palette=pal_local,\n",
    "        marker=\"o\",\n",
    "        linewidth=linewidth,\n",
    "        markersize=markersize,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_ylim(0, ymax)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Articles\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # force every year label\n",
    "    ax.set_xticks(np.arange(len(year_order)))\n",
    "    ax.set_xticklabels(year_order, rotation=90, ha=\"center\")\n",
    "\n",
    "    # legend below chart\n",
    "    ncols = min(8, max(2, int(np.ceil(len(pubs_to_show)/3))))\n",
    "    ax.legend(\n",
    "        title=\"Publication\",\n",
    "        bbox_to_anchor=(0.5, -0.15),\n",
    "        loc=\"upper center\",\n",
    "        ncol=ncols,\n",
    "        frameon=False,\n",
    "        fontsize=16,           # legend text\n",
    "        title_fontsize=16      # legend title\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage — number of records per year (collated) and Top N publications 1995-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL publications\n",
    "plot_grouped_lines_bottom_legend(\n",
    "    df=df_win,\n",
    "    pubs_to_show=pubs_all,\n",
    "    title=\"Number of articles per year (all publications): 1995–2025\",\n",
    "    palette_map=pal,\n",
    "    year_order=year_order,\n",
    "    ymax=ymax_lines,\n",
    "    linewidth=2.8,\n",
    "    markersize=6.5\n",
    ")\n",
    "\n",
    "# TOP N publications\n",
    "TOP_N = 6\n",
    "top_pubs = (\n",
    "    df_win.groupby(\"Publication_display\")[\"articles\"]\n",
    "          .sum().sort_values(ascending=False)\n",
    "          .head(TOP_N).index.tolist()\n",
    ")\n",
    "\n",
    "plot_grouped_lines_bottom_legend(\n",
    "    df=df_win,\n",
    "    pubs_to_show=top_pubs,\n",
    "    title=f\"Number of articles per year (top {TOP_N} publications): 1995–2025\",\n",
    "    palette_map=pal,\n",
    "    year_order=year_order,\n",
    "    ymax=ymax_lines,\n",
    "    linewidth=3.0,\n",
    "    markersize=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage — number of records per year (each year view) 1995-2025\n",
    "- Produces 30+ graph, uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 1) Build monthly averages with year & month ----------\n",
    "monthly_base = (\n",
    "    articles_all.dropna(subset=[\"year_month\"])\n",
    "    .assign(year_month=lambda d: pd.to_datetime(d[\"year_month\"], errors=\"coerce\"))\n",
    "    .dropna(subset=[\"year_month\"])\n",
    "    .assign(\n",
    "        year=lambda d: d[\"year_month\"].dt.year.astype(int),\n",
    "        month=lambda d: d[\"year_month\"].dt.month.astype(int),\n",
    "    )\n",
    ")\n",
    "\n",
    "# monthly articles per pub\n",
    "monthly_articles = (\n",
    "    monthly_base\n",
    "    .groupby([\"Publication_display\",\"Publication_key\",\"year\",\"month\"], as_index=False)\n",
    "    .agg(articles=(\"article_key\",\"nunique\"))\n",
    ")\n",
    "\n",
    "# monthly total mentions per pub\n",
    "monthly_mentions = (\n",
    "    monthly_base\n",
    "    .groupby([\"Publication_display\",\"Publication_key\",\"year\",\"month\"], as_index=False)\n",
    "    .agg(mentions=(\"mentions\",\"sum\"))\n",
    ")\n",
    "\n",
    "# merge + compute average\n",
    "monthly_avg = (\n",
    "    pd.merge(\n",
    "        monthly_articles, monthly_mentions,\n",
    "        on=[\"Publication_display\",\"Publication_key\",\"year\",\"month\"], how=\"outer\"\n",
    "    )\n",
    "    .fillna({\"articles\": 0, \"mentions\": 0})\n",
    "    .assign(\n",
    "        avg_mentions_per_article=lambda df: np.where(\n",
    "            df[\"articles\"] > 0, df[\"mentions\"] / df[\"articles\"], np.nan\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------- 2) Palette helper ----------\n",
    "def palette_by_display(df, master_palette):\n",
    "    out = {}\n",
    "    for _, r in df[[\"Publication_display\",\"Publication_key\"]].drop_duplicates().iterrows():\n",
    "        out[r[\"Publication_display\"]] = master_palette.get(r[\"Publication_key\"], \"#999999\")\n",
    "    return out\n",
    "\n",
    "pal_all = palette_by_display(monthly_avg, master_palette)\n",
    "\n",
    "# List of years & pubs to iterate\n",
    "years_in_data = (\n",
    "    monthly_avg[\"year\"].dropna().astype(int).sort_values().unique().tolist()\n",
    ")\n",
    "pubs_all = sorted(monthly_avg[\"Publication_display\"].unique())\n",
    "\n",
    "# ---------- 3) Per-year peak (for dynamic height) ----------\n",
    "per_year_peak = (\n",
    "    monthly_avg\n",
    "    .groupby(\"year\")[\"avg_mentions_per_article\"]\n",
    "    .max(min_count=1)   # NaN if no data that year\n",
    ")\n",
    "\n",
    "# Cap extremes so one outlier doesn’t squash everything\n",
    "peak_cap = np.nanpercentile(per_year_peak, 95) if per_year_peak.notna().any() else 1.0\n",
    "\n",
    "# ---------- 4) Grid builder for a given year (keep NaN gaps) ----------\n",
    "def monthly_avg_grid_for_year(df, year, pubs):\n",
    "    dy = df[df[\"year\"] == year][[\"Publication_display\",\"month\",\"avg_mentions_per_article\"]].copy()\n",
    "    grid = pd.MultiIndex.from_product([pubs, range(1,13)],\n",
    "                                      names=[\"Publication_display\",\"month\"])\n",
    "    out = (\n",
    "        dy.set_index([\"Publication_display\",\"month\"])\n",
    "          .reindex(grid)   # keep NaN where no articles (no fake zeros)\n",
    "          .reset_index()\n",
    "          .rename(columns={\"avg_mentions_per_article\": \"avg\"})\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# ---------- 5) Plot per-year with dynamic height ----------\n",
    "sns.set_context(\"talk\", font_scale=1.15)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "FIGWIDTH   = 28\n",
    "MIN_HEIGHT = 8\n",
    "MAX_HEIGHT = 16\n",
    "LINEWIDTH  = 2.8\n",
    "MARKERSZ   = 6.5\n",
    "month_labels = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "\n",
    "for y in years_in_data:\n",
    "    df_y = monthly_avg_grid_for_year(monthly_avg, y, pubs_all)\n",
    "\n",
    "    # Keep only publications that have any non-NaN avg in this year\n",
    "    keep_pubs = (\n",
    "        df_y.groupby(\"Publication_display\")[\"avg\"]\n",
    "            .apply(lambda s: s.notna().any())\n",
    "    )\n",
    "    keep_pubs = keep_pubs[keep_pubs].index.tolist()\n",
    "    if not keep_pubs:\n",
    "        print(f\"No data for {y}; skipping.\")\n",
    "        continue\n",
    "    df_y = df_y[df_y[\"Publication_display\"].isin(keep_pubs)]\n",
    "\n",
    "    # Per-year dynamic limits & height\n",
    "    yr_peak = float(per_year_peak.get(y, np.nan))\n",
    "    if not np.isfinite(yr_peak) or yr_peak <= 0:\n",
    "        yr_peak = 1.0\n",
    "    yr_ylim = yr_peak * 1.15\n",
    "\n",
    "    scale = min(yr_peak, peak_cap) / max(peak_cap, 1e-9)  # normalize to [0,1]\n",
    "    fig_h = MIN_HEIGHT + (MAX_HEIGHT - MIN_HEIGHT) * scale\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(FIGWIDTH, fig_h))\n",
    "    ax = sns.lineplot(\n",
    "        data=df_y.sort_values([\"Publication_display\", \"month\"]),\n",
    "        x=\"month\", y=\"avg\",\n",
    "        hue=\"Publication_display\",\n",
    "        palette={p: pal_all.get(p, \"#999999\") for p in keep_pubs},\n",
    "        marker=\"o\",\n",
    "        linewidth=LINEWIDTH,\n",
    "        markersize=MARKERSZ,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(1, 12)\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels(month_labels, rotation=0)\n",
    "    ax.set_ylim(0, yr_ylim)     # per-year y-limit\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "    ax.set_title(f\"Average Soros mentions per article — {y} (all publications)\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Avg mentions per article\")\n",
    "\n",
    "    # Legend below the chart\n",
    "    ncols = min(8, max(2, int(np.ceil(len(keep_pubs)/3))))\n",
    "    ax.legend(\n",
    "        title=\"Publication\",\n",
    "        bbox_to_anchor=(0.5, -0.18),\n",
    "        loc=\"upper center\",\n",
    "        ncol=ncols,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.22)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage Comparison Graphs\n",
    "- Soros mentions per year (all publications) 1995-2025\n",
    "- Average sentiment probability per year (EastView pubs + Zavtra + Tsargrad only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "start_year = 1995\n",
    "end_year   = 2025\n",
    "year_order = [str(y) for y in range(start_year, end_year + 1)]\n",
    "\n",
    "def complete_grid_counts(df, value_col, pubs, year_order):\n",
    "    \"\"\"Complete grid for counts/mentions; missing -> 0.\"\"\"\n",
    "    tmp = (df.assign(year_str=df[\"year\"].astype(str))\n",
    "             .groupby([\"year_str\",\"Publication_display\"], as_index=False)[value_col].sum())\n",
    "    grid = pd.MultiIndex.from_product([year_order, pubs], names=[\"year_str\",\"Publication_display\"])\n",
    "    out = (tmp.set_index([\"year_str\",\"Publication_display\"])\n",
    "              .reindex(grid, fill_value=0)\n",
    "              .reset_index())\n",
    "    out[\"year_str\"] = pd.Categorical(out[\"year_str\"], categories=year_order, ordered=True)\n",
    "    return out\n",
    "\n",
    "def complete_grid_probs(df, pubs, year_order):\n",
    "    \"\"\"Complete grid for probability; keep NaN (don't fabricate).\"\"\"\n",
    "    tmp = (df.assign(year_str=df[\"year\"].astype(str))\n",
    "             .groupby([\"year_str\",\"Publication_display\"], as_index=False)[\"probability\"].mean())\n",
    "    grid = pd.MultiIndex.from_product([year_order, pubs], names=[\"year_str\",\"Publication_display\"])\n",
    "    out = (tmp.set_index([\"year_str\",\"Publication_display\"])\n",
    "              .reindex(grid)\n",
    "              .reset_index())\n",
    "    out[\"year_str\"] = pd.Categorical(out[\"year_str\"], categories=year_order, ordered=True)\n",
    "    return out\n",
    "\n",
    "def parse_any_year(df, candidates):\n",
    "    \"\"\"Return a Series of years by parsing the first workable date column.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            s = df[c]\n",
    "            # If it's already datetime:\n",
    "            try:\n",
    "                if pd.api.types.is_datetime64_any_dtype(s):\n",
    "                    y = s.dt.year\n",
    "                    if y.notna().any():\n",
    "                        return y\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Try parse to datetime:\n",
    "            dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                return dt.dt.year\n",
    "    # If nothing worked, return all-NaNs\n",
    "    return pd.Series([np.nan]*len(df), index=df.index)\n",
    "\n",
    "# Build yearly base metrics\n",
    "yearly_counts = (\n",
    "    articles_all.dropna(subset=[\"year\"])\n",
    "      .assign(year=lambda d: d[\"year\"].astype(int))\n",
    "      .groupby([\"Publication_display\",\"Publication_key\",\"year\"], as_index=False)\n",
    "      .agg(\n",
    "          articles=(\"article_key\",\"nunique\"),\n",
    "          mentions=(\"mentions\",\"sum\")\n",
    "      )\n",
    ")\n",
    "\n",
    "pal_all = palette_by_display(yearly_counts, master_palette)\n",
    "pubs_all = sorted(yearly_counts[\"Publication_display\"].unique().tolist())\n",
    "\n",
    "df_articles = complete_grid_counts(yearly_counts, \"articles\", pubs_all, year_order)\n",
    "df_mentions = complete_grid_counts(yearly_counts, \"mentions\", pubs_all, year_order)\n",
    "\n",
    "ymax_articles = int(np.ceil(df_articles[\"articles\"].max() * 1.10)) if len(df_articles) else 1\n",
    "ymax_mentions = int(np.ceil(df_mentions[\"mentions\"].max() * 1.10)) if len(df_mentions) else 1\n",
    "\n",
    "\n",
    "# Styling (full-width figs)\n",
    "sns.set_context(\"talk\", font_scale=1.15)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Articles / year\n",
    "\n",
    "\n",
    "\n",
    "# --- helper: legend below (dedupe, don’t drop first item) ---\n",
    "def legend_below_no_dupes(ax, title=\"Publication\", ncol=8, y=-0.12, fs=16):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    seen = set()\n",
    "    new_h, new_l = [], []\n",
    "    for h, l in zip(handles, labels):\n",
    "        if l and l != \"Publication_display\" and l not in seen:\n",
    "            new_h.append(h); new_l.append(l); seen.add(l)\n",
    "    ax.legend(new_h, new_l, title=title, ncol=ncol, loc=\"upper center\",\n",
    "              bbox_to_anchor=(0.5, y), frameon=False,\n",
    "              fontsize=fs, title_fontsize=fs)\n",
    "\n",
    "# Ensure we use the same publication order everywhere\n",
    "hue_order = sorted(df_articles[\"Publication_display\"].unique().tolist())\n",
    "\n",
    "# ---------------------------\n",
    "# Articles / year\n",
    "# ---------------------------\n",
    "fig1, ax1 = plt.subplots(figsize=(32, 22))\n",
    "sns.lineplot(\n",
    "    data=df_articles.sort_values([\"Publication_display\",\"year_str\"]),\n",
    "    x=\"year_str\", y=\"articles\",\n",
    "    hue=\"Publication_display\",\n",
    "    hue_order=hue_order,             # <-- enforce order (EIR included)\n",
    "    palette=pal_all,\n",
    "    marker=\"o\", linewidth=3.0, markersize=7, ax=ax1\n",
    ")\n",
    "ax1.set_ylim(0, ymax_articles)\n",
    "ax1.set_xlabel(\"Year\"); ax1.set_ylabel(\"Articles\")\n",
    "ax1.set_title(\"Number of articles per year (all publications): 1995–2025\")\n",
    "ax1.set_xticks(np.arange(len(year_order)))\n",
    "ax1.set_xticklabels(year_order, rotation=90, ha=\"center\")\n",
    "ax1.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "legend_below_no_dupes(ax1, title=\"Publication\", ncol=8, y=-0.12, fs=16)  # <-- fixed legend\n",
    "plt.tight_layout(); plt.subplots_adjust(bottom=0.20); plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Mentions / year\n",
    "# ---------------------------\n",
    "fig2, ax2 = plt.subplots(figsize=(32, 22))\n",
    "sns.lineplot(\n",
    "    data=df_mentions.sort_values([\"Publication_display\",\"year_str\"]),\n",
    "    x=\"year_str\", y=\"mentions\",\n",
    "    hue=\"Publication_display\",\n",
    "    hue_order=hue_order,             # <-- enforce same order\n",
    "    palette=pal_all,\n",
    "    marker=\"o\", linewidth=3.0, markersize=7, ax=ax2\n",
    ")\n",
    "ax2.set_ylim(0, ymax_mentions)\n",
    "ax2.set_xlabel(\"Year\"); ax2.set_ylabel(\"Soros mentions\")\n",
    "ax2.set_title(\"Soros mentions per year not averaged (all publications): 1995–2025\")\n",
    "ax2.set_xticks(np.arange(len(year_order)))\n",
    "ax2.set_xticklabels(year_order, rotation=90, ha=\"center\")\n",
    "ax2.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "legend_below_no_dupes(ax2, title=\"Publication\", ncol=8, y=-0.10, fs=16)  # <-- fixed legend\n",
    "plt.tight_layout(); plt.subplots_adjust(bottom=0.20); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage - Comparison Graphs Soros mentions per year Averaged\n",
    "- Average is computed by: `avg_mentions_per_article = mentions/ articles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make continuous monthly time axis for the whole window\n",
    "start_year, end_year = 1995, 2025\n",
    "all_months = pd.date_range(f\"{start_year}-01-01\", f\"{end_year}-12-01\", freq=\"MS\")\n",
    "\n",
    "# If you don't already have a palette for every pub:\n",
    "def palette_by_display(df, master_palette):\n",
    "    out = {}\n",
    "    for _, r in df[[\"Publication_display\",\"Publication_key\"]].drop_duplicates().iterrows():\n",
    "        out[r[\"Publication_display\"]] = master_palette.get(r[\"Publication_key\"], \"#999999\")\n",
    "    return out\n",
    "\n",
    "# 1) Ensure monthly_avg has year_month (1st of month)\n",
    "monthly_avg_time = monthly_avg.copy()\n",
    "monthly_avg_time[\"year_month\"] = pd.to_datetime(\n",
    "    dict(year=monthly_avg_time[\"year\"], month=monthly_avg_time[\"month\"], day=1),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "monthly_avg_time = monthly_avg_time.dropna(subset=[\"year_month\"])\n",
    "\n",
    "# 2) Complete grid (months × pubs); keep NaN so gaps remain where a pub had no articles\n",
    "pubs_all = sorted(monthly_avg_time[\"Publication_display\"].unique().tolist())\n",
    "grid = pd.MultiIndex.from_product([all_months, pubs_all], names=[\"year_month\",\"Publication_display\"])\n",
    "\n",
    "monthly_full = (\n",
    "    monthly_avg_time\n",
    "    .set_index([\"year_month\",\"Publication_display\"])[\"avg_mentions_per_article\"]\n",
    "    .reindex(grid)  # NaN for months with no articles (no fake zeros)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"avg_mentions_per_article\":\"avg\"})\n",
    ")\n",
    "\n",
    "# 3) Palette just for the pubs present\n",
    "pal_all = palette_by_display(monthly_avg_time, master_palette)\n",
    "\n",
    "# 4) Plot: one big multi-series line chart\n",
    "sns.set_context(\"talk\", font_scale=1.15)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(32, 22))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=monthly_full.sort_values([\"Publication_display\",\"year_month\"]),\n",
    "    x=\"year_month\", y=\"avg\",\n",
    "    hue=\"Publication_display\",\n",
    "    palette={p: pal_all.get(p, \"#999999\") for p in pubs_all},\n",
    "    marker=\"o\", linewidth=3.0, markersize=7,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Y range\n",
    "ymax_all = monthly_full[\"avg\"].max(skipna=True)\n",
    "ymax_all = float(ymax_all * 1.15) if pd.notna(ymax_all) and ymax_all > 0 else 1.0\n",
    "ax.set_ylim(0, ymax_all)\n",
    "\n",
    "# X axis: show EVERY year label\n",
    "years = pd.date_range(f\"{start_year}-01-01\", f\"{end_year}-01-01\", freq=\"YS\")\n",
    "ax.set_xticks(years)\n",
    "ax.set_xticklabels([str(d.year) for d in years], rotation=90, ha=\"center\")\n",
    "\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Avg mentions per article\")\n",
    "ax.set_title(\"Average Soros mentions per article — all publications (1995–2025)\")\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "# Legend below (drop redundant first handle if present)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "if labels and labels[0].lower() == \"publication_display\":\n",
    "    handles, labels = handles[1:], labels[1:]\n",
    "ax.legend(handles=handles, labels=labels, title=\"Publication\",\n",
    "    ncol=min(8, max(2, int(np.ceil(len(pubs_all)/3)))),\n",
    "    loc=\"upper center\", bbox_to_anchor=(0.5, -0.15),\n",
    "    frameon=False,\n",
    "    fontsize=16,           # legend text\n",
    "    title_fontsize=16      # legend title\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage - Comparison Graphs Soros mentions per year (each year view)\n",
    "- Produces 30+ graph, uncomment to run\n",
    "- Average is computed by: `avg_mentions_per_article = mentions/ articles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- compute per-year peak (max avg mentions/article in that year) ---\n",
    "per_year_peak = (\n",
    "    monthly_avg\n",
    "    .groupby(\"year\")[\"avg_mentions_per_article\"]\n",
    "    .max(min_count=1)   # NaN if a year has no data\n",
    ")\n",
    "\n",
    "# Cap extremes so one outlier year doesn’t make all the other charts tiny\n",
    "peak_cap = np.nanpercentile(per_year_peak, 95) if per_year_peak.notna().any() else 1.0\n",
    "\n",
    "# Figure-size controls (you can tweak these)\n",
    "FIGWIDTH   = 28\n",
    "MIN_HEIGHT = 8\n",
    "MAX_HEIGHT = 16\n",
    "LINEWIDTH  = 2.8\n",
    "MARKERSZ   = 7\n",
    "\n",
    "for y in years_in_data:\n",
    "    # grid for year y (you already have this function)\n",
    "    df_y = monthly_avg_grid_for_year(monthly_avg, y, pubs_all)\n",
    "\n",
    "    # Keep only pubs that actually have any non-NaN avg that year\n",
    "    keep_pubs = (\n",
    "        df_y.groupby(\"Publication_display\")[\"avg\"]\n",
    "            .apply(lambda s: s.notna().any())\n",
    "    )\n",
    "    keep_pubs = keep_pubs[keep_pubs].index.tolist()\n",
    "    if not keep_pubs:\n",
    "        print(f\"No data for {y}; skipping.\")\n",
    "        continue\n",
    "    df_y = df_y[df_y[\"Publication_display\"].isin(keep_pubs)]\n",
    "\n",
    "    # ----- dynamic limits + dynamic figure height -----\n",
    "    yr_peak = float(per_year_peak.get(y, np.nan))\n",
    "    if not np.isfinite(yr_peak) or yr_peak <= 0:\n",
    "        # fall back to a small default to avoid errors\n",
    "        yr_peak = 1.0\n",
    "\n",
    "    # y-axis limit = a bit above the year’s peak\n",
    "    yr_ylim = yr_peak * 1.15\n",
    "\n",
    "    # height scaled to the (capped) year peak so low-variance years aren't tiny\n",
    "    scale = min(yr_peak, peak_cap) / max(peak_cap, 1e-9)  # normalize to [0,1]\n",
    "    fig_h = MIN_HEIGHT + (MAX_HEIGHT - MIN_HEIGHT) * scale\n",
    "\n",
    "    # ----- plot -----\n",
    "    plt.figure(figsize=(FIGWIDTH, fig_h))\n",
    "    ax = sns.lineplot(\n",
    "        data=df_y.sort_values([\"Publication_display\", \"month\"]),\n",
    "        x=\"month\", y=\"avg\",\n",
    "        hue=\"Publication_display\",\n",
    "        palette={p: pal_all.get(p, \"#999999\") for p in keep_pubs},\n",
    "        marker=\"o\",\n",
    "        linewidth=LINEWIDTH,\n",
    "        markersize=MARKERSZ,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(1, 12)\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])\n",
    "    ax.set_ylim(0, yr_ylim)                  # <-- per-year y-limit\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "    ax.set_title(f\"Average Soros mentions per article — {y} (all publications)\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Avg mentions per article\")\n",
    "\n",
    "    # Legend below the chart\n",
    "    ncols = min(8, max(2, int(np.ceil(len(keep_pubs)/3))))\n",
    "    ax.legend(\n",
    "        title=\"Publication\",\n",
    "        bbox_to_anchor=(0.5, -0.18),\n",
    "        loc=\"upper center\",\n",
    "        ncol=ncols,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.22)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "## Average negativity probability over time (average negativity probability (0–1))\n",
    "- Per-article negativity score: for every article we turn the model’s score + probability into “probability the article is negative”:\n",
    "    - if score == \"negative\" → neg_prob = probability\n",
    "    - if score == \"positive\" → neg_prob = 1 - probability\n",
    "    - if score == \"neutral\" → neg_prob = 0.5\n",
    "    - otherwise → NaN (dropped from averages)\n",
    "- Averaging: for each publication and each time bucket (month or year), we take the simple arithmetic mean of neg_prob across all its articles in that bucket. Months with no articles stay NaN (shown as gaps, not zeros).\n",
    "- Publications: all EastView outlets (the 11 titles), Zavtra, and Tsargrad—they all have the compatible score/probability fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_time = articles_all.dropna(subset=[\"year_month\",\"neg_prob\"]).copy()\n",
    "neg_time[\"year_month\"] = pd.to_datetime(neg_time[\"year_month\"], errors=\"coerce\")\n",
    "neg_time = neg_time.dropna(subset=[\"year_month\"])\n",
    "\n",
    "monthly_neg = (\n",
    "    neg_time\n",
    "    .groupby([\"Publication_display\",\"Publication_key\",\"year_month\"], as_index=False)\n",
    "    .agg(mean_neg_prob=(\"neg_prob\",\"mean\"))\n",
    "    .sort_values([\"Publication_display\",\"year_month\"])\n",
    ")\n",
    "\n",
    "# Palette helper (uses your master_palette via Publication_key)\n",
    "def palette_by_display(df, master_palette):\n",
    "    m = {}\n",
    "    for _, r in df[[\"Publication_display\",\"Publication_key\"]].drop_duplicates().iterrows():\n",
    "        m[r[\"Publication_display\"]] = master_palette.get(r[\"Publication_key\"], \"#999999\")\n",
    "    return m\n",
    "\n",
    "pal = palette_by_display(monthly_neg, master_palette)\n",
    "\n",
    "start_year = 1995\n",
    "end_year   = 2025\n",
    "all_months = pd.date_range(f\"{start_year}-01-01\", f\"{end_year}-12-01\", freq=\"MS\")\n",
    "\n",
    "pubs = sorted(monthly_neg[\"Publication_display\"].unique().tolist())\n",
    "\n",
    "# complete grid: (month x publication), keep NaN for missing means (so lines gap rather than fake data)\n",
    "grid = pd.MultiIndex.from_product([all_months, pubs], names=[\"year_month\",\"Publication_display\"])\n",
    "monthly_full = (\n",
    "    monthly_neg.set_index([\"year_month\",\"Publication_display\"])[\"mean_neg_prob\"]\n",
    "               .reindex(grid)\n",
    "               .reset_index()\n",
    "               .rename(columns={\"mean_neg_prob\":\"neg_mean\"})\n",
    ")\n",
    "\n",
    "# Styling (full-width charts)\n",
    "sns.set_context(\"talk\", font_scale=1.15)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# ALL publications, full-width, legend below\n",
    "fig, ax = plt.subplots(figsize=(32, 22))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=monthly_full.sort_values([\"Publication_display\",\"year_month\"]),\n",
    "    x=\"year_month\", y=\"neg_mean\",\n",
    "    hue=\"Publication_display\",\n",
    "    palette=pal,\n",
    "    marker=\"o\",\n",
    "    linewidth=3.0,\n",
    "    markersize=7,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel(\"Negativity probability (0–1)\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_title(\"Average negativity probability over time\")\n",
    "\n",
    "# Year ticks on every year; labels for the year only\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "# Legend underneath\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[1:], labels=labels[1:],  # drop the label for the mapped variable\n",
    "          title=\"Publication\",\n",
    "          ncol=min(8, max(2, int(np.ceil(len(pubs)/3)))),\n",
    "          loc=\"upper center\", \n",
    "          bbox_to_anchor=(0.5, -0.15), \n",
    "          frameon=False,\n",
    "          fontsize=16,           # legend text\n",
    "          title_fontsize=16)      # legend title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.20)\n",
    "plt.show()\n",
    "\n",
    "# One publication per graph\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "HEIGHT  = 7.2     # was ~3.8\n",
    "ASPECT  = 2.0     # width = ASPECT * HEIGHT\n",
    "HSPACE  = 0.60    # more vertical breathing room between facets\n",
    "\n",
    "\n",
    "def facet_line_single(data, **kws):\n",
    "    ax = plt.gca()\n",
    "    pub = data[\"Publication_display\"].iloc[0]\n",
    "    d = data.sort_values(\"year_month\")\n",
    "    sns.lineplot(\n",
    "        data=d, x=\"year_month\", y=\"neg_mean\",\n",
    "        marker=\"o\", linewidth=3.0, markersize=6,\n",
    "        color=pal.get(pub, \"#999999\"), ax=ax\n",
    "    )\n",
    "    # midline at 0.5\n",
    "    ax.axhline(0.5, linestyle=\"--\", linewidth=1.6, color=\"#555555\", alpha=0.6, zorder=0)\n",
    "\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "    ax.tick_params(axis=\"x\", rotation=90, labelbottom=True)\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    monthly_full.dropna(subset=[\"neg_mean\"]),\n",
    "    col=\"Publication_display\",\n",
    "    col_wrap=1,               # stacked single column\n",
    "    height=HEIGHT,\n",
    "    aspect=ASPECT,\n",
    "    sharey=True, sharex=True,\n",
    "    margin_titles=True\n",
    ")\n",
    "g.map_dataframe(facet_line_single)\n",
    "g.set_axis_labels(\"Month\", \"Negativity probability (0–1)\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set(ylim=(0, 1))\n",
    "\n",
    "# Force y-ticks and add some padding/breathing room\n",
    "for ax in g.axes.flat:\n",
    "    ax.yaxis.set_major_locator(mticker.FixedLocator([0, .25, .5, .75, 1]))\n",
    "    ax.yaxis.set_major_formatter(mticker.FixedFormatter([\"0\", \"0.25\", \"0.5\", \"0.75\", \"1\"]))\n",
    "    ax.tick_params(axis=\"y\", labelleft=True)\n",
    "    ax.tick_params(axis=\"x\", rotation=90, pad=6)  # extra space under x labels\n",
    "    ax.margins(x=0.01, y=0.06)                    # a bit of space inside each axis\n",
    "\n",
    "# More space around the entire figure and between rows\n",
    "g.fig.subplots_adjust(top=0.96, bottom=0.07, left=0.09, right=0.99, hspace=HSPACE)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average negativity probability over time (top 6 publications by total article count (1995–2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average negativity probability over time (top 6 publications by total article count (1995–2025))\n",
    "neg_time = articles_all.dropna(subset=[\"year_month\",\"neg_prob\"]).copy()\n",
    "neg_time[\"year_month\"] = pd.to_datetime(neg_time[\"year_month\"], errors=\"coerce\")\n",
    "neg_time = neg_time.dropna(subset=[\"year_month\"])\n",
    "\n",
    "monthly_neg = (\n",
    "    neg_time\n",
    "    .groupby([\"Publication_display\",\"Publication_key\",\"year_month\"], as_index=False)\n",
    "    .agg(mean_neg_prob=(\"neg_prob\",\"mean\"))\n",
    "    .sort_values([\"Publication_display\",\"year_month\"])\n",
    ")\n",
    "\n",
    "# Palette helper (uses your master_palette via Publication_key)\n",
    "def palette_by_display(df, master_palette):\n",
    "    m = {}\n",
    "    for _, r in df[[\"Publication_display\",\"Publication_key\"]].drop_duplicates().iterrows():\n",
    "        m[r[\"Publication_display\"]] = master_palette.get(r[\"Publication_key\"], \"#999999\")\n",
    "    return m\n",
    "\n",
    "pal = palette_by_display(monthly_neg, master_palette)\n",
    "start_year = 1995\n",
    "end_year   = 2025\n",
    "all_months = pd.date_range(f\"{start_year}-01-01\", f\"{end_year}-12-01\", freq=\"MS\")\n",
    "\n",
    "pubs = sorted(monthly_neg[\"Publication_display\"].unique().tolist())\n",
    "\n",
    "# complete grid: (month x publication), keep NaN for missing means (so lines gap rather than fake data)\n",
    "grid = pd.MultiIndex.from_product([all_months, pubs], names=[\"year_month\",\"Publication_display\"])\n",
    "monthly_full = (\n",
    "    monthly_neg.set_index([\"year_month\",\"Publication_display\"])[\"mean_neg_prob\"]\n",
    "               .reindex(grid)\n",
    "               .reset_index()\n",
    "               .rename(columns={\"mean_neg_prob\":\"neg_mean\"})\n",
    ")\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.15)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# TOP 6 publications (by total articles, 1995–2025) WITH neg_prob data\n",
    "win_mask = (yearly_counts[\"year\"] >= 1995) & (yearly_counts[\"year\"] <= 2025)\n",
    "\n",
    "# pubs that actually have negativity data\n",
    "pubs_with_neg = monthly_neg[\"Publication_display\"].unique()\n",
    "\n",
    "top6 = (\n",
    "    yearly_counts.loc[win_mask & yearly_counts[\"Publication_display\"].isin(pubs_with_neg)]\n",
    "    .groupby(\"Publication_display\")[\"articles\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(6)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Filter the completed monthly grid to those top 6 pubs\n",
    "monthly_full_top = monthly_full[monthly_full[\"Publication_display\"].isin(top6)].copy()\n",
    "\n",
    "# Local palette for exactly these pubs (keeps your assigned colors)\n",
    "pal_top = {p: pal.get(p, \"#999999\") for p in top6}\n",
    "\n",
    "# Full-width line chart, legend underneath, every year labeled\n",
    "fig, ax = plt.subplots(figsize=(32, 22))\n",
    "sns.lineplot(\n",
    "    data=monthly_full_top.sort_values([\"Publication_display\",\"year_month\"]),\n",
    "    x=\"year_month\", y=\"neg_mean\",\n",
    "    hue=\"Publication_display\",\n",
    "    palette=pal_top,\n",
    "    marker=\"o\",\n",
    "    linewidth=3.0,\n",
    "    markersize=7,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel(\"Negativity probability (0–1)\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_title(\"Average negativity probability over time — Top 6 publications (1995–2025)\")\n",
    "\n",
    "# Show every year on the x-axis\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "# Legend underneath\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(\n",
    "    handles=handles[1:], labels=labels[1:],   # drop mapped var label\n",
    "    title=\"Publication\",\n",
    "    ncol=min(6, len(top6)),\n",
    "    loc=\"upper center\", bbox_to_anchor=(0.5, -0.10), frameon=False,\n",
    "    fontsize=16,           # legend text\n",
    "    title_fontsize=16      # legend title\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average negativity probability over time (Per year)\n",
    "- Produces 30+ graph, uncomment to run\n",
    "- This loop creates one line chart per year showing the average negativity probability (0–1) by month for each publication.\n",
    "- For every year, it builds a complete 12-month grid per outlet (leaving NaN gaps where an outlet had no data so lines don’t fake zeros), filters to outlets that actually have values that year, and can optionally keep only the top-N outlets.\n",
    "- It then plots the month-by-month lines with your publication colors, forces clear month labels, fixes the y-axis to 0–1 for comparability, and places a wrapped legend below the figure so the plot area stays uncluttered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "ONLY_TOP_N = None   # set to an int (e.g., 6) to cap pubs per year, else None\n",
    "FIGSIZE     = (28, 12)\n",
    "LINEWIDTH   = 3.0\n",
    "MARKERSIZE  = 7\n",
    "\n",
    "years_in_data = (\n",
    "    monthly_neg[\"year_month\"].dt.year.dropna().astype(int).sort_values().unique().tolist()\n",
    ")\n",
    "pubs_all = sorted(monthly_neg[\"Publication_display\"].unique())\n",
    "\n",
    "def monthly_neg_grid_for_year(df, year, pubs):\n",
    "    dy = df[df[\"year_month\"].dt.year == year].copy()\n",
    "    dy[\"month\"] = dy[\"year_month\"].dt.month\n",
    "    dy = dy[[\"Publication_display\", \"month\", \"mean_neg_prob\"]]\n",
    "    grid = pd.MultiIndex.from_product([pubs, range(1, 13)],\n",
    "                                      names=[\"Publication_display\", \"month\"])\n",
    "    out = (dy.set_index([\"Publication_display\", \"month\"])\n",
    "             .reindex(grid)        # keep NaN to show gaps\n",
    "             .reset_index()\n",
    "             .rename(columns={\"mean_neg_prob\": \"neg_mean\"}))\n",
    "    return out\n",
    "\n",
    "\n",
    "for y in years_in_data:\n",
    "    df_y = monthly_neg_grid_for_year(monthly_neg, y, pubs_all)\n",
    "\n",
    "    # 2) Keep only pubs with at least one real value this year\n",
    "    keep_pubs = (\n",
    "        df_y.groupby(\"Publication_display\")[\"neg_mean\"]\n",
    "            .apply(lambda s: s.notna().any())\n",
    "    )\n",
    "    keep_pubs = keep_pubs[keep_pubs].index.tolist()\n",
    "    if not keep_pubs:\n",
    "        continue\n",
    "\n",
    "    # ALWAYS filter df_y to the pubs we will actually color\n",
    "    df_y = df_y[df_y[\"Publication_display\"].isin(keep_pubs)]\n",
    "\n",
    "    # 3) Optionally limit to top-N by months-with-data in that year\n",
    "    if ONLY_TOP_N is not None and len(keep_pubs) > ONLY_TOP_N:\n",
    "        month_counts = (\n",
    "            df_y.groupby(\"Publication_display\")[\"neg_mean\"]\n",
    "                .apply(lambda s: s.notna().sum())\n",
    "                .sort_values(ascending=False)\n",
    "        )\n",
    "        keep_pubs = month_counts.head(ONLY_TOP_N).index.tolist()\n",
    "        df_y = df_y[df_y[\"Publication_display\"].isin(keep_pubs)]\n",
    "\n",
    "    # 4) Build palette exactly for the pubs we are plotting, and set hue_order\n",
    "    hue_order = sorted(keep_pubs)\n",
    "    pal_local = {p: pal.get(p, \"#999999\") for p in hue_order}\n",
    "\n",
    "    # 5) Plot\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    ax = sns.lineplot(\n",
    "        data=df_y.sort_values([\"Publication_display\",\"month\"]),\n",
    "        x=\"month\", y=\"neg_mean\",\n",
    "        hue=\"Publication_display\",\n",
    "        hue_order=hue_order,\n",
    "        palette=pal_local,\n",
    "        marker=\"o\",\n",
    "        linewidth=LINEWIDTH,\n",
    "        markersize=MARKERSIZE,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(1, 12)\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0, .25, .5, .75, 1])\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "    ax.set_title(f\"Average negativity probability by month — {y}\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Negativity probability (0–1)\")\n",
    "\n",
    "    ncols = min(8, max(2, int(np.ceil(len(hue_order)/3))))\n",
    "    ax.legend(\n",
    "        title=\"Publication\",\n",
    "        bbox_to_anchor=(0.5, -0.18),\n",
    "        loc=\"upper center\",\n",
    "        ncol=ncols,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.22)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-source comparisons: Soros mentions per article "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Cross-source comparisons: Soros mentions per article  “all increased / all decreased” shading\n",
    "- Big event detection & synchronized surges: Code 1 with METRIC=\"articles\" (coverage) or \"mentions\" (total chatter).\n",
    "- No average applied\n",
    "- (SMOOTH_WINDOW, default 3 months).\n",
    "- Best to view for major agenda spikes across outlets. If all four outlets publish more Soros pieces, you’ll see synchronized “all increased” months. It’s ideal for spotting event-driven coverage waves.\n",
    "\n",
    "### Overview:\n",
    "- mentions: total Soros mentions in a month. Spikes when a newsroom publishes more pieces or writes pieces with lots of mentions. It’s a volume pulse of the conversation.\n",
    "- articles: number of Soros-related articles in a month. Pure coverage volume (how many pieces), regardless of how intensely each article mentions Soros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly per-source table\n",
    "src_map = {\n",
    "    \"EIR\":      articles_all[\"Publication_display\"].eq(\"EIR\"),\n",
    "    \"RT\":       articles_all[\"Publication_display\"].eq(\"RT\"),\n",
    "    \"Zavtra\":   articles_all[\"Publication_display\"].eq(\"Zavtra\"),\n",
    "    \"Tsargrad\": articles_all[\"Publication_display\"].eq(\"Tsargrad\"),\n",
    "}\n",
    "\n",
    "monthly = []\n",
    "for label, mask in src_map.items():\n",
    "    tmp = (\n",
    "        articles_all.loc[mask]\n",
    "        .dropna(subset=[\"year_month\"])\n",
    "        .assign(year_month=lambda d: pd.to_datetime(d[\"year_month\"], errors=\"coerce\")\n",
    "                                    .dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))  # <-- fixed\n",
    "        .groupby(\"year_month\", as_index=False)\n",
    "        .agg(mentions=(\"mentions\",\"sum\"),\n",
    "             articles=(\"article_key\",\"nunique\"))\n",
    "        .assign(source=label)\n",
    "    )\n",
    "    monthly.append(tmp)\n",
    "\n",
    "monthly = pd.concat(monthly, ignore_index=True)\n",
    "\n",
    "# Continuous monthly index 1995–2025\n",
    "start, end = pd.Timestamp(\"1995-01-01\"), pd.Timestamp(\"2025-12-01\")\n",
    "all_months = pd.date_range(start, end, freq=\"MS\")\n",
    "\n",
    "# 2) Mtric & compute wide series per source\n",
    "METRIC        = \"mentions\"   # \"avg\" (recommended) | \"mentions\" | \"articles\"\n",
    "MIN_ARTICLES  = 1       # ignore avg for months where a source had < N articles\n",
    "SMOOTH_WINDOW = 3       # rolling window in months\n",
    "\n",
    "def make_wide_counts(col):\n",
    "    w = (monthly.pivot(index=\"year_month\", columns=\"source\", values=col)\n",
    "                 .reindex(all_months))\n",
    "    return w\n",
    "\n",
    "def make_wide_metric(metric):\n",
    "    if metric == \"articles\":\n",
    "        return make_wide_counts(\"articles\").fillna(0)\n",
    "    if metric == \"mentions\":\n",
    "        return make_wide_counts(\"mentions\").fillna(0)\n",
    "    # avg mentions per article (NaN if too few articles that month)\n",
    "    w_m = make_wide_counts(\"mentions\")\n",
    "    w_a = make_wide_counts(\"articles\")\n",
    "    avg = (w_m / w_a).where(w_a >= MIN_ARTICLES)  # guard low-volume noise\n",
    "    return avg\n",
    "\n",
    "wide = make_wide_metric(METRIC)\n",
    "\n",
    "# Smooth (per source) to reduce noise\n",
    "wide_smooth = wide.rolling(window=SMOOTH_WINDOW, min_periods=1).mean()\n",
    "\n",
    "# Compute \"all up / all down\" flags on smoothed series\n",
    "diff = wide_smooth.diff()\n",
    "\n",
    "# only consider a month if all series are present (or relax with .any(skipna=True))\n",
    "all_up   = diff.gt(0).all(axis=1)\n",
    "all_down = diff.lt(0).all(axis=1)\n",
    "\n",
    "# Plot\n",
    "color_map = {\n",
    "    \"EIR\":      eir_colors.get(\"eir\", \"#3066a8\"),\n",
    "    \"RT\":       rt_colors.get(\"rt\", \"#31a354\"),\n",
    "    \"Zavtra\":   master_palette.get(\"Zavtra\", \"#5b31a3\"),\n",
    "    \"Tsargrad\": master_palette.get(\"Tsargrad\", \"#a35b31\"),\n",
    "}\n",
    "\n",
    "title_map = {\n",
    "    \"articles\": \"Total Soros-related articles per month (1995–2025)\",\n",
    "    \"mentions\": \"Total Soros mentions per month (1995–2025)\",\n",
    "    \"avg\":      \"Average Soros mentions per article per month (1995–2025)\",\n",
    "}\n",
    "ylabel_map = {\n",
    "    \"articles\": \"Articles\",\n",
    "    \"mentions\": \"Mentions\",\n",
    "    \"avg\":      \"Avg mentions per article\",\n",
    "}\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22, \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11, \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 13\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(32, 16))\n",
    "\n",
    "for col in [\"EIR\",\"RT\",\"Zavtra\",\"Tsargrad\"]:\n",
    "    if col in wide_smooth.columns:\n",
    "        ax.plot(wide_smooth.index, wide_smooth[col],\n",
    "                marker=\"o\", linewidth=3.0, markersize=6.5,\n",
    "                label=col, color=color_map[col])\n",
    "\n",
    "# Shade months where all moved the same direction\n",
    "ymin, ymax = ax.get_ylim()\n",
    "for x, inc, dec in zip(wide_smooth.index, all_up, all_down):\n",
    "    if bool(inc) or bool(dec):\n",
    "        ax.axvspan(x - pd.offsets.Day(15), x + pd.offsets.Day(15),\n",
    "                   color=(\"green\" if inc else \"red\"), alpha=0.15)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "# X axis: yearly ticks\n",
    "ax.set_xlim(start, end)\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "ax.set_title(title_map[METRIC])\n",
    "ax.set_xlabel(\"Month\"); ax.set_ylabel(ylabel_map[METRIC])\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "# Legend below; bump it up slightly if it overlaps the bottom margin\n",
    "ax.legend(title=\"Source\", ncol=4, loc=\"upper center\",\n",
    "          bbox_to_anchor=(0.5, -0.10), frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Cross-source comparisons: Soros Average mentions per article  “all increased / all decreased” shading\n",
    "- Bias/intensity shifts inside articles: Code 2 (avg) with sensible MIN_ARTICLES, MIN_SOURCES, EPS, and smoothing.\n",
    "- Average applied\n",
    "- avg = mentions / articles, but only where articles ≥ MIN_ARTICLES; otherwise set to NaN. Then we smooth with a rolling mean (SMOOTH_WINDOW, default 3 months).\n",
    "- Best for when you care about narrative intensity rather than just volume. If an outlet keeps publishing the same number of pieces but starts stuffing each with more Soros references, avg climbs while articles stays flat.\n",
    "    - Intensity may genuinely be stable even as volume changes—so avg won’t move while mentions / articles do.\n",
    "### Overview:\n",
    "- avg: mentions per article = mentions / articles. This normalizes for output and shows intensity per piece (“how Soros-heavy is the average article?”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_map = {\n",
    "    \"EIR\":      articles_all[\"Publication_display\"].eq(\"EIR\"),\n",
    "    \"RT\":       articles_all[\"Publication_display\"].eq(\"RT\"),\n",
    "    \"Zavtra\":   articles_all[\"Publication_display\"].eq(\"Zavtra\"),\n",
    "    \"Tsargrad\": articles_all[\"Publication_display\"].eq(\"Tsargrad\"),\n",
    "}\n",
    "\n",
    "monthly = []\n",
    "for label, mask in src_map.items():\n",
    "    tmp = (\n",
    "        articles_all.loc[mask]\n",
    "        .dropna(subset=[\"year_month\"])\n",
    "        .assign(year_month=lambda d: pd.to_datetime(d[\"year_month\"], errors=\"coerce\")\n",
    "                                    .dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))  # <- fixed\n",
    "        .groupby(\"year_month\", as_index=False)\n",
    "        .agg(mentions=(\"mentions\",\"sum\"),\n",
    "             articles=(\"article_key\",\"nunique\"))\n",
    "        .assign(source=label)\n",
    "    )\n",
    "    monthly.append(tmp)\n",
    "monthly = pd.concat(monthly, ignore_index=True)\n",
    "\n",
    "# Continuous monthly index 1995–2025\n",
    "start, end = pd.Timestamp(\"1995-01-01\"), pd.Timestamp(\"2025-12-01\")\n",
    "all_months = pd.date_range(start, end, freq=\"MS\")\n",
    "\n",
    "# Build metric = average mentions/article\n",
    "MIN_ARTICLES   = 1     # was 3; loosen so more months survive\n",
    "SMOOTH_WINDOW  = 3     # 3-month rolling mean\n",
    "EPS            = 0.01  # ignore tiny wiggles\n",
    "MIN_SOURCES    = 3     # require at least K sources present that month\n",
    "\n",
    "def wide_counts(col):\n",
    "    return (monthly.pivot(index=\"year_month\", columns=\"source\", values=col)\n",
    "                   .reindex(all_months))\n",
    "\n",
    "w_m = wide_counts(\"mentions\")\n",
    "w_a = wide_counts(\"articles\")\n",
    "avg = (w_m / w_a).where(w_a >= MIN_ARTICLES)        # NaN when too few articles\n",
    "avg_smooth = avg.rolling(window=SMOOTH_WINDOW, min_periods=1).mean()\n",
    "\n",
    "# Robust \"all up / all down\"\n",
    "d = avg_smooth.diff()\n",
    "\n",
    "present = d.notna().sum(axis=1)                      # how many sources have a valid diff\n",
    "inc_any = (d >  EPS).any(axis=1)                     # at least one increased\n",
    "dec_any = (d < -EPS).any(axis=1)                     # at least one decreased\n",
    "has_mix = ((d > EPS).any(axis=1)) & ((d < -EPS).any(axis=1))  # both directions present\n",
    "\n",
    "all_up   = inc_any & ~has_mix & (present >= MIN_SOURCES)\n",
    "all_down = dec_any & ~has_mix & (present >= MIN_SOURCES)\n",
    "\n",
    "color_map = {\n",
    "    \"EIR\":      eir_colors.get(\"eir\", \"#3066a8\"),\n",
    "    \"RT\":       rt_colors.get(\"rt\", \"#31a354\"),\n",
    "    \"Zavtra\":   master_palette.get(\"Zavtra\", \"#5b31a3\"),\n",
    "    \"Tsargrad\": master_palette.get(\"Tsargrad\", \"#a35b31\"),\n",
    "}\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22, \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11, \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 13\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(32, 16))\n",
    "\n",
    "for src in [\"EIR\",\"RT\",\"Zavtra\",\"Tsargrad\"]:\n",
    "    if src in avg_smooth.columns:\n",
    "        ax.plot(avg_smooth.index, avg_smooth[src],\n",
    "                marker=\"o\", linewidth=3.0, markersize=6.5,\n",
    "                label=src, color=color_map[src])\n",
    "\n",
    "# Shade months where all available moved in the same direction\n",
    "ymin, ymax = ax.get_ylim()\n",
    "for x, up, down in zip(avg_smooth.index, all_up, all_down):\n",
    "    if bool(up) or bool(down):\n",
    "        ax.axvspan(x - pd.offsets.Day(15), x + pd.offsets.Day(15),\n",
    "                   color=(\"green\" if up else \"red\"), alpha=0.15)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "# X axis: yearly ticks\n",
    "ax.set_xlim(start, end)\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "ax.set_title(\"Average Soros mentions per article per month (smoothed) — 1995–2025\")\n",
    "ax.set_xlabel(\"Month\"); ax.set_ylabel(\"Avg mentions per article\")\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "# Legend below\n",
    "ax.legend(title=\"Source\", ncol=4, loc=\"upper center\",\n",
    "          bbox_to_anchor=(0.5, -0.10), frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.16)\n",
    "plt.show()\n",
    "\n",
    "# Optional: quick sanity counts\n",
    "print(\"Flagged months — all up:\", int(all_up.sum()), \"| all down:\", int(all_down.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the list of publications to plot\n",
    "#    - EV 11 = rows whose Publication_key is in your custom_palette keys\n",
    "#    - plus the four single sources (EIR, RT, Zavtra, Tsargrad)\n",
    "# ---------------------------------------\n",
    "ev_mask   = articles_all[\"Publication_key\"].isin(custom_palette.keys())\n",
    "ev_titles = (articles_all.loc[ev_mask, \"Publication_display\"]\n",
    "                        .dropna().sort_values().unique().tolist())\n",
    "\n",
    "singles   = [\"EIR\", \"RT\", \"Zavtra\", \"Tsargrad\"]\n",
    "pubs_scope = singles + ev_titles   # <- now includes all 11 EV pubs + singles\n",
    "\n",
    "# Monthly per-publication table (mentions & articles)\n",
    "monthly = (\n",
    "    articles_all.loc[articles_all[\"Publication_display\"].isin(pubs_scope)]\n",
    "      .dropna(subset=[\"year_month\"])\n",
    "      .assign(\n",
    "          year_month=lambda d: pd.to_datetime(d[\"year_month\"], errors=\"coerce\")\n",
    "                                .dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "      )\n",
    "      .groupby([\"year_month\", \"Publication_display\", \"Publication_key\"], as_index=False)\n",
    "      .agg(\n",
    "          mentions=(\"mentions\",\"sum\"),\n",
    "          articles=(\"article_key\",\"nunique\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# Continuous monthly index 1995–2025\n",
    "start, end  = pd.Timestamp(\"1995-01-01\"), pd.Timestamp(\"2025-12-01\")\n",
    "all_months  = pd.date_range(start, end, freq=\"MS\")\n",
    "\n",
    "# Average mentions / article per pub per month (with a guard)\n",
    "MIN_ARTICLES   = 1   # keep months where a pub has ≥ this many articles\n",
    "SMOOTH_WINDOW  = 3   # rolling-month smoothing\n",
    "EPS            = 0.01\n",
    "MIN_SOURCES    = 3   # need at least this many pubs present to consider \"ALL\"\n",
    "\n",
    "def wide_counts(df, col):\n",
    "    \"\"\"Pivot to wide (rows=month, cols=Publication_display).\"\"\"\n",
    "    wide = (df.pivot(index=\"year_month\", columns=\"Publication_display\", values=col)\n",
    "              .reindex(all_months))\n",
    "    return wide\n",
    "\n",
    "w_m = wide_counts(monthly, \"mentions\")\n",
    "w_a = wide_counts(monthly, \"articles\")\n",
    "\n",
    "avg = (w_m / w_a).where(w_a >= MIN_ARTICLES)                 # NaN when too few articles\n",
    "avg_smooth = avg.rolling(window=SMOOTH_WINDOW, min_periods=1).mean()\n",
    "\n",
    "# “All up / all down” flags across ALL series in scope\n",
    "d = avg_smooth.diff()\n",
    "\n",
    "present = d.notna().sum(axis=1)\n",
    "inc_any = (d >  EPS).any(axis=1)\n",
    "dec_any = (d < -EPS).any(axis=1)\n",
    "has_mix = ((d > EPS).any(axis=1)) & ((d < -EPS).any(axis=1))\n",
    "\n",
    "# If you literally want *every* series to move the same way, use .all(axis=1)\n",
    "# If that’s too strict with 15+ pubs, keep the robust version below:\n",
    "all_up   = inc_any & ~has_mix & (present >= MIN_SOURCES)\n",
    "all_down = dec_any & ~has_mix & (present >= MIN_SOURCES)\n",
    "\n",
    "# Colors for every publication we’ll plot\n",
    "def palette_by_display(df, master_palette):\n",
    "    m = {}\n",
    "    for _, r in df[[\"Publication_display\",\"Publication_key\"]].drop_duplicates().iterrows():\n",
    "        m[r[\"Publication_display\"]] = master_palette.get(r[\"Publication_key\"], \"#999999\")\n",
    "    return m\n",
    "\n",
    "pal = palette_by_display(monthly, master_palette)\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22, \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11, \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 13\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(32, 16))\n",
    "\n",
    "# Only plot columns that actually exist (some pubs might have no data after filtering)\n",
    "pubs_to_plot = [p for p in pubs_scope if p in avg_smooth.columns]\n",
    "for p in pubs_to_plot:\n",
    "    ax.plot(\n",
    "        avg_smooth.index, avg_smooth[p],\n",
    "        marker=\"o\", linewidth=3.0, markersize=6.5,\n",
    "        label=p, color=pal.get(p, \"#999999\")\n",
    "    )\n",
    "\n",
    "# Shade months where “all” available moved the same direction\n",
    "ymin, ymax = ax.get_ylim()\n",
    "for x, up, down in zip(avg_smooth.index, all_up, all_down):\n",
    "    if bool(up) or bool(down):\n",
    "        ax.axvspan(x - pd.offsets.Day(15), x + pd.offsets.Day(15),\n",
    "                   color=(\"green\" if up else \"red\"), alpha=0.15)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "# X axis: yearly ticks 1995–2025\n",
    "ax.set_xlim(start, end)\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "ax.set_title(\"Average Soros mentions per article per month (smoothed) — EV 11 + EIR + RT + Zavtra + Tsargrad\")\n",
    "ax.set_xlabel(\"Month\"); ax.set_ylabel(\"Avg mentions per article\")\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "# Legend below, multi-column\n",
    "ncols = min(8, max(3, int(np.ceil(len(pubs_to_plot)/4))))\n",
    "ax.legend(title=\"Publication\", ncol=ncols, loc=\"upper center\",\n",
    "          bbox_to_anchor=(0.5, -0.10), frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.16)\n",
    "plt.show()\n",
    "\n",
    "print(\"Flagged months — all up:\", int(all_up.sum()), \"| all down:\", int(all_down.sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Cross-source comparisons: Soros Average mentions per article  “all increased / all decreased” shading (each year view)\n",
    "- Produces 30+ graph, uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years that actually have any average data (within 1995–2025)\n",
    "from datetime import datetime\n",
    "years_in_data = sorted({d.year for d in avg_smooth.index if 1995 <= d.year <= 2025 and avg_smooth.loc[d].notna().any()})\n",
    "\n",
    "# Colors (same mapping you’ve been using)\n",
    "color_map = {\n",
    "    \"EIR\":      eir_colors.get(\"eir\", \"#3066a8\"),\n",
    "    \"RT\":       rt_colors.get(\"rt\", \"#31a354\"),\n",
    "    \"Zavtra\":   master_palette.get(\"Zavtra\", \"#5b31a3\"),\n",
    "    \"Tsargrad\": master_palette.get(\"Tsargrad\", \"#a35b31\"),\n",
    "}\n",
    "\n",
    "# Style\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22, \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 12, \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 13\n",
    "})\n",
    "\n",
    "# --- Dynamic scaling controls ---\n",
    "HEADROOM = 1.20     # multiply the year’s peak by this for top of y-axis\n",
    "MIN_TOP  = 0.30     # if a year is super tiny, at least this high\n",
    "\n",
    "for y in years_in_data:\n",
    "    # Slice the smoothed monthly averages for this year\n",
    "    mask = avg_smooth.index.year == y\n",
    "    df_y = avg_smooth.loc[mask]\n",
    "    if df_y.dropna(how=\"all\").empty:\n",
    "        continue\n",
    "\n",
    "    # Compute year-specific y-top from the max across ALL publications for that year\n",
    "    yr_peak = float(np.nanmax(df_y.values))\n",
    "    if not np.isfinite(yr_peak) or yr_peak <= 0:\n",
    "        yr_peak = MIN_TOP\n",
    "    y_top = max(yr_peak * HEADROOM, MIN_TOP)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(30, 12))\n",
    "\n",
    "    # Plot one line per source\n",
    "    for src in [\"EIR\", \"RT\", \"Zavtra\", \"Tsargrad\"]:\n",
    "        if src in df_y.columns:\n",
    "            ax.plot(\n",
    "                df_y.index, df_y[src],\n",
    "                marker=\"o\", linewidth=3.0, markersize=6.5,\n",
    "                label=src, color=color_map.get(src, \"#999999\")\n",
    "            )\n",
    "\n",
    "    # Shade months where all available moved the same way\n",
    "    ymin, _ = ax.get_ylim()\n",
    "    for x in df_y.index:\n",
    "        up = bool(all_up.get(x, False))\n",
    "        down = bool(all_down.get(x, False))\n",
    "        if up or down:\n",
    "            ax.axvspan(x - pd.offsets.Day(15), x + pd.offsets.Day(15),\n",
    "                       color=(\"green\" if up else \"red\"), alpha=0.15)\n",
    "    ax.set_ylim(ymin, y_top)\n",
    "\n",
    "    # X axis: show month names for that year\n",
    "    ax.set_xlim(datetime(y, 1, 1), datetime(y, 12, 31))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "    ax.set_title(f\"Average Soros mentions per article — {y} (dynamic y-scale)\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Avg mentions per article\")\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "    # Legend below\n",
    "    ax.legend(\n",
    "        title=\"Source\", ncol=4, loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.10), frameon=False\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Cross-source comparisons: Soros Average mentions per article  “all increased / all decreased” shading (each year view) - All Eastview Publications\n",
    "- Produces 30+ graph, uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_YEAR, END_YEAR = 1995, 2025\n",
    "MIN_ARTICLES  = 1      # months where a pub has < MIN_ARTICLES -> avg is NaN (ignored)\n",
    "SMOOTH_WINDOW = 3      # rolling months for smoothing (set to 1 for raw)\n",
    "EPS           = 0.01   # ignore tiny wiggles in month-over-month change\n",
    "MIN_SOURCES   = 3      # need at least this many pubs present to consider \"ALL ↑/↓\"\n",
    "\n",
    "# Build the pub scope (EV11 + singles)\n",
    "ev_mask   = articles_all[\"Publication_key\"].isin(custom_palette.keys())\n",
    "ev_titles = (articles_all.loc[ev_mask, \"Publication_display\"]\n",
    "                        .dropna().sort_values().unique().tolist())\n",
    "singles   = [\"EIR\", \"RT\", \"Zavtra\", \"Tsargrad\"]\n",
    "pubs_scope = singles + ev_titles\n",
    "\n",
    "# Monthly table per pub (mentions, articles)\n",
    "monthly = (\n",
    "    articles_all.loc[articles_all[\"Publication_display\"].isin(pubs_scope)]\n",
    "      .dropna(subset=[\"year_month\"])\n",
    "      .assign(\n",
    "          year_month=lambda d: pd.to_datetime(d[\"year_month\"], errors=\"coerce\")\n",
    "                                 .dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "      )\n",
    "      .groupby([\"year_month\", \"Publication_display\", \"Publication_key\"], as_index=False)\n",
    "      .agg(\n",
    "          mentions=(\"mentions\",\"sum\"),\n",
    "          articles=(\"article_key\",\"nunique\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# Continuous monthly index for the whole window\n",
    "start, end = pd.Timestamp(f\"{START_YEAR}-01-01\"), pd.Timestamp(f\"{END_YEAR}-12-01\")\n",
    "all_months = pd.date_range(start, end, freq=\"MS\")\n",
    "\n",
    "def wide_counts(df, col):\n",
    "    return (df.pivot(index=\"year_month\", columns=\"Publication_display\", values=col)\n",
    "              .reindex(all_months))\n",
    "\n",
    "w_m = wide_counts(monthly, \"mentions\")\n",
    "w_a = wide_counts(monthly, \"articles\")\n",
    "\n",
    "# Avg mentions/article; NaN when too few articles\n",
    "avg = (w_m / w_a).where(w_a >= MIN_ARTICLES)\n",
    "avg_smooth = avg.rolling(window=SMOOTH_WINDOW, min_periods=1).mean()\n",
    "\n",
    "# Palette {Publication_display -> color}\n",
    "def palette_by_display(df, master_palette):\n",
    "    m = {}\n",
    "    for _, r in df[[\"Publication_display\",\"Publication_key\"]].drop_duplicates().iterrows():\n",
    "        m[r[\"Publication_display\"]] = master_palette.get(r[\"Publication_key\"], \"#999999\")\n",
    "    return m\n",
    "pal = palette_by_display(monthly, master_palette)\n",
    "\n",
    "# Years that actually have any data\n",
    "years_in_data = (avg_smooth.dropna(how=\"all\").index.year\n",
    "                 .astype(int)).unique()\n",
    "years_in_data = [y for y in range(START_YEAR, END_YEAR+1) if y in years_in_data]\n",
    "\n",
    "# Plot styling\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22, \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 12, \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 13\n",
    "})\n",
    "month_labels = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "\n",
    "for y in years_in_data:\n",
    "    # ----- slice to this year -----\n",
    "    yr = avg_smooth.loc[(avg_smooth.index.year == y)]\n",
    "    if yr.empty or yr.isna().all().all():\n",
    "        continue\n",
    "\n",
    "    # Pubs with any data this year\n",
    "    pubs_y = [c for c in yr.columns if yr[c].notna().any()]\n",
    "    if not pubs_y:\n",
    "        continue\n",
    "\n",
    "    # Month numbers for x axis\n",
    "    mnums = yr.index.month\n",
    "    # Tidy frame for seaborn (keep NaNs so gaps appear)\n",
    "    df_y = yr.copy()\n",
    "    df_y[\"month\"] = mnums\n",
    "    long_y = df_y.melt(id_vars=\"month\", var_name=\"Publication\", value_name=\"avg\")\n",
    "\n",
    "    # ----- within-year \"ALL ↑ / ALL ↓\" on smoothed series -----\n",
    "    d = yr.diff()\n",
    "    present = d.notna().sum(axis=1)\n",
    "    inc_any = (d >  EPS).any(axis=1)\n",
    "    dec_any = (d < -EPS).any(axis=1)\n",
    "    has_mix = ((d > EPS).any(axis=1)) & ((d < -EPS).any(axis=1))\n",
    "    all_up_y   = inc_any & ~has_mix & (present >= MIN_SOURCES)\n",
    "    all_down_y = dec_any & ~has_mix & (present >= MIN_SOURCES)\n",
    "\n",
    "    # Dynamic y-limit (scale to this year's peak so small years aren't squished)\n",
    "    yr_peak = float(yr.max(skipna=True).max())\n",
    "    yr_ylim = (yr_peak * 1.15) if np.isfinite(yr_peak) and yr_peak > 0 else 1.0\n",
    "\n",
    "    # ----- plot -----\n",
    "    fig, ax = plt.subplots(figsize=(28, 12))\n",
    "    sns.lineplot(\n",
    "        data=long_y.sort_values([\"Publication\",\"month\"]),\n",
    "        x=\"month\", y=\"avg\",\n",
    "        hue=\"Publication\",\n",
    "        hue_order=sorted(pubs_y),\n",
    "        palette={p: pal.get(p, \"#999999\") for p in pubs_y},\n",
    "        marker=\"o\", linewidth=3.0, markersize=6.5, ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(1, 12)\n",
    "    ax.set_xticks(range(1,13))\n",
    "    ax.set_xticklabels(month_labels)\n",
    "    ax.set_ylim(0, yr_ylim)\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "    ax.set_title(f\"Average Soros mentions per article per month (smoothed) — {y}\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Avg mentions per article\")\n",
    "\n",
    "    # Shade months where all moved same direction (tolerant to flats)\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    for month_num, up, down in zip(mnums, all_up_y, all_down_y):\n",
    "        if bool(up) or bool(down):\n",
    "            ax.axvspan(month_num - 0.5, month_num + 0.5,\n",
    "                       color=(\"green\" if up else \"red\"), alpha=0.15)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    # Legend below\n",
    "    ncols = min(8, max(2, int(np.ceil(len(pubs_y)/4))))\n",
    "    ax.legend(\n",
    "        title=\"Publication\",\n",
    "        ncol=ncols,\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.12),\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-source comparisons: Soros negative probability per article “all increased / all decreased” shading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG \n",
    "SMOOTH_WINDOW = 3   # rolling window in months; set to 1 for no smoothing\n",
    "EPS           = 0.005  # ignore tiny wiggles (month-over-month change smaller than this)\n",
    "MIN_PUBS      = 3   # require at least this many pubs present to evaluate \"ALL ↑/↓\"\n",
    "START_YEAR, END_YEAR = 1995, 2025  # x-axis window\n",
    "\n",
    "# Set monthly average negativity probability\n",
    "#    neg_prob is already in articles_all (derived earlier from score+probability)\n",
    "#    Keep only comparable sources: EastView 11 + Zavtra + Tsargrad\n",
    "\n",
    "# Expect: custom_palette contains the 11 EV keys (e.g., \"kommersant\", \"pravda\", ...).\n",
    "# master_palette should also include \"Zavtra\" and \"Tsargrad\".\n",
    "def palette_by_display(df, master_palette):\n",
    "    out = {}\n",
    "    for _, r in df[[\"Publication_display\",\"Publication_key\"]].drop_duplicates().iterrows():\n",
    "        out[r[\"Publication_display\"]] = master_palette.get(r[\"Publication_key\"], \"#999999\")\n",
    "    return out\n",
    "\n",
    "neg_time = (\n",
    "    articles_all\n",
    "      .dropna(subset=[\"year_month\", \"neg_prob\", \"Publication_display\", \"Publication_key\"])\n",
    "      .copy()\n",
    ")\n",
    "neg_time[\"year_month\"] = pd.to_datetime(neg_time[\"year_month\"], errors=\"coerce\")\n",
    "neg_time = neg_time.dropna(subset=[\"year_month\"])\n",
    "\n",
    "# Filter to comparable sources (EV 11 via palette keys) + Zavtra + Tsargrad\n",
    "keep_mask = (\n",
    "    neg_time[\"Publication_key\"].isin(custom_palette.keys()) |\n",
    "    neg_time[\"Publication_display\"].isin([\"Zavtra\", \"Tsargrad\"])\n",
    ")\n",
    "neg_time = neg_time.loc[keep_mask].copy()\n",
    "\n",
    "# Monthly mean neg_prob\n",
    "monthly_neg = (\n",
    "    neg_time\n",
    "    .groupby([\"Publication_display\",\"Publication_key\",\"year_month\"], as_index=False)\n",
    "    .agg(mean_neg_prob=(\"neg_prob\",\"mean\"))\n",
    "    .sort_values([\"Publication_display\",\"year_month\"])\n",
    ")\n",
    "\n",
    "# Build palette {Publication_display -> color}\n",
    "pal = palette_by_display(monthly_neg, master_palette)\n",
    "\n",
    "# Wide matrix with continuous monthly index\n",
    "start, end = pd.Timestamp(f\"{START_YEAR}-01-01\"), pd.Timestamp(f\"{END_YEAR}-12-01\")\n",
    "all_months = pd.date_range(start, end, freq=\"MS\")\n",
    "pubs = sorted(monthly_neg[\"Publication_display\"].unique().tolist())\n",
    "\n",
    "wide = (\n",
    "    monthly_neg\n",
    "      .pivot(index=\"year_month\", columns=\"Publication_display\", values=\"mean_neg_prob\")\n",
    "      .reindex(all_months)   # keep NaN where a pub had no articles that month\n",
    ")\n",
    "\n",
    "# Smooth & tolerant ALL-UP / ALL-DOWN flags (allow flats)\n",
    "wide_smooth = wide.rolling(window=SMOOTH_WINDOW, min_periods=1).mean()\n",
    "d = wide_smooth.diff()\n",
    "\n",
    "# Count how many pubs have valid month-over-month changes\n",
    "present = d.notna().sum(axis=1)\n",
    "\n",
    "# \"Some up\" / \"Some down\" using EPS; flats (|diff| <= EPS) are neutral\n",
    "some_up   = (d >  EPS).any(axis=1)\n",
    "some_down = (d < -EPS).any(axis=1)\n",
    "\n",
    "# Tolerant definitions: at least one moved in that direction, and nobody moved opposite\n",
    "all_up   = some_up   & ~some_down & (present >= MIN_PUBS)\n",
    "all_down = some_down & ~some_up   & (present >= MIN_PUBS)\n",
    "\n",
    "print(\"Flagged months — ALL UP:\", int(all_up.sum()), \" | ALL DOWN:\", int(all_down.sum()))\n",
    "\n",
    "# Plot (full width, legend below, yearly ticks, green/red shading)\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22, \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 11, \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 14\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(32, 16))\n",
    "\n",
    "# Plot each publication\n",
    "for pub in pubs:\n",
    "    ax.plot(\n",
    "        wide_smooth.index, wide_smooth[pub],\n",
    "        marker=\"o\", linewidth=3.0, markersize=6.5,\n",
    "        label=pub, color=pal.get(pub, \"#999999\")\n",
    "    )\n",
    "\n",
    "# X axis: yearly ticks + full window\n",
    "ax.set_xlim(start, end)\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "# Shade months where ALL moved in the same direction (allowing flats)\n",
    "ymin, ymax = ax.get_ylim()\n",
    "for x, up, down in zip(wide_smooth.index, all_up, all_down):\n",
    "    if bool(up) or bool(down):\n",
    "        ax.axvspan(x - pd.offsets.Day(15), x + pd.offsets.Day(15),\n",
    "                   color=(\"green\" if up else \"red\"), alpha=0.15)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "ax.set_title(\"Average negativity probability per month (smoothed) — EV 11 + Zavtra + Tsargrad\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Negativity probability (0–1)\")\n",
    "ax.set_ylim(0, 1)  # natural scale for probabilities\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "# Legend centered below the plot\n",
    "ncols = min(8, max(2, int(np.ceil(len(pubs)/3))))\n",
    "ax.legend(title=\"Publication\", ncol=ncols, loc=\"upper center\",\n",
    "          bbox_to_anchor=(0.5, -0.10), frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-source comparisons: Soros negative probability per article “all increased / all decreased” shading (each year view)\n",
    "- Produces 30+ graph, uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SMOOTH_WINDOW = 3    # rolling window in months; set to 1 for no smoothing\n",
    "EPS           = 0.005  # ignore tiny wiggles (month-over-month change smaller than this)\n",
    "MIN_PUBS      = 3    # require at least this many pubs present to evaluate \"ALL ↑/↓\"\n",
    "START_YEAR, END_YEAR = 1995, 2025\n",
    "\n",
    "# Monthly average negativity probability (EV11 + Zavtra + Tsargrad)\n",
    "def palette_by_display(df, master_palette):\n",
    "    out = {}\n",
    "    for _, r in df[[\"Publication_display\",\"Publication_key\"]].drop_duplicates().iterrows():\n",
    "        out[r[\"Publication_display\"]] = master_palette.get(r[\"Publication_key\"], \"#999999\")\n",
    "    return out\n",
    "\n",
    "neg_time = (\n",
    "    articles_all\n",
    "      .dropna(subset=[\"year_month\", \"neg_prob\", \"Publication_display\", \"Publication_key\"])\n",
    "      .copy()\n",
    ")\n",
    "neg_time[\"year_month\"] = pd.to_datetime(neg_time[\"year_month\"], errors=\"coerce\")\n",
    "neg_time = neg_time.dropna(subset=[\"year_month\"])\n",
    "\n",
    "# Keep ONLY comparable sources (EV11 via custom_palette keys) + Zavtra + Tsargrad\n",
    "keep_mask = (\n",
    "    neg_time[\"Publication_key\"].isin(custom_palette.keys()) |\n",
    "    neg_time[\"Publication_display\"].isin([\"Zavtra\", \"Tsargrad\"])\n",
    ")\n",
    "neg_time = neg_time.loc[keep_mask].copy()\n",
    "\n",
    "monthly_neg = (\n",
    "    neg_time\n",
    "    .groupby([\"Publication_display\",\"Publication_key\",\"year_month\"], as_index=False)\n",
    "    .agg(mean_neg_prob=(\"neg_prob\",\"mean\"))\n",
    "    .sort_values([\"Publication_display\",\"year_month\"])\n",
    ")\n",
    "\n",
    "# Colors\n",
    "pal = palette_by_display(monthly_neg, master_palette)\n",
    "\n",
    "# Continuous monthly index 1995–2025; wide matrix (rows=month, cols=publication)\n",
    "start, end = pd.Timestamp(f\"{START_YEAR}-01-01\"), pd.Timestamp(f\"{END_YEAR}-12-01\")\n",
    "all_months = pd.date_range(start, end, freq=\"MS\")\n",
    "pubs_all   = sorted(monthly_neg[\"Publication_display\"].unique().tolist())\n",
    "\n",
    "wide = (\n",
    "    monthly_neg\n",
    "      .pivot(index=\"year_month\", columns=\"Publication_display\", values=\"mean_neg_prob\")\n",
    "      .reindex(all_months)    # keep NaN where a pub had no articles that month\n",
    ")\n",
    "\n",
    "# Smooth the monthly series before we compute MoM changes\n",
    "wide_smooth = wide.rolling(window=SMOOTH_WINDOW, min_periods=1).mean()\n",
    "\n",
    "# Years to plot (only those with any data)\n",
    "years_in_data = [y for y in range(START_YEAR, END_YEAR+1)\n",
    "                 if wide_smooth.loc[wide_smooth.index.year == y].notna().any().any()]\n",
    "\n",
    "# Per-year plots with ALL↑/ALL↓ shading\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 22, \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 12, \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 13\n",
    "})\n",
    "month_labels = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "\n",
    "for y in years_in_data:\n",
    "    yr = wide_smooth.loc[wide_smooth.index.year == y, pubs_all]\n",
    "    if yr.empty or yr.isna().all().all():\n",
    "        continue\n",
    "\n",
    "    # Keep only pubs with any data this year\n",
    "    pubs_y = [c for c in yr.columns if yr[c].notna().any()]\n",
    "    if not pubs_y:\n",
    "        continue\n",
    "\n",
    "    # Compute tolerant ALL↑ / ALL↓ within this year\n",
    "    d = yr[pubs_y].diff()\n",
    "    present = d.notna().sum(axis=1)\n",
    "    inc_any = (d >  EPS).any(axis=1)\n",
    "    dec_any = (d < -EPS).any(axis=1)\n",
    "    has_mix = ((d > EPS).any(axis=1)) & ((d < -EPS).any(axis=1))\n",
    "    all_up_y   = inc_any & ~has_mix & (present >= MIN_PUBS)\n",
    "    all_down_y = dec_any & ~has_mix & (present >= MIN_PUBS)\n",
    "\n",
    "    # Tidy frame for seaborn (keep NaNs so gaps appear)\n",
    "    df_y = yr[pubs_y].copy()\n",
    "    df_y[\"month\"] = df_y.index.month\n",
    "    long_y = df_y.melt(id_vars=\"month\", var_name=\"Publication\", value_name=\"neg_mean\")\n",
    "\n",
    "    # Plot (full width; fixed 0–1 y-scale for probabilities)\n",
    "    fig, ax = plt.subplots(figsize=(28, 14))\n",
    "    sns.lineplot(\n",
    "        data=long_y.sort_values([\"Publication\",\"month\"]),\n",
    "        x=\"month\", y=\"neg_mean\",\n",
    "        hue=\"Publication\",\n",
    "        hue_order=sorted(pubs_y),\n",
    "        palette={p: pal.get(p, \"#999999\") for p in pubs_y},\n",
    "        marker=\"o\", linewidth=3.0, markersize=6.5, ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(1, 12)\n",
    "    ax.set_xticks(range(1,13))\n",
    "    ax.set_xticklabels(month_labels)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0,.25,.5,.75,1])\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "    ax.set_title(f\"Average negativity probability by month — {y} (EV11 + Zavtra + Tsargrad)\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Negativity probability (0–1)\")\n",
    "\n",
    "    # Shade months where \"all\" moved same direction (flats allowed)\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    months_in_year = yr.index.month\n",
    "    for m, up, down in zip(months_in_year, all_up_y, all_down_y):\n",
    "        if bool(up) or bool(down):\n",
    "            ax.axvspan(m - 0.5, m + 0.5, color=(\"green\" if up else \"red\"), alpha=0.15)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    # Legend centered below\n",
    "    ncols = min(8, max(2, int(np.ceil(len(pubs_y)/4))))\n",
    "    ax.legend(title=\"Publication\", ncol=ncols, loc=\"upper center\",\n",
    "              bbox_to_anchor=(0.5, -0.12), frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.18)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps (mentions & sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mentions pivot\n",
    "hm_df = (\n",
    "    articles_all.dropna(subset=[\"year_month\"])\n",
    "      .groupby([\"Publication_display\",\"Publication_key\",\"year_month\"], as_index=False)\n",
    "      .agg(mentions=(\"mentions\",\"sum\"))\n",
    ")\n",
    "hm_pivot = hm_df.pivot_table(index=\"year_month\", columns=\"Publication_display\", values=\"mentions\", fill_value=0)\n",
    "hm_pivot = hm_pivot.sort_index()\n",
    "\n",
    "plt.figure(figsize=(28, 10))  # big\n",
    "ax = sns.heatmap(hm_pivot.T, cmap=\"YlOrRd\", cbar_kws={\"label\":\"Mentions\"})\n",
    "ax.set_title(\"Heatmap — Total Monthly Soros mentions\", fontsize=20)\n",
    "ax.set_ylabel(\"Publication\", fontsize=14)\n",
    "\n",
    "# Format x-axis: years only, vertical\n",
    "idx = hm_pivot.index.to_timestamp() if hasattr(hm_pivot.index, \"to_timestamp\") else pd.to_datetime(hm_pivot.index)\n",
    "jan_locs = [i for i, d in enumerate(idx) if d.month == 1]\n",
    "year_labels = [idx[i].strftime(\"%Y\") for i in jan_locs]\n",
    "\n",
    "ax.set_xlabel(\"Year\", fontsize=14)\n",
    "ax.set_xticks(jan_locs)\n",
    "ax.set_xticklabels(year_labels, rotation=90, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg\n",
    "# Monthly average mentions per article\n",
    "d = (\n",
    "    articles_all.dropna(subset=[\"year_month\"])\n",
    "    .assign(year_month=lambda x: pd.to_datetime(x[\"year_month\"], errors=\"coerce\")\n",
    "                                 .dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    ")\n",
    "\n",
    "monthly_avg = (\n",
    "    d.groupby([\"Publication_display\",\"Publication_key\",\"year_month\"], as_index=False)\n",
    "     .agg(total_mentions=(\"mentions\",\"sum\"),\n",
    "          n_articles=(\"article_key\",\"nunique\"))\n",
    "     .assign(avg_mentions=lambda df: df[\"total_mentions\"].div(df[\"n_articles\"]).where(df[\"n_articles\"]>0))\n",
    ")\n",
    "\n",
    "avg_pivot = (\n",
    "    monthly_avg.pivot_table(index=\"year_month\",\n",
    "                            columns=\"Publication_display\",\n",
    "                            values=\"avg_mentions\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Ensure a continuous monthly index so year ticks line up\n",
    "start, end = \"1995-01-01\", \"2025-12-01\"\n",
    "idx = pd.date_range(start, end, freq=\"MS\")\n",
    "avg_pivot_cont = (\n",
    "    avg_pivot.copy()\n",
    "             .reindex(idx)          # keep months with no data as NaN (gaps)\n",
    "             .sort_index()\n",
    ")\n",
    "\n",
    "# Set vmax so one outlier doesn’t wash out the colormap\n",
    "vals = avg_pivot_cont.to_numpy().astype(float)\n",
    "vmax95 = np.nanpercentile(vals, 95) if np.isfinite(np.nanpercentile(vals, 95)) else None\n",
    "# sensible fallback if everything is 0/NaN\n",
    "vmax = float(vmax95) if vmax95 and vmax95 > 0 else 1.0\n",
    "\n",
    "plt.figure(figsize=(28, 10))\n",
    "ax = sns.heatmap(\n",
    "    avg_pivot_cont.T,                 # pubs on y, months on x\n",
    "    cmap=\"YlGnBu\",\n",
    "    mask=avg_pivot_cont.T.isna(),     # hide months with no articles\n",
    "    vmin=0, vmax=vmax,\n",
    "    cbar_kws={\"label\": \"Avg mentions per article\"}\n",
    ")\n",
    "\n",
    "ax.set_title(\"Heatmap — Monthly average Soros mentions per article\", fontsize=20)\n",
    "ax.set_ylabel(\"Publication\", fontsize=14)\n",
    "\n",
    "# X-axis: show only January labels (years)\n",
    "jan_locs = [i for i, dt in enumerate(avg_pivot_cont.index) if dt.month == 1]\n",
    "year_labels = [avg_pivot_cont.index[i].strftime(\"%Y\") for i in jan_locs]\n",
    "ax.set_xlabel(\"Year\", fontsize=14)\n",
    "ax.set_xticks(jan_locs)\n",
    "ax.set_xticklabels(year_labels, rotation=90, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only sources with compatible sentiment: EV11 + Zavtra + Tsargrad\n",
    "keep_mask = (\n",
    "    articles_all[\"Publication_key\"].isin(custom_palette.keys()) |\n",
    "    articles_all[\"Publication_display\"].isin([\"Zavtra\",\"Tsargrad\"])\n",
    ")\n",
    "\n",
    "hs_df = (\n",
    "    articles_all.loc[keep_mask]\n",
    "      .dropna(subset=[\"year_month\",\"neg_prob\"])\n",
    "      .assign(year_month=lambda d: pd.to_datetime(d[\"year_month\"], errors=\"coerce\")\n",
    "                                     .dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "      .dropna(subset=[\"year_month\"])\n",
    "      .groupby([\"Publication_display\",\"Publication_key\",\"year_month\"], as_index=False)\n",
    "      .agg(mean_neg=(\"neg_prob\",\"mean\"))\n",
    ")\n",
    "\n",
    "# Pivot and reindex to a continuous monthly timeline (1995–2025)\n",
    "months = pd.date_range(\"1995-01-01\", \"2025-12-01\", freq=\"MS\")\n",
    "hs_pivot = (\n",
    "    hs_df.pivot_table(index=\"year_month\", columns=\"Publication_display\", values=\"mean_neg\")\n",
    "         .reindex(months)\n",
    "         .sort_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(28, 10))\n",
    "ax = sns.heatmap(\n",
    "    hs_pivot.T, vmin=0, vmax=1, cmap=\"coolwarm_r\",\n",
    "    mask=hs_pivot.T.isna(), cbar_kws={\"label\":\"Negativity (0–1)\"}\n",
    ")\n",
    "ax.set_title(\"Heatmap — Monthly average negativity probability\", fontsize=20)\n",
    "ax.set_ylabel(\"Publication\", fontsize=14)\n",
    "\n",
    "# Year ticks at each January\n",
    "jan_locs = [i for i, dt in enumerate(hs_pivot.index) if dt.month == 1]\n",
    "year_labels = [hs_pivot.index[i].strftime(\"%Y\") for i in jan_locs]\n",
    "ax.set_xlabel(\"Year\", fontsize=14)\n",
    "ax.set_xticks(jan_locs)\n",
    "ax.set_xticklabels(year_labels, rotation=90, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentions per 1,000 words (where full text is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best-effort word counts from common text columns\n",
    "def word_count_series(df):\n",
    "    for c in [\"ArticleTextEnglish\",\"ArticleTextEnglish1\", \"ArticleTextEnglish2\", \"ArticleTextEnglish3\", \"ArticleTextEnglish4\", \"ArticleTextEnglish5\", \"ArticleTextEnglish6\", \"content\"]:\n",
    "        if c in df.columns:\n",
    "            return df[c].astype(str).str.split().map(len)\n",
    "    return pd.Series(np.nan, index=df.index)\n",
    "\n",
    "# Build per-publication table with total words + mentions\n",
    "wc_frames = []\n",
    "for label, df in [(\"EastView\", eastview_data), (\"Zavtra\", zavtra_data), (\"Tsargrad\", tsargrad_data), (\"RT\", rt_data), (\"EIR\", eir_data)]:\n",
    "    if df is None: continue\n",
    "    tmp = pd.DataFrame()\n",
    "    tmp[\"Publication_display\"] = label if label in [\"Zavtra\",\"Tsargrad\",\"RT\",\"EIR\"] else df[\"Publication\"]\n",
    "    tmp[\"Publication_key\"] = (df[\"Publication\"].map(eastview_map).fillna(df[\"Publication\"].astype(str).str.lower().str.replace(\" \",\"_\", regex=False))\n",
    "                              if label==\"EastView\" else label.lower())\n",
    "    tmp[\"mentions\"] = pd.to_numeric(df.get(\"soros_count\", df.get(\"soros\", 0)), errors=\"coerce\").fillna(0).astype(int)\n",
    "    tmp[\"words\"] = word_count_series(df)\n",
    "    wc_frames.append(tmp[[\"Publication_display\",\"Publication_key\",\"mentions\",\"words\"]])\n",
    "\n",
    "wc = pd.concat(wc_frames, ignore_index=True).dropna(subset=[\"words\"])\n",
    "per_pub_1000w = (\n",
    "    wc.groupby([\"Publication_display\",\"Publication_key\"], as_index=False)\n",
    "      .agg(total_mentions=(\"mentions\",\"sum\"), total_words=(\"words\",\"sum\"))\n",
    "      .assign(mentions_per_1000w=lambda d: 1000*d[\"total_mentions\"]/d[\"total_words\"])\n",
    "      .sort_values(\"mentions_per_1000w\", ascending=False)\n",
    ")\n",
    "display(per_pub_1000w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage rate vs negativity (who talks a lot & is negative?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# --- compute coverage vs negativity (your code) ---\n",
    "article_level = (articles_all\n",
    "    .groupby([\"Publication_display\",\"Publication_key\",\"article_key\"], as_index=False)\n",
    "    .agg(has_mention=(\"mentions\", lambda s: int((s > 0).sum() > 0)),\n",
    "         neg_prob=(\"neg_prob\", \"mean\"))\n",
    ")\n",
    "\n",
    "per_pub_cov = (\n",
    "    article_level.groupby([\"Publication_display\",\"Publication_key\"], as_index=False)\n",
    "      .agg(coverage_rate=(\"has_mention\",\"mean\"),\n",
    "           mean_neg=(\"neg_prob\",\"mean\"),\n",
    "           n_articles=(\"article_key\",\"nunique\"))\n",
    ")\n",
    "\n",
    "# --- palette ---\n",
    "pal = palette_by_display(per_pub_cov, master_palette)\n",
    "\n",
    "# --- bubble size scale (adjust if you want bigger/smaller bubbles) ---\n",
    "# map n_articles -> marker size points^2\n",
    "size_base = 40\n",
    "size_k    = 3.0\n",
    "sizes = size_base + size_k * per_pub_cov[\"n_articles\"].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 12))\n",
    "\n",
    "# plot all points in one call (vectorized)\n",
    "ax.scatter(\n",
    "    per_pub_cov[\"coverage_rate\"],\n",
    "    per_pub_cov[\"mean_neg\"],\n",
    "    s=sizes,\n",
    "    c=per_pub_cov[\"Publication_display\"].map(lambda d: pal.get(d, \"#999999\")),\n",
    "    alpha=0.9,\n",
    "    linewidths=0.5,\n",
    "    edgecolors=\"white\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Coverage rate (share with ≥1 mention)\")\n",
    "ax.set_ylabel(\"Mean negativity (0–1)\")\n",
    "ax.set_title(\"Coverage vs negativity by publication (bubble size = #articles)\")\n",
    "ax.grid(True, linestyle=\":\", alpha=.5)\n",
    "\n",
    "# -----------------------------\n",
    "# LEGEND(S) AT THE BOTTOM\n",
    "# -----------------------------\n",
    "\n",
    "# 1) Color legend (one swatch per publication, fixed marker size)\n",
    "pubs = per_pub_cov[\"Publication_display\"].tolist()\n",
    "color_handles = [\n",
    "    Line2D([0],[0], marker='o', linestyle='',\n",
    "           markerfacecolor=pal.get(p, \"#999999\"), markeredgecolor=\"none\",\n",
    "           markersize=10, label=p)\n",
    "    for p in pubs\n",
    "]\n",
    "\n",
    "# 2) Size legend (optional): pick nice example sizes (low/median/high)\n",
    "ex_values = np.unique(np.round(np.quantile(per_pub_cov[\"n_articles\"], [0.2, 0.5, 0.8])).astype(int))\n",
    "size_handles = [\n",
    "    plt.scatter([], [], s=size_base + size_k * v, color=\"#777\", alpha=.35, edgecolor=\"none\",\n",
    "                label=f\"{v} articles\")\n",
    "    for v in ex_values\n",
    "]\n",
    "\n",
    "# Build one combined legend at the bottom (colors on top row, sizes on second row)\n",
    "handles = color_handles + size_handles\n",
    "ncols_color = min(6, max(2, int(np.ceil(len(color_handles)/3))))\n",
    "ncols_size  = len(size_handles)\n",
    "\n",
    "leg = ax.legend(\n",
    "    handles=handles,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.14),   # push below plot\n",
    "    frameon=False,\n",
    "    ncol=max(ncols_color, ncols_size),\n",
    "    title=\"Publication  •  Bubble size = #articles\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.22)     # extra space for the bottom legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds & top keywords per publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\", \"wordcloud\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD CLOUDS (one per publication, using assigned colors)\n",
    "\n",
    "# Ensure stopwords are available\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --- Stopword sets (EN + RU + your custom adds) ---\n",
    "stop_en = set(stopwords.words('english'))\n",
    "try:\n",
    "    stop_ru = set(stopwords.words('russian'))\n",
    "except:\n",
    "    stop_ru = set()\n",
    "custom_stops = {\n",
    "    # add/remove as you like\n",
    "    \"soros\",\"soroses\",\"sorosomania\",\"sorosites\",\n",
    "    \"said\",\"say\",\"says\",\"also\",\"one\",\"two\",\"new\",\"would\",\"could\",\n",
    "    \"в\",\"и\",\"на\",\"с\",\"что\",\"по\",\"это\",\"как\",\"из\",\"за\",\"для\",\"к\",\"от\",\"не\",\"его\",\"ее\",\"их\",\"мы\",\"они\",\"он\",\"она\"\n",
    "}\n",
    "STOP = stop_en | stop_ru | custom_stops\n",
    "\n",
    "# --- Helper: map Publication_display -> your palette key ---\n",
    "disp_to_key = {\n",
    "    # singles\n",
    "    \"RT\": \"rt\",\n",
    "    \"EIR\": \"eir\",\n",
    "    \"Zavtra\": \"Zavtra\",\n",
    "    \"Tsargrad\": \"Tsargrad\",\n",
    "    # EastView (use your eastview_map if defined)\n",
    "    **eastview_map\n",
    "}\n",
    "\n",
    "def pub_color(pub_display):\n",
    "    \"\"\"Return the hex color for this publication, using your master_palette.\"\"\"\n",
    "    key = disp_to_key.get(pub_display, disp_to_key.get(pub_display.strip(), pub_display.lower()))\n",
    "    return master_palette.get(key, \"#444444\")\n",
    "\n",
    "# --- Helper: get best-available text columns from a DataFrame ---\n",
    "TEXT_CANDIDATES = [\n",
    "    \"ArticleTextEnglish\", \"ArticleText\", \"article_text\",\n",
    "    \"translated_article_excerpt\", \"content\", \"text\",\n",
    "    \"soros_sentence\"  # last resort (short)\n",
    "]\n",
    "\n",
    "def collect_text_from_df(df):\n",
    "    if df is None or len(df) == 0:\n",
    "        return \"\"\n",
    "    for col in TEXT_CANDIDATES:\n",
    "        if col in df.columns:\n",
    "            # Join only non-null strings\n",
    "            return \" \".join(df[col].dropna().astype(str).tolist())\n",
    "    return \"\"\n",
    "\n",
    "# --- Build a dict {Publication_display: concatenated text} using your original frames ---\n",
    "pub_text = {}\n",
    "\n",
    "# Singles (use source-specific DataFrames if present)\n",
    "pub_text[\"Zavtra\"]   = collect_text_from_df(globals().get(\"zavtra_data\"))\n",
    "pub_text[\"Tsargrad\"] = collect_text_from_df(globals().get(\"tsargrad_data\"))\n",
    "pub_text[\"RT\"]       = collect_text_from_df(globals().get(\"rt_data\"))\n",
    "pub_text[\"EIR\"]      = collect_text_from_df(globals().get(\"eir_data\"))\n",
    "\n",
    "# EastView sub-pubs from eastview_data (group by \"Publication\")\n",
    "ev_df = globals().get(\"eastview_data\")\n",
    "if ev_df is not None and \"Publication\" in ev_df.columns:\n",
    "    for name, sub in ev_df.groupby(\"Publication\", dropna=True):\n",
    "        pub_text[name] = collect_text_from_df(sub)\n",
    "\n",
    "# Optional: fall back to articles_all (grouped) if some pubs are still empty\n",
    "missing_pubs = [p for p, t in pub_text.items() if not t]\n",
    "if missing_pubs:\n",
    "    for p in missing_pubs:\n",
    "        txt = collect_text_from_df(articles_all.loc[articles_all[\"Publication_display\"] == p])\n",
    "        if txt:\n",
    "            pub_text[p] = txt\n",
    "\n",
    "# --- Generate a word cloud per pub (single-color using assigned hex) ---\n",
    "def solid_color_func(hex_color):\n",
    "    \"\"\"Return a wordcloud color_func that paints every word the same hex color.\"\"\"\n",
    "    def _f(*args, **kwargs):\n",
    "        return hex_color\n",
    "    return _f\n",
    "\n",
    "# You can tweak these for aesthetic/legibility\n",
    "WC_KW = dict(\n",
    "    width=1400, height=800,\n",
    "    background_color=\"white\",\n",
    "    max_words=300,\n",
    "    collocations=False,  # treat bigrams independently\n",
    "    prefer_horizontal=0.9\n",
    ")\n",
    "\n",
    "for pub, txt in pub_text.items():\n",
    "    if not txt or not isinstance(txt, str) or len(txt.strip()) == 0:\n",
    "        continue\n",
    "\n",
    "    # Simple token filter against stopwords\n",
    "    tokens = [w for w in txt.split() if w.isalpha() and w.lower() not in STOP]\n",
    "    if not tokens:\n",
    "        continue\n",
    "\n",
    "    wc = WordCloud(**WC_KW).generate(\" \".join(tokens))\n",
    "    color = pub_color(pub)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wc.recolor(color_func=solid_color_func(color)), interpolation=\"bilinear\")\n",
    "    plt.title(f\"Word cloud — {pub}\", fontsize=18)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# (Optional) quick sanity check: which pubs produced text?\n",
    "for p, t in pub_text.items():\n",
    "    print(f\"{p:15s} | characters: {len(t):>8d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF keywords, SVD scatter, and LDA topics (per publication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_series(df):\n",
    "    for c in [\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\",\"content\"]:\n",
    "        if c in df.columns:\n",
    "            return df[c].astype(str).fillna(\"\")\n",
    "    return pd.Series([], dtype=str)\n",
    "\n",
    "\n",
    "#----------\n",
    "# ------ PUBLICATIONS CHANGE HERE ------\n",
    "# pick 6 focal publications\n",
    "focals = [\"Zavtra\",\"Tsargrad\",\"RT\",\"EIR\",\"Kommersant\",\"Vedomosti\"]\n",
    "#----------\n",
    "\n",
    "docs = []\n",
    "labels = []\n",
    "for pub in focals:\n",
    "    if pub==\"Zavtra\":   s = get_text_series(zavtra_data)\n",
    "    elif pub==\"Tsargrad\": s = get_text_series(tsargrad_data)\n",
    "    elif pub==\"RT\": s = rt_data[\"content\"].astype(str).fillna(\"\")\n",
    "    elif pub==\"EIR\": s = eir_data[\"content\"].astype(str).fillna(\"\")\n",
    "    else:\n",
    "        s = eastview_data.loc[eastview_data[\"Publication\"]==pub, \"ArticleTextEnglish\"].astype(str).fillna(\"\")\n",
    "    if len(s)==0: continue\n",
    "    docs.extend(s.tolist()); labels.extend([pub]*len(s))\n",
    "\n",
    "# TF-IDF\n",
    "vec = TfidfVectorizer(max_features=10000, stop_words=\"english\")\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "# Top TF-IDF terms per publication\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())\n",
    "tfidf_df[\"pub\"] = labels\n",
    "top_terms = {}\n",
    "for pub, g in tfidf_df.groupby(\"pub\"):\n",
    "    mean_scores = g.drop(columns=[\"pub\"]).mean().sort_values(ascending=False).head(20)\n",
    "    top_terms[pub] = mean_scores\n",
    "    print(f\"\\nTop TF-IDF terms — {pub}\\n\", mean_scores.head(10))\n",
    "\n",
    "# SVD to 2D (document scatter)\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "XY = svd.fit_transform(X)\n",
    "plt.figure(figsize=(12, 14))\n",
    "for pub in np.unique(labels):\n",
    "    idx = [i for i,l in enumerate(labels) if l==pub]\n",
    "    plt.scatter(XY[idx,0], XY[idx,1], s=8, alpha=0.5, label=pub,\n",
    "                color=master_palette.get(eastview_map.get(pub, pub.lower()), \"#999999\"))\n",
    "plt.legend(markerscale=3, bbox_to_anchor=(1.02, 1), loc=\"upper left\", frameon=False)\n",
    "plt.title(\"SVD (LSA) document scatter by publication\")\n",
    "plt.xlabel(\"Component 1\"); plt.ylabel(\"Component 2\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# LDA topics (example K=8 on whole corpus)\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=42, learning_method=\"batch\")\n",
    "lda_Z = lda.fit_transform(X)\n",
    "terms = vec.get_feature_names_out()\n",
    "def top_words(topic_vec, n=10):\n",
    "    return [terms[i] for i in topic_vec.argsort()[-n:][::-1]]\n",
    "for k, comp in enumerate(lda.components_):\n",
    "    print(f\"Topic {k+1}: \", \", \".join(top_words(comp, n=12)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVFlW_Rsy-8Y"
   },
   "source": [
    "# SVD (LSA) scatterplot for all publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_series(df):\n",
    "    for c in [\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\",\"content\"]:\n",
    "        if c in df.columns:\n",
    "            return df[c].astype(str).fillna(\"\")\n",
    "    return pd.Series([], dtype=str)\n",
    "    \n",
    "EXTRA_STOPS = {\n",
    "    \"said\",\"say\",\"says\",\"mr\",\"ms\",\"mrs\",\"today\",\"yesterday\",\"tomorrow\",\n",
    "    \"news\",\"report\",\"reports\",\"reported\",\"according\",\"week\",\"weeks\",\"month\",\"months\",\n",
    "    \"year\",\"years\",\"daily\",\"update\",\"live\",\"video\",\"photo\",\"photos\",\"twitter\",\"facebook\",\n",
    "    \"gov\",\"govt\",\"via\",\"—\",\"–\",\"’\",\"“\",\"”\",\"amp\"\n",
    "}\n",
    "\n",
    "STOPWORDS = sorted(set(ENGLISH_STOP_WORDS) | EXTRA_STOPS)\n",
    "\n",
    "# Drop 1–2 letter tokens and numbers\n",
    "TOKEN_PATTERN = r\"(?u)\\b[a-zA-Z]{3,}\\b\"\n",
    "\n",
    "# Collect data\n",
    "focals = [\"Zavtra\",\"Tsargrad\",\"RT\",\"EIR\",\"Kommersant\",\"Vedomosti\"]\n",
    "\n",
    "docs = []\n",
    "labels = []\n",
    "for pub in focals:\n",
    "    if pub == \"Zavtra\":\n",
    "        s = get_text_series(zavtra_data)\n",
    "    elif pub == \"Tsargrad\":\n",
    "        s = get_text_series(tsargrad_data)\n",
    "    elif pub == \"RT\":\n",
    "        s = rt_data[\"content\"].astype(str).fillna(\"\")\n",
    "    elif pub == \"EIR\":\n",
    "        s = eir_data[\"content\"].astype(str).fillna(\"\")\n",
    "    else:\n",
    "        s = eastview_data.loc[eastview_data[\"Publication\"]==pub, \"ArticleTextEnglish\"].astype(str).fillna(\"\")\n",
    "    if len(s) == 0:\n",
    "        continue\n",
    "    docs.extend(s.tolist())\n",
    "    labels.extend([pub] * len(s))\n",
    "\n",
    "# TF-IDF\n",
    "vec = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words=STOPWORDS,      # <-- list ok\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z]{3,}\\b\",\n",
    "    lowercase=True,\n",
    "    max_df=0.90,\n",
    "    min_df=2,\n",
    "    strip_accents=\"unicode\"    # optional: normalize accented chars\n",
    ")\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "\n",
    "terms = vec.get_feature_names_out()\n",
    "\n",
    "# Top TF-IDF terms per publication\n",
    "top_terms = {}\n",
    "labels_arr = np.array(labels)\n",
    "for pub in sorted(set(labels)):\n",
    "    idx = np.where(labels_arr == pub)[0]\n",
    "    if len(idx) == 0:\n",
    "        continue\n",
    "    mean_vec = X[idx].mean(axis=0).A1              # sparse-safe mean\n",
    "    top_idx = mean_vec.argsort()[-20:][::-1]\n",
    "    top_terms[pub] = pd.Series(mean_vec[top_idx], index=terms[top_idx])\n",
    "    print(f\"\\nTop TF-IDF terms — {pub}\\n\", top_terms[pub].head(10))\n",
    "\n",
    "# SVD (LSA) scatter\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "XY = svd.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(12, 14), dpi=120)\n",
    "for pub in sorted(set(labels)):\n",
    "    idx = np.where(labels_arr == pub)[0]\n",
    "    # falls back to grey if your palettes/ map aren’t in scope\n",
    "    color = master_palette.get(eastview_map.get(pub, pub.lower()), \"#999999\") if 'master_palette' in globals() and 'eastview_map' in globals() else \"#999999\"\n",
    "    plt.scatter(\n",
    "        XY[idx, 0], XY[idx, 1],\n",
    "        s=14, alpha=0.55, label=pub, color=color, edgecolors=\"none\"\n",
    "    )\n",
    "\n",
    "plt.title(\"SVD (LSA) document scatter by publication\", fontsize=22, pad=12)\n",
    "plt.xlabel(\"Component 1\", fontsize=16)\n",
    "plt.ylabel(\"Component 2\", fontsize=16)\n",
    "plt.grid(ls=\":\", alpha=.25)\n",
    "\n",
    "plt.legend(\n",
    "    title=\"Publication\",\n",
    "    ncol=min(6, len(set(labels))),\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.08),\n",
    "    frameon=False,\n",
    "    fontsize=14,\n",
    "    title_fontsize=14\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.12)\n",
    "plt.show()\n",
    "\n",
    "# LDA topics\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=42, learning_method=\"batch\")\n",
    "lda_Z = lda.fit_transform(X)\n",
    "\n",
    "def top_words(topic_vec, n=12):\n",
    "    idx = topic_vec.argsort()[-n:][::-1]\n",
    "    return [terms[i] for i in idx]\n",
    "\n",
    "for k, comp in enumerate(lda.components_):\n",
    "    print(f\"Topic {k+1}: \", \", \".join(top_words(comp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD (LSA) scatterplot for all publications over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import defaultdict\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "START_YEAR, END_YEAR = 1995, 2025\n",
    "WINDOW_YEARS        = 5          # 5-year buckets\n",
    "MIN_DOCS_WINDOW     = 60         # skip a window if fewer docs total than this\n",
    "MIN_DOCS_PER_PUB    = 8          # pub must have at least this many docs in window\n",
    "MAX_DOCS_PER_PUB    = 2000       # cap for speed; set None to disable\n",
    "RANDOM_SEED         = 42\n",
    "\n",
    "# Display/visual\n",
    "POINT_SIZE  = 24      # bigger!\n",
    "POINT_ALPHA = 0.45\n",
    "CENTROID_SIZE = 180\n",
    "LABEL_FONTSIZE = 11\n",
    "\n",
    "# Stopwords / tokenization\n",
    "EXTRA_STOPS = {\n",
    "    \"said\",\"say\",\"says\",\"mr\",\"ms\",\"mrs\",\"today\",\"yesterday\",\"tomorrow\",\n",
    "    \"news\",\"report\",\"reports\",\"reported\",\"according\",\"week\",\"weeks\",\"month\",\"months\",\n",
    "    \"year\",\"years\",\"daily\",\"update\",\"live\",\"video\",\"photo\",\"photos\",\"twitter\",\"facebook\",\n",
    "    \"gov\",\"govt\",\"via\",\"—\",\"–\",\"’\",\"“\",\"”\",\"amp\"\n",
    "}\n",
    "STOPWORDS = sorted(set(ENGLISH_STOP_WORDS) | EXTRA_STOPS)\n",
    "TOKEN_PATTERN = r\"(?u)\\b[a-zA-Z]{3,}\\b\"\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def get_text_series(df):\n",
    "    for c in [\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\",\"content\",\"ArticleText\"]:\n",
    "        if c in df.columns:\n",
    "            return df[c].astype(str).fillna(\"\")\n",
    "    return pd.Series([], dtype=str)\n",
    "\n",
    "def has_year_col(df):\n",
    "    return (\"year_month\" in df.columns) and pd.api.types.is_datetime64_any_dtype(df[\"year_month\"])\n",
    "\n",
    "def texts_for_pub_range(pub, y0, y1):\n",
    "    \"\"\"Return list of texts for one publication across [y0, y1].\"\"\"\n",
    "    if pub == \"Zavtra\":\n",
    "        df = zavtra_data\n",
    "    elif pub == \"Tsargrad\":\n",
    "        df = tsargrad_data\n",
    "    elif pub == \"RT\":\n",
    "        df = rt_data\n",
    "    elif pub == \"EIR\":\n",
    "        df = eir_data\n",
    "    else:\n",
    "        # EastView sub-publication\n",
    "        df = eastview_data.loc[eastview_data.get(\"Publication\",\"\") == pub]\n",
    "\n",
    "    if df is None or df.empty or not has_year_col(df):\n",
    "        return []\n",
    "\n",
    "    s = get_text_series(df)\n",
    "    y = df[\"year_month\"].dt.year\n",
    "    sel = s[(y >= y0) & (y <= y1)]\n",
    "    if MAX_DOCS_PER_PUB and len(sel) > MAX_DOCS_PER_PUB:\n",
    "        idx = rng.choice(len(sel), size=MAX_DOCS_PER_PUB, replace=False)\n",
    "        sel = sel.iloc[idx]\n",
    "    return sel.tolist()\n",
    "\n",
    "def build_palette_from_mapping(names, master_palette, eastview_map=None):\n",
    "    out = {}\n",
    "    for name in names:\n",
    "        key = eastview_map.get(name, name) if eastview_map else name\n",
    "        color = master_palette.get(key) or master_palette.get(str(key).lower(), \"#999999\")\n",
    "        out[name] = color\n",
    "    return out\n",
    "\n",
    "# ---------- Publication universe (EastView pubs + singles) ----------\n",
    "ev_pubs = sorted(eastview_data[\"Publication\"].dropna().astype(str).unique().tolist()) if \"Publication\" in eastview_data.columns else []\n",
    "pubs_all = ev_pubs + [\"Zavtra\",\"Tsargrad\",\"RT\",\"EIR\"]\n",
    "\n",
    "# ---------- Build windows ----------\n",
    "windows = []\n",
    "y = START_YEAR\n",
    "while y <= END_YEAR:\n",
    "    y0, y1 = y, min(y + WINDOW_YEARS - 1, END_YEAR)\n",
    "    windows.append((y0, y1))\n",
    "    y += WINDOW_YEARS\n",
    "\n",
    "# ---------- (1) Fit a GLOBAL TF-IDF + SVD basis across ALL windows ----------\n",
    "all_docs_for_basis = []\n",
    "for (y0, y1) in windows:\n",
    "    for pub in pubs_all:\n",
    "        texts = texts_for_pub_range(pub, y0, y1)\n",
    "        if len(texts) >= MIN_DOCS_PER_PUB:\n",
    "            all_docs_for_basis.extend(texts)\n",
    "print(f\"Global basis corpus size: {len(all_docs_for_basis):,} docs\")\n",
    "\n",
    "# If you have very many docs, you can downsample here for speed:\n",
    "# if len(all_docs_for_basis) > 200_000:\n",
    "#     idx = rng.choice(len(all_docs_for_basis), size=200_000, replace=False)\n",
    "#     all_docs_for_basis = [all_docs_for_basis[i] for i in idx]\n",
    "\n",
    "vec = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    stop_words=STOPWORDS,\n",
    "    token_pattern=TOKEN_PATTERN,\n",
    "    lowercase=True,\n",
    "    max_df=0.90,\n",
    "    min_df=2,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "X_global = vec.fit_transform(all_docs_for_basis)\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, random_state=RANDOM_SEED)\n",
    "svd.fit(X_global)  # global orientation for comparability\n",
    "\n",
    "# ---------- (2) Plot each 5-year window ----------\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "\n",
    "for (y0, y1) in windows:\n",
    "    # collect docs/labels for this window\n",
    "    docs, labels = [], []\n",
    "    pub_counts = defaultdict(int)\n",
    "    for pub in pubs_all:\n",
    "        texts = texts_for_pub_range(pub, y0, y1)\n",
    "        if len(texts) >= MIN_DOCS_PER_PUB:\n",
    "            docs.extend(texts)\n",
    "            labels.extend([pub]*len(texts))\n",
    "            pub_counts[pub] = len(texts)\n",
    "\n",
    "    total_docs = len(docs)\n",
    "    if total_docs < MIN_DOCS_WINDOW:\n",
    "        print(f\"[{y0}–{y1}] skipped — only {total_docs} docs (need ≥{MIN_DOCS_WINDOW}).\")\n",
    "        continue\n",
    "\n",
    "    # transform with global vectorizer/SVD for comparable axes\n",
    "    X = vec.transform(docs)\n",
    "    XY = svd.transform(X)\n",
    "\n",
    "    unique_labels = sorted(set(labels))\n",
    "    pal = build_palette_from_mapping(unique_labels, master_palette, eastview_map)\n",
    "\n",
    "    # ---- Figure ----\n",
    "    fig, ax = plt.subplots(figsize=(24, 16), dpi=110)\n",
    "    labels_arr = np.array(labels)\n",
    "\n",
    "    # scatter (bigger points, light outline for visibility)\n",
    "    for pub in unique_labels:\n",
    "        idx = np.where(labels_arr == pub)[0]\n",
    "        ax.scatter(\n",
    "            XY[idx, 0], XY[idx, 1],\n",
    "            s=POINT_SIZE, alpha=POINT_ALPHA,\n",
    "            label=f\"{pub} (n={pub_counts.get(pub,0)})\",\n",
    "            color=pal.get(pub, \"#999999\"),\n",
    "            edgecolors=\"white\", linewidths=0.2\n",
    "        )\n",
    "\n",
    "    # centroids (to see inter-pub proximity clearly)\n",
    "    for pub in unique_labels:\n",
    "        idx = np.where(labels_arr == pub)[0]\n",
    "        cx, cy = np.nanmean(XY[idx, 0]), np.nanmean(XY[idx, 1])\n",
    "        ax.scatter(cx, cy, s=CENTROID_SIZE,\n",
    "                   marker=\"X\", color=pal[pub], alpha=0.95, edgecolors=\"black\", linewidths=0.6, zorder=5)\n",
    "        ax.text(cx, cy, f\"  {pub}\", va=\"center\", ha=\"left\",\n",
    "                fontsize=LABEL_FONTSIZE, weight=\"bold\", color=pal[pub],\n",
    "                zorder=6)\n",
    "\n",
    "    ax.set_title(f\"SVD (LSA) document scatter by publication — {y0}–{y1}\", pad=10)\n",
    "    ax.set_xlabel(\"Component 1\"); ax.set_ylabel(\"Component 2\")\n",
    "    ax.grid(ls=\":\", alpha=.25)\n",
    "\n",
    "    # Legend at bottom\n",
    "    ncols = min(8, max(2, int(np.ceil(len(unique_labels)/3))))\n",
    "    ax.legend(\n",
    "        title=\"Publication\",\n",
    "        ncol=ncols, loc=\"upper center\", bbox_to_anchor=(0.5, -0.08),\n",
    "        frameon=False, fontsize=13, title_fontsize=13\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # No labels\n",
    "# # ==============================\n",
    "# # SVD (LSA) per 5-year window — EV11 + Zavtra + Tsargrad + RT + EIR\n",
    "# # ==============================\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.lines import Line2D\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# # -------------------------------------------------\n",
    "# # 0) CONFIG\n",
    "# # -------------------------------------------------\n",
    "# START_YEAR = 1995\n",
    "# END_YEAR   = 2025\n",
    "# WINDOW     = 5                # 5-year windows: 95–99, 00–04, ...\n",
    "# MIN_DOCS_TOTAL     = 50       # skip window if fewer docs overall\n",
    "# MIN_DOCS_PER_PUB   = 10       # keep pubs with at least this many docs in the window\n",
    "# MIN_CHARS_PER_DOC  = 200      # optional: filter very short texts\n",
    "\n",
    "# POINT_SIZE         = 18\n",
    "# POINT_ALPHA        = 0.45\n",
    "# CENTROID_SIZE      = 140\n",
    "\n",
    "# SHOW_CENTROIDS        = True\n",
    "# SHOW_CENTROID_LABELS  = False    # <-- toggle centroid text labels here\n",
    "# SHOW_LEGEND           = True\n",
    "\n",
    "# TITLE_PREFIX = \"SVD (LSA) — content similarity by publication\"\n",
    "\n",
    "# # -------------------------------------------------\n",
    "# # 1) Utilities\n",
    "# # -------------------------------------------------\n",
    "# def first_present(df, cols):\n",
    "#     for c in cols:\n",
    "#         if c in df.columns:\n",
    "#             return c\n",
    "#     return None\n",
    "\n",
    "# def pick_first_date(df, candidates):\n",
    "#     for c in candidates:\n",
    "#         if c in df.columns:\n",
    "#             dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "#             if dt.notna().any():\n",
    "#                 return dt\n",
    "#     return pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "# def to_month_start(dt_ser):\n",
    "#     dt = pd.to_datetime(dt_ser, errors=\"coerce\")\n",
    "#     return pd.to_datetime({\"year\": dt.dt.year, \"month\": dt.dt.month, \"day\": 1})\n",
    "\n",
    "# # If you already have eastview_map/master_palette in memory, we’ll use them.\n",
    "# # Otherwise, we’ll define safe fallbacks for color lookup.\n",
    "# eastview_map = globals().get(\"eastview_map\", {\n",
    "#     \"Vedomosti\": \"vedomosti\",\n",
    "#     \"Nezavisimaia gazeta\": \"nezavisimaia_gazeta\",\n",
    "#     \"Trud\": \"trud\",\n",
    "#     \"Время МН\": \"b_mh\",\n",
    "#     \"Kommersant\": \"kommersant\",\n",
    "#     \"Общая газета\": \"о_г\",\n",
    "#     \"Sovetskaia Rossiia\": \"sovetskaia_rossiia\",\n",
    "#     \"Novaia gazeta\": \"novaia_gazeta\",\n",
    "#     \"Slovo\": \"slovo\",\n",
    "#     \"Literaturnaia gazeta\": \"literaturnaia_gazeta\",\n",
    "#     \"Pravda\": \"pravda\",\n",
    "# })\n",
    "# master_palette = globals().get(\"master_palette\", {\n",
    "#     # If your session already has your full palette, it will be used instead.\n",
    "#     # Add the common singles for safety:\n",
    "#     \"rt\":  \"#31a354\",\n",
    "#     \"eir\": \"#3066a8\",\n",
    "#     \"Zavtra\":   \"#5b31a3\",\n",
    "#     \"Tsargrad\": \"#a35b31\",\n",
    "# })\n",
    "\n",
    "# def build_palette(names, master_palette, eastview_map=None):\n",
    "#     \"\"\"Map display names to colors using your palette and fallback to gray.\"\"\"\n",
    "#     pal = {}\n",
    "#     for n in names:\n",
    "#         key = eastview_map.get(n, n) if eastview_map else n\n",
    "#         col = (master_palette.get(key) or\n",
    "#                master_palette.get(str(key).lower()) or\n",
    "#                \"#999999\")\n",
    "#         pal[n] = col\n",
    "#     return pal\n",
    "\n",
    "# # Stronger stopwords\n",
    "# EXTRA_STOPS = {\n",
    "#     \"said\",\"say\",\"says\",\"mr\",\"ms\",\"mrs\",\"today\",\"yesterday\",\"tomorrow\",\n",
    "#     \"news\",\"report\",\"reports\",\"reported\",\"according\",\"week\",\"weeks\",\"month\",\"months\",\n",
    "#     \"year\",\"years\",\"daily\",\"update\",\"live\",\"video\",\"photo\",\"photos\",\"twitter\",\"facebook\",\n",
    "#     \"gov\",\"govt\",\"via\",\"—\",\"–\",\"’\",\"“\",\"”\",\"amp\"\n",
    "# }\n",
    "# STOPWORDS = sorted(set(ENGLISH_STOP_WORDS) | EXTRA_STOPS)\n",
    "# TOKEN_PATTERN = r\"(?u)\\b[a-zA-Z]{3,}\\b\"\n",
    "\n",
    "# # -------------------------------------------------\n",
    "# # 2) Assemble one text corpus with year\n",
    "# #     Inputs expected in scope:\n",
    "# #     - eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data\n",
    "# #     Required fields: text-ish columns and a parseable date column.\n",
    "# # -------------------------------------------------\n",
    "# def assemble_all_articles(eastview_data, zavtra_data, tsargrad_data, rt_data=None, eir_data=None):\n",
    "#     frames = []\n",
    "\n",
    "#     # ---------- EastView (multi-pub) ----------\n",
    "#     if eastview_data is not None and len(eastview_data):\n",
    "#         ev = eastview_data.copy()\n",
    "\n",
    "#         pub_col = first_present(ev, [\"Publication\", \"publication\"])\n",
    "#         if pub_col is None:\n",
    "#             ev[\"Publication_clean\"] = \"unknown\"\n",
    "#         else:\n",
    "#             ev[\"Publication_clean\"] = ev[pub_col].map(eastview_map).fillna(\n",
    "#                 ev[pub_col].astype(str).str.lower().str.replace(\" \", \"_\", regex=False)\n",
    "#             )\n",
    "\n",
    "#         # year_month\n",
    "#         if {\"Year\", \"Month\"}.issubset(ev.columns):\n",
    "#             y = pd.to_numeric(ev[\"Year\"], errors=\"coerce\")\n",
    "#             m = pd.to_numeric(ev[\"Month\"], errors=\"coerce\")\n",
    "#             ev[\"year_month\"] = pd.to_datetime({\"year\": y, \"month\": m, \"day\": 1}, errors=\"coerce\")\n",
    "#         else:\n",
    "#             ev[\"year_month\"] = pick_first_date(\n",
    "#                 ev, [\"PublishedDate\",\"PublicationDate\",\"publication_date\",\"translated_date\",\"Date\",\"date\"]\n",
    "#             )\n",
    "#             ev[\"year_month\"] = ev[\"year_month\"].dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "#         text_col = first_present(ev, [\"ArticleTextEnglish\",\"ArticleText\",\"article_text\",\"translated_article_excerpt\",\"content\"])\n",
    "#         if text_col is None:\n",
    "#             ev[\"__text\"] = pd.NA\n",
    "#             text_col = \"__text\"\n",
    "\n",
    "#         frames.append(pd.DataFrame({\n",
    "#             \"Publication_clean\": ev[\"Publication_clean\"],\n",
    "#             \"year_month\": ev[\"year_month\"],\n",
    "#             \"text\": ev[text_col].astype(str)\n",
    "#         }))\n",
    "\n",
    "#     # ---------- Zavtra ----------\n",
    "#     if zavtra_data is not None and len(zavtra_data):\n",
    "#         zv = zavtra_data.copy()\n",
    "#         zv[\"Publication_clean\"] = \"Zavtra\"\n",
    "#         zv_date = pick_first_date(zv, [\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"])\n",
    "#         zv[\"year_month\"] = zv_date.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "#         text_col = first_present(zv, [\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\",\"content\"])\n",
    "#         if text_col is None:\n",
    "#             zv[\"__text\"] = pd.NA\n",
    "#             text_col = \"__text\"\n",
    "#         frames.append(pd.DataFrame({\n",
    "#             \"Publication_clean\": \"Zavtra\",\n",
    "#             \"year_month\": zv[\"year_month\"],\n",
    "#             \"text\": zv[text_col].astype(str)\n",
    "#         }))\n",
    "\n",
    "#     # ---------- Tsargrad ----------\n",
    "#     if tsargrad_data is not None and len(tsargrad_data):\n",
    "#         ts = tsargrad_data.copy()\n",
    "#         ts[\"Publication_clean\"] = \"Tsargrad\"\n",
    "#         ts_date = pick_first_date(ts, [\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"])\n",
    "#         ts[\"year_month\"] = ts_date.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "#         text_col = first_present(ts, [\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\",\"content\"])\n",
    "#         if text_col is None:\n",
    "#             ts[\"__text\"] = pd.NA\n",
    "#             text_col = \"__text\"\n",
    "#         frames.append(pd.DataFrame({\n",
    "#             \"Publication_clean\": \"Tsargrad\",\n",
    "#             \"year_month\": ts[\"year_month\"],\n",
    "#             \"text\": ts[text_col].astype(str)\n",
    "#         }))\n",
    "\n",
    "#     # ---------- RT ----------\n",
    "#     if rt_data is not None and len(rt_data):\n",
    "#         rt = rt_data.copy()\n",
    "#         rt[\"Publication_clean\"] = \"RT\"\n",
    "#         rt_date = pick_first_date(rt, [\"published_at\",\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"])\n",
    "#         rt[\"year_month\"] = rt_date.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "#         text_col = first_present(rt, [\"content\",\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\"])\n",
    "#         if text_col is None:\n",
    "#             rt[\"__text\"] = pd.NA\n",
    "#             text_col = \"__text\"\n",
    "#         frames.append(pd.DataFrame({\n",
    "#             \"Publication_clean\": \"RT\",\n",
    "#             \"year_month\": rt[\"year_month\"],\n",
    "#             \"text\": rt[text_col].astype(str)\n",
    "#         }))\n",
    "\n",
    "#     # ---------- EIR ----------\n",
    "#     if eir_data is not None and len(eir_data):\n",
    "#         ei = eir_data.copy()\n",
    "#         ei[\"Publication_clean\"] = \"EIR\"\n",
    "#         # 'date' often numeric YYYYMMDD\n",
    "#         if \"date\" in ei.columns:\n",
    "#             ei_dt = pd.to_datetime(ei[\"date\"].astype(\"string\"), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "#         else:\n",
    "#             ei_dt = pick_first_date(ei, [\"published_at\",\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"])\n",
    "#         ei[\"year_month\"] = ei_dt.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "#         text_col = first_present(ei, [\"content\",\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\"])\n",
    "#         if text_col is None:\n",
    "#             ei[\"__text\"] = pd.NA\n",
    "#             text_col = \"__text\"\n",
    "#         frames.append(pd.DataFrame({\n",
    "#             \"Publication_clean\": \"EIR\",\n",
    "#             \"year_month\": ei[\"year_month\"],\n",
    "#             \"text\": ei[text_col].astype(str)\n",
    "#         }))\n",
    "\n",
    "#     all_articles = pd.concat(frames, ignore_index=True)\n",
    "#     all_articles[\"year\"] = pd.to_datetime(all_articles[\"year_month\"], errors=\"coerce\").dt.year\n",
    "#     # optional: filter short texts\n",
    "#     all_articles[\"text_len\"] = all_articles[\"text\"].astype(str).str.len()\n",
    "#     all_articles = all_articles[all_articles[\"text_len\"] >= MIN_CHARS_PER_DOC].dropna(subset=[\"year\"])\n",
    "#     return all_articles[[\"Publication_clean\",\"year\",\"text\"]]\n",
    "\n",
    "# # -------------------------------------------------\n",
    "# # 3) Plot SVD for a given 5-year window\n",
    "# # -------------------------------------------------\n",
    "# def plot_svd_window(df_window, title_suffix, palette):\n",
    "#     \"\"\"df_window has columns: Publication_clean, text\"\"\"\n",
    "#     docs   = df_window[\"text\"].astype(str).tolist()\n",
    "#     labels = df_window[\"Publication_clean\"].astype(str).tolist()\n",
    "#     labels_arr = np.array(labels)\n",
    "\n",
    "#     # TF-IDF\n",
    "#     vec = TfidfVectorizer(\n",
    "#         max_features=12000,\n",
    "#         stop_words=STOPWORDS,\n",
    "#         token_pattern=TOKEN_PATTERN,\n",
    "#         lowercase=True,\n",
    "#         max_df=0.92,\n",
    "#         min_df=2,\n",
    "#         strip_accents=\"unicode\"\n",
    "#     )\n",
    "#     X = vec.fit_transform(docs)\n",
    "\n",
    "#     # SVD → 2D\n",
    "#     svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "#     XY = svd.fit_transform(X)\n",
    "\n",
    "#     unique_labels = sorted(pd.unique(labels))\n",
    "#     pal = {k: palette.get(k, \"#999999\") for k in unique_labels}\n",
    "\n",
    "#     # --- Figure ---\n",
    "#     fig, ax = plt.subplots(figsize=(24, 16), dpi=110)\n",
    "\n",
    "#     # scatter points\n",
    "#     for pub in unique_labels:\n",
    "#         idx = np.where(labels_arr == pub)[0]\n",
    "#         ax.scatter(\n",
    "#             XY[idx, 0], XY[idx, 1],\n",
    "#             s=POINT_SIZE, alpha=POINT_ALPHA,\n",
    "#             label=pub if SHOW_LEGEND else None,\n",
    "#             color=pal.get(pub, \"#999999\"),\n",
    "#             edgecolors=\"white\", linewidths=0.2\n",
    "#         )\n",
    "\n",
    "#     # centroids\n",
    "#     if SHOW_CENTROIDS:\n",
    "#         for pub in unique_labels:\n",
    "#             idx = np.where(labels_arr == pub)[0]\n",
    "#             cx, cy = np.nanmean(XY[idx, 0]), np.nanmean(XY[idx, 1])\n",
    "#             ax.scatter(cx, cy, s=CENTROID_SIZE, marker=\"X\",\n",
    "#                        color=pal[pub], alpha=0.95,\n",
    "#                        edgecolors=\"black\", linewidths=0.6, zorder=5)\n",
    "#             if SHOW_CENTROID_LABELS:\n",
    "#                 ax.text(cx, cy, f\"  {pub}\", va=\"center\", ha=\"left\",\n",
    "#                         fontsize=14, weight=\"bold\", color=pal[pub], zorder=6)\n",
    "\n",
    "#     ax.set_title(f\"{TITLE_PREFIX} — {title_suffix}\", fontsize=22, pad=10)\n",
    "#     ax.set_xlabel(\"Component 1\", fontsize=15)\n",
    "#     ax.set_ylabel(\"Component 2\", fontsize=15)\n",
    "#     ax.grid(ls=\":\", alpha=.25)\n",
    "\n",
    "#     # Legend at bottom (manual, stable)\n",
    "#     if SHOW_LEGEND:\n",
    "#         handles = [Line2D([0],[0], marker=\"o\", linestyle=\"\",\n",
    "#                           markersize=8, markerfacecolor=pal.get(n,\"#999999\"),\n",
    "#                           markeredgecolor=\"white\", label=n)\n",
    "#                    for n in unique_labels]\n",
    "#         ncols = min(8, max(2, int(np.ceil(len(unique_labels)/3))))\n",
    "#         leg = ax.legend(handles=handles, title=\"Publication\",\n",
    "#                         ncol=ncols, loc=\"upper center\",\n",
    "#                         bbox_to_anchor=(0.5, -0.08), frameon=False,\n",
    "#                         fontsize=13, title_fontsize=13)\n",
    "#     else:\n",
    "#         leg = ax.get_legend()\n",
    "#         if leg: leg.remove()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.subplots_adjust(bottom=0.12)\n",
    "#     plt.show()\n",
    "\n",
    "# # -------------------------------------------------\n",
    "# # 4) Run: assemble corpus, then loop 5-year windows\n",
    "# # -------------------------------------------------\n",
    "# # Expect these DataFrames in memory:\n",
    "# # eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data\n",
    "# all_articles_text = assemble_all_articles(\n",
    "#     eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data\n",
    "# )\n",
    "\n",
    "# # Build palette for possible names we’ll see\n",
    "# all_names = sorted(all_articles_text[\"Publication_clean\"].unique().tolist())\n",
    "# global_palette = build_palette(all_names, master_palette, eastview_map)\n",
    "\n",
    "# # Loop windows\n",
    "# for s in range(START_YEAR, END_YEAR+1, WINDOW):\n",
    "#     e = min(s + WINDOW - 1, END_YEAR)\n",
    "#     mask = (all_articles_text[\"year\"] >= s) & (all_articles_text[\"year\"] <= e)\n",
    "#     df_w = all_articles_text.loc[mask].copy()\n",
    "\n",
    "#     # filter pubs with enough docs in window\n",
    "#     counts = df_w[\"Publication_clean\"].value_counts()\n",
    "#     keep_pubs = counts[counts >= MIN_DOCS_PER_PUB].index.tolist()\n",
    "#     df_w = df_w[df_w[\"Publication_clean\"].isin(keep_pubs)]\n",
    "\n",
    "#     n_docs = len(df_w)\n",
    "#     if n_docs < MIN_DOCS_TOTAL or len(keep_pubs) < 2:\n",
    "#         print(f\"[{s}–{e}] skipped — docs={n_docs}, pubs>={MIN_DOCS_PER_PUB}={len(keep_pubs)}\")\n",
    "#         continue\n",
    "\n",
    "#     print(f\"[{s}–{e}] plotting — docs={n_docs}, pubs={len(keep_pubs)}\")\n",
    "#     plot_svd_window(df_w, f\"{s}–{e}\", global_palette)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD before 2011 and after 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SVD comparison: BEFORE 2011 (≤2010) vs AFTER 2011 (≥2011)\n",
    "# - EV 11 + Zavtra + Tsargrad + RT + EIR\n",
    "# - Shared TF-IDF + SVD space for apples-to-apples\n",
    "# - Two period plots + centroid shift plot\n",
    "# =============================================\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "SPLIT_YEAR           = 2011     # compare ≤2010 vs ≥2011\n",
    "MIN_DOCS_TOTAL       = 100      # skip a plot if fewer docs overall\n",
    "MIN_DOCS_PER_PUB_PRE = 10       # require at least this many docs for a pub pre-2011\n",
    "MIN_DOCS_PER_PUB_POST= 10       # and post-2011\n",
    "MIN_CHARS_PER_DOC    = 200      # filter super-short texts\n",
    "\n",
    "POINT_SIZE   = 18\n",
    "POINT_ALPHA  = 0.45\n",
    "CENTROID_SZ  = 140\n",
    "\n",
    "SHOW_CENTROIDS        = True\n",
    "SHOW_CENTROID_LABELS  = False     # toggle labels on centroid markers\n",
    "SHOW_LEGEND           = True\n",
    "\n",
    "TITLE_PREFIX = \"SVD (LSA) — content similarity by publication\"\n",
    "\n",
    "# -------------------------\n",
    "# Helpers (palette + parsing)\n",
    "# -------------------------\n",
    "def first_present(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def pick_first_date(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                return dt\n",
    "    return pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "# Use your mappings if they exist; otherwise, safe defaults:\n",
    "eastview_map = globals().get(\"eastview_map\", {\n",
    "    \"Vedomosti\": \"vedomosti\",\n",
    "    \"Nezavisimaia gazeta\": \"nezavisimaia_gazeta\",\n",
    "    \"Trud\": \"trud\",\n",
    "    \"Время МН\": \"b_mh\",\n",
    "    \"Kommersant\": \"kommersant\",\n",
    "    \"Общая газета\": \"о_г\",\n",
    "    \"Sovetskaia Rossiia\": \"sovetskaia_rossiia\",\n",
    "    \"Novaia gazeta\": \"novaia_gazeta\",\n",
    "    \"Slovo\": \"slovo\",\n",
    "    \"Literaturnaia gazeta\": \"literaturnaia_gazeta\",\n",
    "    \"Pravda\": \"pravda\",\n",
    "})\n",
    "master_palette = globals().get(\"master_palette\", {\n",
    "    \"rt\":  \"#31a354\",\n",
    "    \"eir\": \"#3066a8\",\n",
    "    \"Zavtra\":   \"#5b31a3\",\n",
    "    \"Tsargrad\": \"#a35b31\",\n",
    "})\n",
    "\n",
    "def build_palette(names, master_palette, eastview_map=None):\n",
    "    pal = {}\n",
    "    for n in names:\n",
    "        key = eastview_map.get(n, n) if eastview_map else n\n",
    "        col = (master_palette.get(key) or\n",
    "               master_palette.get(str(key).lower()) or\n",
    "               \"#999999\")\n",
    "        pal[n] = col\n",
    "    return pal\n",
    "\n",
    "# Stronger stopword list\n",
    "EXTRA_STOPS = {\n",
    "    \"said\",\"say\",\"says\",\"mr\",\"ms\",\"mrs\",\"today\",\"yesterday\",\"tomorrow\",\n",
    "    \"news\",\"report\",\"reports\",\"reported\",\"according\",\"week\",\"weeks\",\"month\",\"months\",\n",
    "    \"year\",\"years\",\"daily\",\"update\",\"live\",\"video\",\"photo\",\"photos\",\"twitter\",\"facebook\",\n",
    "    \"gov\",\"govt\",\"via\",\"—\",\"–\",\"’\",\"“\",\"”\",\"amp\"\n",
    "}\n",
    "STOPWORDS = sorted(set(ENGLISH_STOP_WORDS) | EXTRA_STOPS)\n",
    "TOKEN_PATTERN = r\"(?u)\\b[a-zA-Z]{3,}\\b\"\n",
    "\n",
    "# -------------------------\n",
    "# Assemble articles (EV 11 + Zavtra + Tsargrad + RT + EIR)\n",
    "# Expect these to exist: eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data\n",
    "# -------------------------\n",
    "def assemble_all_articles(eastview_data, zavtra_data, tsargrad_data, rt_data=None, eir_data=None):\n",
    "    frames = []\n",
    "\n",
    "    # EastView\n",
    "    if eastview_data is not None and len(eastview_data):\n",
    "        ev = eastview_data.copy()\n",
    "        pub_col = first_present(ev, [\"Publication\",\"publication\"])\n",
    "        if pub_col is None:\n",
    "            ev[\"Publication_clean\"] = \"unknown\"\n",
    "        else:\n",
    "            ev[\"Publication_clean\"] = ev[pub_col].map(eastview_map).fillna(\n",
    "                ev[pub_col].astype(str).str.lower().str.replace(\" \", \"_\", regex=False)\n",
    "            )\n",
    "        if {\"Year\",\"Month\"}.issubset(ev.columns):\n",
    "            y = pd.to_numeric(ev[\"Year\"], errors=\"coerce\")\n",
    "            m = pd.to_numeric(ev[\"Month\"], errors=\"coerce\")\n",
    "            ev[\"year_month\"] = pd.to_datetime({\"year\": y, \"month\": m, \"day\": 1}, errors=\"coerce\")\n",
    "        else:\n",
    "            ev[\"year_month\"] = pick_first_date(\n",
    "                ev, [\"PublishedDate\",\"PublicationDate\",\"publication_date\",\"translated_date\",\"Date\",\"date\"]\n",
    "            ).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "        text_col = first_present(ev, [\"ArticleTextEnglish\",\"ArticleText\",\"article_text\",\"translated_article_excerpt\",\"content\"])\n",
    "        if text_col is None:\n",
    "            ev[\"__text\"] = pd.NA; text_col = \"__text\"\n",
    "        frames.append(pd.DataFrame({\n",
    "            \"Publication_clean\": ev[\"Publication_clean\"],\n",
    "            \"year_month\": ev[\"year_month\"],\n",
    "            \"text\": ev[text_col].astype(str)\n",
    "        }))\n",
    "\n",
    "    # Zavtra\n",
    "    if zavtra_data is not None and len(zavtra_data):\n",
    "        zv = zavtra_data.copy()\n",
    "        zv[\"Publication_clean\"] = \"Zavtra\"\n",
    "        zv_dt = pick_first_date(zv, [\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"])\n",
    "        zv[\"year_month\"] = zv_dt.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "        text_col = first_present(zv, [\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\",\"content\"])\n",
    "        if text_col is None:\n",
    "            zv[\"__text\"] = pd.NA; text_col = \"__text\"\n",
    "        frames.append(pd.DataFrame({\n",
    "            \"Publication_clean\": \"Zavtra\",\n",
    "            \"year_month\": zv[\"year_month\"],\n",
    "            \"text\": zv[text_col].astype(str)\n",
    "        }))\n",
    "\n",
    "    # Tsargrad\n",
    "    if tsargrad_data is not None and len(tsargrad_data):\n",
    "        ts = tsargrad_data.copy()\n",
    "        ts[\"Publication_clean\"] = \"Tsargrad\"\n",
    "        ts_dt = pick_first_date(ts, [\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"])\n",
    "        ts[\"year_month\"] = ts_dt.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "        text_col = first_present(ts, [\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\",\"content\"])\n",
    "        if text_col is None:\n",
    "            ts[\"__text\"] = pd.NA; text_col = \"__text\"\n",
    "        frames.append(pd.DataFrame({\n",
    "            \"Publication_clean\": \"Tsargrad\",\n",
    "            \"year_month\": ts[\"year_month\"],\n",
    "            \"text\": ts[text_col].astype(str)\n",
    "        }))\n",
    "\n",
    "    # RT\n",
    "    if rt_data is not None and len(rt_data):\n",
    "        rt = rt_data.copy()\n",
    "        rt[\"Publication_clean\"] = \"RT\"\n",
    "        rt_dt = pick_first_date(rt, [\"published_at\",\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"])\n",
    "        rt[\"year_month\"] = rt_dt.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "        text_col = first_present(rt, [\"content\",\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\"])\n",
    "        if text_col is None:\n",
    "            rt[\"__text\"] = pd.NA; text_col = \"__text\"\n",
    "        frames.append(pd.DataFrame({\n",
    "            \"Publication_clean\": \"RT\",\n",
    "            \"year_month\": rt[\"year_month\"],\n",
    "            \"text\": rt[text_col].astype(str)\n",
    "        }))\n",
    "\n",
    "    # EIR\n",
    "    if eir_data is not None and len(eir_data):\n",
    "        ei = eir_data.copy()\n",
    "        ei[\"Publication_clean\"] = \"EIR\"\n",
    "        if \"date\" in ei.columns:\n",
    "            ei_dt = pd.to_datetime(ei[\"date\"].astype(\"string\"), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "        else:\n",
    "            ei_dt = pick_first_date(ei, [\"published_at\",\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"])\n",
    "        ei[\"year_month\"] = ei_dt.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "        text_col = first_present(ei, [\"content\",\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\"])\n",
    "        if text_col is None:\n",
    "            ei[\"__text\"] = pd.NA; text_col = \"__text\"\n",
    "        frames.append(pd.DataFrame({\n",
    "            \"Publication_clean\": \"EIR\",\n",
    "            \"year_month\": ei[\"year_month\"],\n",
    "            \"text\": ei[text_col].astype(str)\n",
    "        }))\n",
    "\n",
    "    all_articles = pd.concat(frames, ignore_index=True)\n",
    "    all_articles[\"year\"] = pd.to_datetime(all_articles[\"year_month\"], errors=\"coerce\").dt.year\n",
    "    all_articles[\"text_len\"] = all_articles[\"text\"].astype(str).str.len()\n",
    "    all_articles = all_articles[(all_articles[\"text_len\"] >= MIN_CHARS_PER_DOC) & all_articles[\"year\"].notna()]\n",
    "    return all_articles[[\"Publication_clean\",\"year\",\"text\"]]\n",
    "\n",
    "# -------------------------\n",
    "# Plotters\n",
    "# -------------------------\n",
    "def plot_period_scatter(XY, labels_arr, mask, title_suffix, palette):\n",
    "    \"\"\"Scatter + centroids for a given boolean mask (pre or post).\"\"\"\n",
    "    idx = np.where(mask)[0]\n",
    "    if idx.size == 0:\n",
    "        print(f\"[{title_suffix}] no docs — skipping\")\n",
    "        return\n",
    "\n",
    "    labs = labels_arr[idx]\n",
    "    pubs = sorted(pd.unique(labs))\n",
    "    pal  = {p: palette.get(p, \"#999999\") for p in pubs}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(24, 16), dpi=110)\n",
    "\n",
    "    # points\n",
    "    for p in pubs:\n",
    "        sel = idx[labs == p]\n",
    "        ax.scatter(\n",
    "            XY[sel, 0], XY[sel, 1],\n",
    "            s=POINT_SIZE, alpha=POINT_ALPHA,\n",
    "            label=p if SHOW_LEGEND else None,\n",
    "            color=pal.get(p, \"#999999\"),\n",
    "            edgecolors=\"white\", linewidths=0.2\n",
    "        )\n",
    "\n",
    "    # centroids\n",
    "    if SHOW_CENTROIDS:\n",
    "        for p in pubs:\n",
    "            sel = idx[labs == p]\n",
    "            cx, cy = np.nanmean(XY[sel, 0]), np.nanmean(XY[sel, 1])\n",
    "            ax.scatter(cx, cy, s=CENTROID_SZ, marker=\"X\",\n",
    "                       color=pal[p], alpha=0.95,\n",
    "                       edgecolors=\"black\", linewidths=0.6, zorder=5)\n",
    "            if SHOW_CENTROID_LABELS:\n",
    "                ax.text(cx, cy, f\"  {p}\", va=\"center\", ha=\"left\",\n",
    "                        fontsize=14, weight=\"bold\", color=pal[p], zorder=6)\n",
    "\n",
    "    ax.set_title(f\"{TITLE_PREFIX} — {title_suffix}\", fontsize=22, pad=10)\n",
    "    ax.set_xlabel(\"Component 1\", fontsize=15)\n",
    "    ax.set_ylabel(\"Component 2\", fontsize=15)\n",
    "    ax.grid(ls=\":\", alpha=.25)\n",
    "\n",
    "    if SHOW_LEGEND:\n",
    "        handles = [Line2D([0],[0], marker=\"o\", linestyle=\"\",\n",
    "                          markersize=8, markerfacecolor=pal.get(n,\"#999999\"),\n",
    "                          markeredgecolor=\"white\", label=n)\n",
    "                   for n in pubs]\n",
    "        ncols = min(8, max(2, int(np.ceil(len(pubs)/3))))\n",
    "        ax.legend(handles=handles, title=\"Publication\", ncol=ncols,\n",
    "                  loc=\"upper center\", bbox_to_anchor=(0.5, -0.08), frameon=False,\n",
    "                  fontsize=13, title_fontsize=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.12)\n",
    "    plt.show()\n",
    "\n",
    "def plot_centroid_shift(XY, labels_arr, pre_mask, post_mask, palette,\n",
    "                        min_docs_pre=MIN_DOCS_PER_PUB_PRE, min_docs_post=MIN_DOCS_PER_PUB_POST):\n",
    "    \"\"\"Arrow plot from pre→post centroids (same SVD space).\"\"\"\n",
    "    pre_idx  = np.where(pre_mask)[0]\n",
    "    post_idx = np.where(post_mask)[0]\n",
    "    if pre_idx.size == 0 or post_idx.size == 0:\n",
    "        print(\"[Centroid shift] no docs in one of the periods — skipping\")\n",
    "        return\n",
    "\n",
    "    pre_labs  = labels_arr[pre_idx]\n",
    "    post_labs = labels_arr[post_idx]\n",
    "\n",
    "    pubs = sorted(set(pre_labs) | set(post_labs))\n",
    "    # keep only pubs with enough docs in BOTH periods\n",
    "    keep = []\n",
    "    for p in pubs:\n",
    "        n_pre  = (pre_labs  == p).sum()\n",
    "        n_post = (post_labs == p).sum()\n",
    "        if n_pre >= min_docs_pre and n_post >= min_docs_post:\n",
    "            keep.append(p)\n",
    "    if not keep:\n",
    "        print(\"[Centroid shift] no publications pass min doc thresholds — skipping\")\n",
    "        return\n",
    "\n",
    "    pal = {p: palette.get(p, \"#999999\") for p in keep}\n",
    "\n",
    "    # compute centroids\n",
    "    C_pre, C_post = {}, {}\n",
    "    for p in keep:\n",
    "        C_pre[p]  = XY[pre_idx[pre_labs  == p]].mean(axis=0)\n",
    "        C_post[p] = XY[post_idx[post_labs == p]].mean(axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(24, 16), dpi=110)\n",
    "\n",
    "    for p in keep:\n",
    "        (x0, y0) = C_pre[p]\n",
    "        (x1, y1) = C_post[p]\n",
    "        ax.scatter(x0, y0, s=CENTROID_SZ, marker=\"o\", color=pal[p],\n",
    "                   edgecolors=\"black\", linewidths=0.6, zorder=4)\n",
    "        ax.scatter(x1, y1, s=CENTROID_SZ, marker=\"X\", color=pal[p],\n",
    "                   edgecolors=\"black\", linewidths=0.6, zorder=5)\n",
    "        ax.annotate(\"\",\n",
    "                    xy=(x1, y1), xytext=(x0, y0),\n",
    "                    arrowprops=dict(arrowstyle=\"->\", lw=2, color=pal[p], alpha=0.9))\n",
    "\n",
    "        if SHOW_CENTROID_LABELS:\n",
    "            ax.text(x1, y1, f\"  {p}\", va=\"center\", ha=\"left\",\n",
    "                    fontsize=14, weight=\"bold\", color=pal[p], zorder=6)\n",
    "\n",
    "    ax.set_title(f\"{TITLE_PREFIX} — centroid shift (≤{SPLIT_YEAR-1} → ≥{SPLIT_YEAR})\", fontsize=22, pad=10)\n",
    "    ax.set_xlabel(\"Component 1\", fontsize=15)\n",
    "    ax.set_ylabel(\"Component 2\", fontsize=15)\n",
    "    ax.grid(ls=\":\", alpha=.25)\n",
    "\n",
    "    handles = [\n",
    "        Line2D([0],[0], marker=\"o\", linestyle=\"\", markersize=9, markerfacecolor=\"#777777\",\n",
    "               markeredgecolor=\"black\", label=\"Pre (≤{}) centroid\".format(SPLIT_YEAR-1)),\n",
    "        Line2D([0],[0], marker=\"X\", linestyle=\"\", markersize=9, markerfacecolor=\"#777777\",\n",
    "               markeredgecolor=\"black\", label=\"Post (≥{}) centroid\".format(SPLIT_YEAR)),\n",
    "    ] + [Line2D([0],[0], color=palette.get(p,\"#999999\"), lw=3, label=p) for p in keep]\n",
    "\n",
    "    ncols = min(8, max(2, int(np.ceil(len(keep)/3))))\n",
    "    ax.legend(handles=handles, title=\"Shift & Publications\",\n",
    "              ncol=ncols, loc=\"upper center\", bbox_to_anchor=(0.5, -0.10),\n",
    "              frameon=False, fontsize=13, title_fontsize=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.14)\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Build corpus → shared TF-IDF + SVD → plot\n",
    "# -------------------------\n",
    "# Expect these in memory: eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data\n",
    "all_articles_text = assemble_all_articles(\n",
    "    eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data\n",
    ")\n",
    "\n",
    "# palette across all seen names\n",
    "all_names = sorted(all_articles_text[\"Publication_clean\"].unique().tolist())\n",
    "global_palette = build_palette(all_names, master_palette, eastview_map)\n",
    "\n",
    "# split masks\n",
    "pre_mask_all  = all_articles_text[\"year\"] <= (SPLIT_YEAR - 1)\n",
    "post_mask_all = all_articles_text[\"year\"] >= SPLIT_YEAR\n",
    "\n",
    "# keep publications with enough docs per period\n",
    "counts_pre  = all_articles_text.loc[pre_mask_all,  \"Publication_clean\"].value_counts()\n",
    "counts_post = all_articles_text.loc[post_mask_all, \"Publication_clean\"].value_counts()\n",
    "keep_pubs = sorted(set(counts_pre[counts_pre >= MIN_DOCS_PER_PUB_PRE].index)\n",
    "                   & set(counts_post[counts_post >= MIN_DOCS_PER_PUB_POST].index))\n",
    "if not keep_pubs:\n",
    "    print(\"No publications pass the per-period minimum doc thresholds—lower MIN_DOCS_PER_PUB_* or widen the years.\")\n",
    "\n",
    "# filter to those pubs\n",
    "mask_keep = all_articles_text[\"Publication_clean\"].isin(keep_pubs)\n",
    "df = all_articles_text.loc[mask_keep].copy()\n",
    "\n",
    "# sanity totals\n",
    "n_pre  = pre_mask_all[mask_keep].sum()\n",
    "n_post = post_mask_all[mask_keep].sum()\n",
    "print(f\"PRE (≤{SPLIT_YEAR-1}): {n_pre} docs across {len(keep_pubs)} pubs\")\n",
    "print(f\"POST(≥{SPLIT_YEAR}): {n_post} docs across {len(keep_pubs)} pubs\")\n",
    "\n",
    "if (n_pre + n_post) >= MIN_DOCS_TOTAL and len(keep_pubs) >= 2:\n",
    "    # vectorize ALL docs together -> shared space\n",
    "    docs   = df[\"text\"].astype(str).tolist()\n",
    "    labels = df[\"Publication_clean\"].astype(str).tolist()\n",
    "    years  = df[\"year\"].astype(int).values\n",
    "\n",
    "    vec = TfidfVectorizer(\n",
    "        max_features=12000,\n",
    "        stop_words=STOPWORDS,\n",
    "        token_pattern=TOKEN_PATTERN,\n",
    "        lowercase=True,\n",
    "        max_df=0.92,\n",
    "        min_df=2,\n",
    "        strip_accents=\"unicode\"\n",
    "    )\n",
    "    X = vec.fit_transform(docs)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    XY  = svd.fit_transform(X)\n",
    "\n",
    "    labels_arr = np.array(labels)\n",
    "    pre_mask   = years <= (SPLIT_YEAR - 1)\n",
    "    post_mask  = years >= SPLIT_YEAR\n",
    "\n",
    "    # (A) pre-2011 plot\n",
    "    plot_period_scatter(XY, labels_arr, pre_mask,\n",
    "                        title_suffix=f\"Pre {SPLIT_YEAR} (≤{SPLIT_YEAR-1})\",\n",
    "                        palette=global_palette)\n",
    "\n",
    "    # (B) post-2011 plot\n",
    "    plot_period_scatter(XY, labels_arr, post_mask,\n",
    "                        title_suffix=f\"Post {SPLIT_YEAR} (≥{SPLIT_YEAR})\",\n",
    "                        palette=global_palette)\n",
    "\n",
    "    # (C) centroid shift (same space)\n",
    "    plot_centroid_shift(XY, labels_arr, pre_mask, post_mask, global_palette)\n",
    "else:\n",
    "    print(\"Not enough total docs or publications—adjust thresholds or ranges and re-run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def get_text_series(df):\n",
    "    for c in [\"ArticleTextEnglish\",\"ArticleText\",\"article_text\",\"translated_article_excerpt\",\"content\"]:\n",
    "        if c in df.columns:\n",
    "            return df[c].astype(str).fillna(\"\")\n",
    "    return pd.Series([], dtype=str)\n",
    "\n",
    "def pick_first_present(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def build_palette_from_mapping(levels, master_palette, eastview_map=None):\n",
    "    pal = {}\n",
    "    for name in levels:\n",
    "        key = eastview_map.get(name, name) if eastview_map else name\n",
    "        color = master_palette.get(key) or master_palette.get(str(key).lower(), \"#999999\")\n",
    "        pal[name] = color\n",
    "    return pal\n",
    "\n",
    "def ensure_bottom_legend(ax, names, palette, title=\"Publication\", ncols=6, y_offset=-0.10, fontsize=13):\n",
    "    # Manual legend so every pub shows even if some have very few points\n",
    "    handles = [Line2D([0],[0], marker='o', lw=0, ms=8,\n",
    "                      color=palette.get(n, \"#999999\"), label=n) for n in names]\n",
    "    leg = ax.legend(handles=handles, title=title, ncol=ncols,\n",
    "                    loc=\"upper center\", bbox_to_anchor=(0.5, y_offset),\n",
    "                    frameon=False, fontsize=fontsize)\n",
    "    leg.get_title().set_fontsize(fontsize)\n",
    "    return leg\n",
    "\n",
    "# Your EV display->palette key mapping (extend as needed)\n",
    "eastview_map = {\n",
    "    \"Vedomosti\": \"vedomosti\",\n",
    "    \"Nezavisimaia gazeta\": \"nezavisimaia_gazeta\",\n",
    "    \"Trud\": \"trud\",\n",
    "    \"Время МН\": \"b_mh\",\n",
    "    \"Kommersant\": \"kommersant\",\n",
    "    \"Общая газета\": \"о_г\",\n",
    "    \"Sovetskaia Rossiia\": \"sovetskaia_rossiia\",\n",
    "    \"Novaia gazeta\": \"novaia_gazeta\",\n",
    "    \"Slovo\": \"slovo\",\n",
    "    \"Literaturnaia gazeta\": \"literaturnaia_gazeta\",\n",
    "    \"Pravda\": \"pravda\",\n",
    "}\n",
    "\n",
    "# ---------- build a full corpus: EV subs + Zavtra + Tsargrad + RT + EIR ----------\n",
    "def assemble_corpus_for_svd(eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data):\n",
    "    frames = []\n",
    "\n",
    "    # EastView: one \"pub\" per Publication value\n",
    "    if \"Publication\" in eastview_data.columns:\n",
    "        for pub in eastview_data[\"Publication\"].dropna().unique():\n",
    "            sub = eastview_data.loc[eastview_data[\"Publication\"] == pub]\n",
    "            txt = get_text_series(sub)\n",
    "            if len(txt):\n",
    "                frames.append(pd.DataFrame({\"pub\": pub, \"text\": txt}))\n",
    "\n",
    "    # Single-title sources\n",
    "    if zavtra_data is not None and len(zavtra_data):\n",
    "        frames.append(pd.DataFrame({\"pub\": \"Zavtra\", \"text\": get_text_series(zavtra_data)}))\n",
    "    if tsargrad_data is not None and len(tsargrad_data):\n",
    "        frames.append(pd.DataFrame({\"pub\": \"Tsargrad\", \"text\": get_text_series(tsargrad_data)}))\n",
    "    if rt_data is not None and len(rt_data):\n",
    "        frames.append(pd.DataFrame({\"pub\": \"RT\", \"text\": get_text_series(rt_data)}))\n",
    "    if eir_data is not None and len(eir_data):\n",
    "        frames.append(pd.DataFrame({\"pub\": \"EIR\", \"text\": get_text_series(eir_data)}))\n",
    "\n",
    "    corpus = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=[\"pub\",\"text\"])\n",
    "    corpus[\"text\"] = corpus[\"text\"].astype(str).fillna(\"\").str.strip()\n",
    "    corpus = corpus[corpus[\"text\"].str.len() > 0].reset_index(drop=True)\n",
    "    return corpus\n",
    "\n",
    "corpus = assemble_corpus_for_svd(eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data)\n",
    "print(\"Docs by publication:\\n\", corpus[\"pub\"].value_counts())\n",
    "\n",
    "# ---------- TF-IDF (custom stopwords) ----------\n",
    "EXTRA_STOPS = {\n",
    "    \"said\",\"say\",\"says\",\"mr\",\"ms\",\"mrs\",\"today\",\"yesterday\",\"tomorrow\",\n",
    "    \"news\",\"report\",\"reports\",\"reported\",\"according\",\"week\",\"weeks\",\"month\",\"months\",\n",
    "    \"year\",\"years\",\"daily\",\"update\",\"live\",\"video\",\"photo\",\"photos\",\"twitter\",\"facebook\",\n",
    "    \"gov\",\"govt\",\"via\",\"—\",\"–\",\"’\",\"“\",\"”\",\"amp\"\n",
    "}\n",
    "STOPWORDS = sorted(set(ENGLISH_STOP_WORDS) | EXTRA_STOPS)\n",
    "TOKEN_PATTERN = r\"(?u)\\b[a-zA-Z]{3,}\\b\"\n",
    "\n",
    "vec = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    stop_words=STOPWORDS,\n",
    "    token_pattern=TOKEN_PATTERN,\n",
    "    lowercase=True,\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "X = vec.fit_transform(corpus[\"text\"])\n",
    "labels = corpus[\"pub\"].to_numpy()\n",
    "terms = vec.get_feature_names_out()\n",
    "\n",
    "# ---------- SVD (2D embedding) ----------\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "XY = svd.fit_transform(X)\n",
    "\n",
    "# Colors from your master palette (+ eastview_map for EV subs)\n",
    "all_levels = sorted(pd.unique(labels))\n",
    "pal_all = build_palette_from_mapping(all_levels, master_palette, eastview_map)\n",
    "\n",
    "# ---------- (A) ONE PLOT: all publications together ----------\n",
    "plt.figure(figsize=(12, 14), dpi=120)\n",
    "for pub in all_levels:\n",
    "    idx = np.where(labels == pub)[0]\n",
    "    if len(idx) == 0: \n",
    "        continue\n",
    "    plt.scatter(XY[idx, 0], XY[idx, 1],\n",
    "                s=12, alpha=0.55, label=pub,\n",
    "                color=pal_all.get(pub, \"#999999\"), edgecolors=\"none\")\n",
    "plt.title(\"SVD (LSA) document scatter — ALL publications\", fontsize=22, pad=12)\n",
    "plt.xlabel(\"Component 1\", fontsize=16)\n",
    "plt.ylabel(\"Component 2\", fontsize=16)\n",
    "plt.grid(ls=\":\", alpha=.25)\n",
    "\n",
    "# bottom legend, multi-column\n",
    "ncols = min(8, max(3, int(np.ceil(len(all_levels) / 4))))\n",
    "ensure_bottom_legend(plt.gca(), all_levels, pal_all, ncols=ncols, y_offset=-0.08, fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.12)\n",
    "plt.show()\n",
    "\n",
    "# ---------- (B) MANY PLOTS: one per publication (same embedding & axes) ----------\n",
    "# Keep global axis limits so small-multiples are comparable\n",
    "pad = 0.02\n",
    "x_min, x_max = XY[:,0].min(), XY[:,0].max()\n",
    "y_min, y_max = XY[:,1].min(), XY[:,1].max()\n",
    "xr = x_max - x_min; yr = y_max - y_min\n",
    "xlim = (x_min - xr*pad, x_max + xr*pad)\n",
    "ylim = (y_min - yr*pad, y_max + yr*pad)\n",
    "\n",
    "for pub in all_levels:\n",
    "    idx = np.where(labels == pub)[0]\n",
    "    if len(idx) == 0:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(12, 9), dpi=120)\n",
    "    # faint background of all docs for context\n",
    "    plt.scatter(XY[:, 0], XY[:, 1], s=6, alpha=0.10, color=\"#CCCCCC\", edgecolors=\"none\")\n",
    "    # highlight this publication\n",
    "    plt.scatter(XY[idx, 0], XY[idx, 1],\n",
    "                s=16, alpha=0.70,\n",
    "                color=pal_all.get(pub, \"#999999\"), edgecolors=\"none\", label=pub)\n",
    "\n",
    "    plt.xlim(xlim); plt.ylim(ylim)\n",
    "    plt.title(f\"SVD (LSA) — {pub}\", fontsize=18, pad=10)\n",
    "    plt.xlabel(\"Component 1\"); plt.ylabel(\"Component 2\")\n",
    "    plt.grid(ls=\":\", alpha=.25)\n",
    "\n",
    "    # single legend (just the pub), under the chart\n",
    "    ensure_bottom_legend(plt.gca(), [pub], pal_all, ncols=1, y_offset=-0.10, fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.12)\n",
    "    plt.show()\n",
    "\n",
    "# ---------- (optional) Top TF-IDF terms per publication ----------\n",
    "top_terms = {}\n",
    "for pub in all_levels:\n",
    "    idx = np.where(labels == pub)[0]\n",
    "    if len(idx) == 0:\n",
    "        continue\n",
    "    mean_vec = X[idx].mean(axis=0).A1\n",
    "    top_idx = mean_vec.argsort()[-20:][::-1]\n",
    "    top_terms[pub] = pd.Series(mean_vec[top_idx], index=terms[top_idx])\n",
    "    print(f\"\\nTop TF-IDF terms — {pub}\\n\", top_terms[pub].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5OcIx2k0vYU"
   },
   "source": [
    "### `probability` soros-bias score (0–1) and ArticleTextEnglish is the full article\n",
    "(a) filter/compare by bias\n",
    "(b) see distributions\n",
    "(c) use the bias in SVD scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1759291336026,
     "user": {
      "displayName": "Te' Claire",
      "userId": "06350349370332120081"
     },
     "user_tz": -600
    },
    "id": "lrEFFHfY0spy",
    "outputId": "6a66a7c6-3579-4789-98c3-0833eaaf2296"
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def assemble_all_articles(eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data):\n",
    "    frames = []\n",
    "\n",
    "    # ---- EastView (many pubs) ----\n",
    "    ev = eastview_data.copy()\n",
    "    pub_col = pick_first_present(ev, [\"Publication\", \"publication\"])\n",
    "    ev[\"Publication_clean\"] = (\n",
    "        \"unknown\" if pub_col is None else\n",
    "        ev[pub_col].map(eastview_map).fillna(\n",
    "            ev[pub_col].astype(str).str.lower().str.replace(\" \", \"_\", regex=False)\n",
    "        )\n",
    "    )\n",
    "    ev[\"probability\"] = pd.to_numeric(ev.get(\"probability\", np.nan), errors=\"coerce\")\n",
    "    if {\"Year\", \"Month\"}.issubset(ev.columns):\n",
    "        y = pd.to_numeric(ev[\"Year\"], errors=\"coerce\")\n",
    "        m = pd.to_numeric(ev[\"Month\"], errors=\"coerce\")\n",
    "        ev[\"year_month\"] = pd.to_datetime({\"year\": y, \"month\": m, \"day\": 1}, errors=\"coerce\")\n",
    "    else:\n",
    "        ev[\"year_month\"] = pick_first_date(\n",
    "            ev, [\"PublishedDate\",\"PublicationDate\",\"publication_date\",\"translated_date\",\"Date\",\"date\"]\n",
    "        ).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    ev_text_col  = pick_first_present(ev, [\"ArticleTextEnglish\",\"ArticleText\",\"article_text\",\"translated_article_excerpt\",\"content\"])\n",
    "    ev_title_col = pick_first_present(ev, [\"ArticleTitle\",\"title\",\"translated_title\",\"headline\"])\n",
    "    if ev_text_col is None:  ev[\"__text\"]  = pd.NA\n",
    "    if ev_title_col is None: ev[\"__title\"] = pd.NA\n",
    "    ev_out = pd.DataFrame({\n",
    "        \"Publication_clean\": ev[\"Publication_clean\"],\n",
    "        \"year_month\": ev[\"year_month\"],\n",
    "        \"probability\": ev[\"probability\"],\n",
    "        \"text\": ev[ev_text_col] if ev_text_col else ev[\"__text\"],\n",
    "        \"title\": ev[ev_title_col] if ev_title_col else ev[\"__title\"],\n",
    "        \"ArticleID\": ev.get(\"ArticleID\", pd.Series([pd.NA]*len(ev)))\n",
    "    }).dropna(subset=[\"probability\"])\n",
    "    frames.append(ev_out)\n",
    "\n",
    "    # ---- Zavtra ----\n",
    "    zv = zavtra_data.copy()\n",
    "    zv[\"Publication_clean\"] = \"Zavtra\"\n",
    "    zv[\"probability\"] = pd.to_numeric(zv.get(\"probability\", np.nan), errors=\"coerce\")\n",
    "    zv[\"year_month\"] = pick_first_date(\n",
    "        zv, [\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"]\n",
    "    ).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    zv_text_col  = pick_first_present(zv, [\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\",\"content\"])\n",
    "    zv_title_col = pick_first_present(zv, [\"title\",\"translated_title\",\"headline\"])\n",
    "    if zv_text_col is None:  zv[\"__text\"]  = pd.NA\n",
    "    if zv_title_col is None: zv[\"__title\"] = pd.NA\n",
    "    zv_out = pd.DataFrame({\n",
    "        \"Publication_clean\": \"Zavtra\",\n",
    "        \"year_month\": zv[\"year_month\"],\n",
    "        \"probability\": zv[\"probability\"],\n",
    "        \"text\": zv[zv_text_col] if zv_text_col else zv[\"__text\"],\n",
    "        \"title\": zv[zv_title_col] if zv_title_col else zv[\"__title\"],\n",
    "        \"ArticleID\": zv.get(\"ArticleID\", pd.Series([pd.NA]*len(zv)))\n",
    "    }).dropna(subset=[\"probability\"])\n",
    "    frames.append(zv_out)\n",
    "\n",
    "    # ---- Tsargrad ----\n",
    "    ts = tsargrad_data.copy()\n",
    "    ts[\"Publication_clean\"] = \"Tsargrad\"\n",
    "    ts[\"probability\"] = pd.to_numeric(ts.get(\"probability\", np.nan), errors=\"coerce\")\n",
    "    ts[\"year_month\"] = pick_first_date(\n",
    "        ts, [\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"]\n",
    "    ).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    ts_text_col  = pick_first_present(ts, [\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\",\"content\"])\n",
    "    ts_title_col = pick_first_present(ts, [\"title\",\"translated_title\",\"headline\"])\n",
    "    if ts_text_col is None:  ts[\"__text\"]  = pd.NA\n",
    "    if ts_title_col is None: ts[\"__title\"] = pd.NA\n",
    "    ts_out = pd.DataFrame({\n",
    "        \"Publication_clean\": \"Tsargrad\",\n",
    "        \"year_month\": ts[\"year_month\"],\n",
    "        \"probability\": ts[\"probability\"],\n",
    "        \"text\": ts[ts_text_col] if ts_text_col else ts[\"__text\"],\n",
    "        \"title\": ts[ts_title_col] if ts_title_col else ts[\"__title\"],\n",
    "        \"ArticleID\": ts.get(\"ArticleID\", pd.Series([pd.NA]*len(ts)))\n",
    "    }).dropna(subset=[\"probability\"])\n",
    "    frames.append(ts_out)\n",
    "\n",
    "    # ---- RT ----\n",
    "    if rt_data is not None and len(rt_data):\n",
    "        rt = rt_data.copy()\n",
    "        rt[\"Publication_clean\"] = \"RT\"\n",
    "        rt[\"probability\"] = pd.to_numeric(rt.get(\"probability\", np.nan), errors=\"coerce\")\n",
    "        rt[\"year_month\"] = pick_first_date(\n",
    "            rt, [\"published_at\",\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"]\n",
    "        ).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "        rt_text_col  = pick_first_present(rt, [\"content\",\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\"])\n",
    "        rt_title_col = pick_first_present(rt, [\"title\",\"headline\"])\n",
    "        if rt_text_col is None:  rt[\"__text\"]  = pd.NA\n",
    "        if rt_title_col is None: rt[\"__title\"] = pd.NA\n",
    "        rt = rt.dropna(subset=[\"probability\"]) \n",
    "        rt_out = pd.DataFrame({\n",
    "            \"Publication_clean\": \"RT\",\n",
    "            \"year_month\": rt[\"year_month\"],\n",
    "            \"probability\": rt[\"probability\"],\n",
    "            \"text\": rt[rt_text_col] if rt_text_col else rt[\"__text\"],\n",
    "            \"title\": rt[rt_title_col] if rt_title_col else rt[\"__title\"],\n",
    "            \"ArticleID\": rt.get(\"ArticleID\", pd.Series([pd.NA]*len(rt)))\n",
    "        })\n",
    "        frames.append(rt_out)\n",
    "\n",
    "    # ---- EIR ----\n",
    "    if eir_data is not None and len(eir_data):\n",
    "        ei = eir_data.copy()\n",
    "        ei[\"Publication_clean\"] = \"EIR\"\n",
    "        ei[\"probability\"] = pd.to_numeric(ei.get(\"probability\", np.nan), errors=\"coerce\")\n",
    "        ei[\"year_month\"] = pick_first_date(\n",
    "            ei, [\"published_at\",\"PublicationDate\",\"publication_date\",\"translated_date\",\"PublishedDate\",\"Date\",\"date\"]\n",
    "        ).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "        ei_text_col  = pick_first_present(ei, [\"content\",\"ArticleTextEnglish\",\"article_text\",\"translated_article_excerpt\"])\n",
    "        ei_title_col = pick_first_present(ei, [\"title\",\"headline\"])\n",
    "        if ei_text_col is None:  ei[\"__text\"]  = pd.NA\n",
    "        if ei_title_col is None: ei[\"__title\"] = pd.NA\n",
    "        ei = ei.dropna(subset=[\"probability\"])\n",
    "        ei_out = pd.DataFrame({\n",
    "            \"Publication_clean\": \"EIR\",\n",
    "            \"year_month\": ei[\"year_month\"],\n",
    "            \"probability\": ei[\"probability\"],\n",
    "            \"text\": ei[ei_text_col] if ei_text_col else ei[\"__text\"],\n",
    "            \"title\": ei[ei_title_col] if ei_title_col else ei[\"__title\"],\n",
    "            \"ArticleID\": ei.get(\"ArticleID\", pd.Series([pd.NA]*len(ei)))\n",
    "        })\n",
    "        frames.append(ei_out)\n",
    "\n",
    "    all_articles = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # 3-bin bucket for bias (optional)\n",
    "    all_articles[\"bias_bin\"] = pd.cut(\n",
    "        all_articles[\"probability\"],\n",
    "        bins=[0, 0.33, 0.66, 1.000001],\n",
    "        labels=[\"low\",\"mid\",\"high\"]\n",
    "    )\n",
    "    return all_articles\n",
    "\n",
    "# Build the dataset FIRST\n",
    "all_articles = assemble_all_articles(eastview_data, zavtra_data, tsargrad_data, rt_data, eir_data)\n",
    "\n",
    "# Sanity check for RT/EIR presence\n",
    "kde_df = all_articles.dropna(subset=[\"probability\",\"Publication_clean\"])\n",
    "# print(kde_df[\"Publication_clean\"].value_counts().reindex([\"RT\",\"EIR\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_palette_from_mapping(levels, master_palette, eastview_map=None):\n",
    "    pal = {}\n",
    "    for name in levels:\n",
    "        key = eastview_map.get(name, name) if eastview_map else name\n",
    "        # try exact then lowercase (handles RT/EIR -> \"rt\"/\"eir\")\n",
    "        color = master_palette.get(key) or master_palette.get(str(key).lower(), \"#999999\")\n",
    "        pal[name] = color\n",
    "    return pal\n",
    "\n",
    "def ensure_bottom_legend(ax, names, palette, title=\"Publication\", ncols=6, y_offset=-0.12, fontsize=14):\n",
    "    leg = ax.get_legend()\n",
    "    if leg is not None:\n",
    "        try:\n",
    "            from seaborn import move_legend\n",
    "            move_legend(ax, \"upper center\", bbox_to_anchor=(0.5, y_offset),\n",
    "                        ncol=ncols, frameon=False, title=title)\n",
    "            leg = ax.get_legend()\n",
    "            for t in leg.get_texts(): t.set_fontsize(fontsize)\n",
    "            leg.get_title().set_fontsize(fontsize)\n",
    "            return leg\n",
    "        except Exception:\n",
    "            pass\n",
    "    handles = [Line2D([0],[0], color=palette.get(n, \"#999999\"), lw=3, label=n) for n in names]\n",
    "    leg = ax.legend(handles=handles, title=title, ncol=ncols,\n",
    "                    loc=\"upper center\", bbox_to_anchor=(0.5, y_offset),\n",
    "                    frameon=False, fontsize=fontsize)\n",
    "    leg.get_title().set_fontsize(fontsize)\n",
    "    return leg\n",
    "\n",
    "# KDE of bias probability by publication (with RT & EIR)\n",
    "kde_df  = all_articles.dropna(subset=[\"probability\",\"Publication_clean\"]).copy()\n",
    "levels  = sorted(kde_df[\"Publication_clean\"].unique().tolist())\n",
    "pal_kde = build_palette_from_mapping(levels, master_palette, eastview_map)\n",
    "\n",
    "plt.figure(figsize=(28, 12))\n",
    "ax = sns.kdeplot(\n",
    "    data=kde_df,\n",
    "    x=\"probability\",\n",
    "    hue=\"Publication_clean\",\n",
    "    hue_order=levels,\n",
    "    palette=pal_kde,\n",
    "    common_norm=False,\n",
    "    bw_adjust=1.0,\n",
    "    linewidth=2.5,\n",
    "    fill=False,\n",
    "    legend=True\n",
    ")\n",
    "ax.set_title(\"Bias score (probability) distribution by publication\")\n",
    "ax.set_xlabel(\"Soros bias probability\"); ax.set_ylabel(\"Density\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=.35)\n",
    "\n",
    "ncols = min(8, max(2, int(np.ceil(len(levels)/3))))\n",
    "ensure_bottom_legend(ax, levels, pal_kde, ncols=ncols, y_offset=-0.12, fontsize=14)\n",
    "plt.tight_layout(); plt.subplots_adjust(bottom=0.18); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1DphRNfIBMwDb8X8CgcR9_oXIg3-gv3wY",
     "timestamp": 1726841605600
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
