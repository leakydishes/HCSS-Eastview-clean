{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import FileLink, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Expand display settings in pandas SHOW ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show ALL rows & columns, and don’t truncate text\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)  # <- key for full text\n",
    "pd.set_option(\"display.width\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Expand display settings in pandas SHOW LIMITED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"display.max_rows\")\n",
    "pd.reset_option(\"display.max_columns\")\n",
    "pd.reset_option(\"display.max_colwidth\")\n",
    "pd.reset_option(\"display.width\")\n",
    "# pd.reset_option(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Check articleID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV\n",
    "# Eastview db\n",
    "# df = pd.read_csv(\"soros_21092025_eastview.csv\")\n",
    "#----------------------------------\n",
    "# Tsargard\n",
    "df = pd.read_csv(\"tsargrad_soros_translated_21092025.csv\")\n",
    "# df = pd.read_csv(\"tsargrad_soros_translated_21092025_new.csv\")\n",
    "#----------------------------------\n",
    "# Zavtra db\n",
    "# df = pd.read_csv(\"zavtra_soros_translated_01092025.csv\")\n",
    "#----------------------------------\n",
    "\n",
    "\n",
    "# Filter rows where score is empty (NaN or blank string)\n",
    "missing_score = df[df[\"score\"].isna() | (df[\"score\"].astype(str).str.strip() == \"\")]\n",
    "\n",
    "# Display the first few rows (with all headers)\n",
    "# print(missing_score.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## How many records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total records:\", len(df))\n",
    "\n",
    "# If you also want to see how many are missing score:\n",
    "missing_mask = df[\"score\"].isna() | (df[\"score\"].astype(str).str.strip() == \"\")\n",
    "print(\"Records missing score:\", missing_mask.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Eastview join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def has_value(x):\n",
    "    return (x is not None) and (not pd.isna(x)) and (str(x).strip().lower() not in {\"\", \"nan\", \"none\", \"null\"})\n",
    "\n",
    "def norm_url(s):\n",
    "    if pd.isna(s):\n",
    "        return s\n",
    "    return str(s).strip().rstrip(\"/\")\n",
    "\n",
    "def choose_best_text(row, text_cols):\n",
    "    \"\"\"\n",
    "    Choose the 'best' text to use for ArticleTextEnglish when a score/prob exists.\n",
    "    Strategy: pick the longest non-empty among ArticleTextEnglish, ArticleTextEnglish1..18.\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    best_len = -1\n",
    "    for c in text_cols:\n",
    "        if c in row and has_value(row[c]):\n",
    "            s = str(row[c]).strip()\n",
    "            if len(s) > best_len:\n",
    "                best = s\n",
    "                best_len = len(s)\n",
    "    return best\n",
    "\n",
    "def merge_scores_with_texts(base_csv, scored_csv, out_csv, key=\"ArticleLink\"):\n",
    "    # 1) Load\n",
    "    df = pd.read_csv(base_csv)\n",
    "    dfu = pd.read_csv(scored_csv)\n",
    "\n",
    "    # 2) Normalize key\n",
    "    if key not in df.columns or key not in dfu.columns:\n",
    "        raise KeyError(f\"'{key}' must exist in both CSVs.\")\n",
    "    df[key] = df[key].apply(norm_url)\n",
    "    dfu[key] = dfu[key].apply(norm_url)\n",
    "\n",
    "    # 3) Keep only scored rows with usable values\n",
    "    if \"score\" not in dfu.columns or \"probability\" not in dfu.columns:\n",
    "        raise KeyError(\"Scored file must contain 'score' and 'probability' columns.\")\n",
    "    dfu = dfu[dfu[\"score\"].apply(has_value) & dfu[\"probability\"].apply(has_value)].copy()\n",
    "\n",
    "    # 4) Identify auxiliary columns to bring over\n",
    "    text_cols_all = [\"ArticleTextEnglish\"] + [f\"ArticleTextEnglish{i}\" for i in range(1, 19)]\n",
    "    aux_candidates = text_cols_all + [\"Term\", \"term\", \"word\", \"Word\"]\n",
    "    aux_cols_present = [c for c in aux_candidates if c in dfu.columns]\n",
    "\n",
    "    # 5) If duplicates on key in scored file, keep highest probability\n",
    "    dfu[\"probability_num\"] = pd.to_numeric(dfu[\"probability\"], errors=\"coerce\")\n",
    "    dfu = (dfu.sort_values(\"probability_num\", ascending=False)\n",
    "              .drop_duplicates(subset=[key], keep=\"first\"))\n",
    "\n",
    "    # 6) Build a 'best_text' in scored rows for updating ArticleTextEnglish\n",
    "    dfu[\"_best_text\"] = dfu.apply(lambda r: choose_best_text(r, text_cols_all), axis=1)\n",
    "\n",
    "    # 7) Merge: pull score/prob + aux columns\n",
    "    bring_cols = [key, \"score\", \"probability\", \"_best_text\"] + aux_cols_present\n",
    "    merged = df.merge(dfu[bring_cols], on=key, how=\"left\", suffixes=(\"\", \"_new\"))\n",
    "\n",
    "    # 8) Coalesce score/probability into base\n",
    "    if \"score_new\" in merged.columns:\n",
    "        merged[\"score\"] = merged[\"score\"].where(merged[\"score\"].apply(has_value), merged[\"score_new\"])\n",
    "        merged.drop(columns=[\"score_new\"], inplace=True)\n",
    "    if \"probability_new\" in merged.columns:\n",
    "        merged[\"probability\"] = merged[\"probability\"].where(merged[\"probability\"].apply(has_value), merged[\"probability_new\"])\n",
    "        merged.drop(columns=[\"probability_new\"], inplace=True)\n",
    "\n",
    "    # 9) Update base ArticleTextEnglish WHEN a score/probability exists and we have a best_text\n",
    "    #    (This \"updates\" the original text field as requested.)\n",
    "    if \"ArticleTextEnglish\" not in merged.columns:\n",
    "        merged[\"ArticleTextEnglish\"] = pd.NA\n",
    "    cond_has_scored = merged[\"score\"].apply(has_value) & merged[\"probability\"].apply(has_value)\n",
    "    merged.loc[cond_has_scored & merged[\"_best_text\"].apply(has_value), \"ArticleTextEnglish\"] = merged.loc[\n",
    "        cond_has_scored, \"_best_text\"\n",
    "    ]\n",
    "\n",
    "    # 10) Ensure all aux columns exist in final output.\n",
    "    # For any aux col:\n",
    "    #   - if the base already has it, only fill missing values from the scored version (col_new)\n",
    "    #   - if the base doesn't have it, create/rename from the scored version\n",
    "    for c in aux_cols_present:\n",
    "        new_col = f\"{c}_new\"\n",
    "        if new_col in merged.columns and c in merged.columns:\n",
    "            merged[c] = merged[c].where(merged[c].apply(has_value), merged[new_col])\n",
    "            merged.drop(columns=[new_col], inplace=True)\n",
    "        elif new_col in merged.columns and c not in merged.columns:\n",
    "            merged.rename(columns={new_col: c}, inplace=True)\n",
    "\n",
    "    # 11) Drop helper columns\n",
    "    if \"_best_text\" in merged.columns:\n",
    "        merged.drop(columns=[\"_best_text\"], inplace=True)\n",
    "\n",
    "    # 12) Optional: drop rows still missing score/probability (as per previous step)\n",
    "    final = merged[merged[\"score\"].apply(has_value) & merged[\"probability\"].apply(has_value)].copy()\n",
    "\n",
    "    # 13) Save\n",
    "    final.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Merged rows with scores: {final.shape[0]} / {merged.shape[0]}\")\n",
    "    print(f\"Saved: {out_csv}\")\n",
    "    return final, merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Zavtra pair (if you want to re-run with the same logic) ----\n",
    "# final_zavtra, merged_zavtra = merge_scores_with_texts(\n",
    "#     base_csv=\"zavtra_soros_translated_01092025.csv\",\n",
    "#     scored_csv=\"zavtra_soros_01102025_scored.csv\",\n",
    "#     out_csv=\"zavtra_soros_final.csv\",\n",
    "#     key=\"ArticleLink\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Example: Eastlink pair ----\n",
    "final_eastlink, merged_eastlink = merge_scores_with_texts(\n",
    "    base_csv=\"eastlink_soros_translated_01092025.csv\",\n",
    "    scored_csv=\"soros_translated_01102025.csv\",\n",
    "    out_csv=\"eastlink_soros_final.csv\",\n",
    "    key=\"ArticleLink\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Add back terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- config ---\n",
    "in_csv  = \"eastlink_soros_final.csv\"\n",
    "out_csv = \"eastlink_soros_final_replaced.csv\"   # change to same name if you want in-place\n",
    "target_ids_raw = [\n",
    "    \"4158790\",\"42260504\",\"19335327\",\"4280\",\"3723631\",\n",
    "    \"9367502\",\"5838159\",\"2188538\",\"23528922\"\n",
    "]\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(in_csv)\n",
    "\n",
    "# Normalise ArticleID so strings like \"4280.0\" match too\n",
    "df[\"_ArticleID_int\"] = pd.to_numeric(df[\"ArticleID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "target_ids = pd.to_numeric(pd.Series(target_ids_raw), errors=\"coerce\").astype(\"Int64\")\n",
    "mask_ids = df[\"_ArticleID_int\"].isin(set(target_ids))\n",
    "\n",
    "# Prepare counts (before)\n",
    "pattern_ci = r\"(?i)\\bSoros\\b\"   # whole-word, case-insensitive\n",
    "subset = df.loc[mask_ids, [\"ArticleID\", \"ArticleTextEnglish\"]].copy()\n",
    "subset[\"count_before\"] = subset[\"ArticleTextEnglish\"].fillna(\"\").str.count(pattern_ci)\n",
    "\n",
    "# Do the replacement only for the targeted IDs\n",
    "repl_pat = re.compile(r\"\\b[Ss]oros\\b\")  # same as (?i)\\bSoros\\b but lets us keep 'Soroses' case\n",
    "col = \"ArticleTextEnglish\"\n",
    "df.loc[mask_ids & df[col].notna(), col] = (\n",
    "    df.loc[mask_ids & df[col].notna(), col]\n",
    "      .str.replace(repl_pat, \"Soroses\", regex=True)\n",
    ")\n",
    "\n",
    "# Counts (after) for sanity check\n",
    "subset[\"count_after\"] = df.loc[mask_ids, \"ArticleTextEnglish\"].fillna(\"\").str.count(pattern_ci)\n",
    "subset[\"replacements\"] = subset[\"count_before\"] - subset[\"count_after\"]\n",
    "\n",
    "print(\"Total rows targeted:\", mask_ids.sum())\n",
    "print(\"Total 'Soros' tokens replaced:\",\n",
    "      int(subset[\"replacements\"].clip(lower=0).sum()))\n",
    "\n",
    "# Optional: inspect which IDs actually changed\n",
    "changed = subset[subset[\"replacements\"] > 0]\n",
    "display(changed[[\"ArticleID\",\"replacements\"]].sort_values(\"replacements\", ascending=False).head(20))\n",
    "\n",
    "# Save\n",
    "df.drop(columns=[\"_ArticleID_int\"], inplace=True)\n",
    "df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Files to process\n",
    "inputs = [\n",
    "    \"eastlink_soros_final_replaced2.csv\",\n",
    "    \"zavtra_soros_final.csv\",\n",
    "]\n",
    "\n",
    "# Use today's date in the filename (keep your project’s ddmmyyyy style)\n",
    "date_suffix = \"01102025\"  # 01-10-2025\n",
    "\n",
    "# Regex: whole-word, case-insensitive, for the listed variants\n",
    "pattern = r\"(?i)\\b(?:soros|soroses|sorosyats)\\b\"\n",
    "\n",
    "def add_soros_count(in_csv):\n",
    "    df = pd.read_csv(in_csv)\n",
    "\n",
    "    # find all ArticleTextEnglish* columns that exist\n",
    "    text_cols = [c for c in df.columns if c.startswith(\"ArticleTextEnglish\")]\n",
    "    if not text_cols:\n",
    "        print(f\"[WARN] No ArticleTextEnglish* columns in {in_csv}\")\n",
    "        df[\"soros_count\"] = 0\n",
    "    else:\n",
    "        # sum counts across all text columns, treating NaN as empty string\n",
    "        counts = None\n",
    "        for i, c in enumerate(text_cols):\n",
    "            col_counts = df[c].fillna(\"\").astype(str).str.count(pattern)\n",
    "            counts = col_counts if counts is None else (counts + col_counts)\n",
    "\n",
    "        df[\"soros_count\"] = counts.astype(int)\n",
    "\n",
    "    # build output path with date suffix\n",
    "    root, ext = os.path.splitext(in_csv)\n",
    "    out_csv = f\"{root}_{date_suffix}{ext}\"\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Processed: {in_csv}  →  {out_csv}\")\n",
    "    print(\"  ArticleTextEnglish* columns:\", text_cols)\n",
    "    print(\"  Rows with soros_count > 0:\", int((df['soros_count'] > 0).sum()))\n",
    "    return out_csv\n",
    "\n",
    "# Run for both files\n",
    "outputs = [add_soros_count(p) for p in inputs]\n",
    "\n",
    "# Quick peek (optional)\n",
    "for out in outputs:\n",
    "    df_check = pd.read_csv(out, nrows=5)\n",
    "    display(df_check.head(3)[[\"soros_count\"] + [c for c in df_check.columns if c.startswith(\"ArticleTextEnglish\")][:2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- config ---\n",
    "in_csv  = \"eastlink_soros_final_replaced.csv\"   # set to the file you're editing\n",
    "out_csv = \"eastlink_soros_final_replaced2.csv\"  # set same as in_csv for in-place\n",
    "target_id_raw = \"10022567\"\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(in_csv)\n",
    "\n",
    "# Robust match on ArticleID\n",
    "df[\"_ArticleID_int\"] = pd.to_numeric(df[\"ArticleID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "target_id = pd.to_numeric(target_id_raw, errors=\"coerce\")\n",
    "mask = df[\"_ArticleID_int\"].eq(target_id)\n",
    "\n",
    "# Count matches before (for sanity)\n",
    "pattern_ci = r\"(?i)\\bSoros\\b\"\n",
    "before = int(df.loc[mask, \"ArticleTextEnglish\"].fillna(\"\").str.count(pattern_ci).sum())\n",
    "\n",
    "# Replace only for the targeted row(s)\n",
    "repl_pat = re.compile(r\"\\b[Ss]oros\\b\")\n",
    "col = \"ArticleTextEnglish\"\n",
    "df.loc[mask & df[col].notna(), col] = (\n",
    "    df.loc[mask & df[col].notna(), col]\n",
    "      .str.replace(repl_pat, \"SOROSYATS\", regex=True)\n",
    ")\n",
    "\n",
    "# Count remaining \"Soros\" tokens after\n",
    "after = int(df.loc[mask, \"ArticleTextEnglish\"].fillna(\"\").str.count(pattern_ci).sum())\n",
    "print(f\"Rows targeted: {mask.sum()} | Replacements made: {before - after}\")\n",
    "\n",
    "# (Optional) display the edited rows\n",
    "display(df.loc[mask, [\"ArticleID\", col]].head())\n",
    "\n",
    "# Save\n",
    "df.drop(columns=[\"_ArticleID_int\"], inplace=True)\n",
    "df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- config ---\n",
    "in_csv  = \"tsargrad_soros_translated_21092025.csv\"\n",
    "out_csv = \"tsargrad_soros_translated_01102025_final.csv\"\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(in_csv)\n",
    "\n",
    "# Columns to search: all ArticleTextEnglish* plus common text fields in this dataset\n",
    "text_cols = [c for c in df.columns if c.startswith(\"ArticleTextEnglish\")]\n",
    "text_cols += [c for c in [\n",
    "    \"ArticleTextEnglish\",\n",
    "] if c in df.columns]\n",
    "\n",
    "# Deduplicate while preserving order\n",
    "text_cols = list(dict.fromkeys(text_cols))\n",
    "print(\"Counting in columns:\", text_cols)\n",
    "\n",
    "# Count whole-word 'soros' (case-insensitive)\n",
    "pattern = re.compile(r\"(?i)\\bSoros\\b\")\n",
    "\n",
    "if text_cols:\n",
    "    counts = None\n",
    "    for c in text_cols:\n",
    "        col_counts = df[c].fillna(\"\").astype(str).str.count(pattern)\n",
    "        counts = col_counts if counts is None else (counts + col_counts)\n",
    "    df[\"soros_count\"] = counts.astype(int)\n",
    "else:\n",
    "    df[\"soros_count\"] = 0\n",
    "\n",
    "# Save\n",
    "df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Saved: {out_csv}\")\n",
    "print(\"Rows with soros_count > 0:\", int((df[\"soros_count\"] > 0).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_score.to_csv(\"missing_score_tsargrad_21092025_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## inspect IDs have missing score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rows with missing/blank score\n",
    "missing_mask = df[\"score\"].isna() | df[\"score\"].astype(str).str.strip().isin([\"\", \"nan\", \"NaN\", \"None\", \"null\"])\n",
    "missing = df.loc[missing_mask].copy()\n",
    "\n",
    "print(\"Missing rows:\", len(missing))  # should be 29\n",
    "\n",
    "# save them\n",
    "out_csv = \"tsargrad_missing_score_21092025.csv\"\n",
    "missing.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "display(FileLink(out_csv))  # clickable link in Jupyter\n",
    "\n",
    "# (optional) Excel + just the IDs\n",
    "# missing.to_excel(\"tsargrad_missing_score_21092025.xlsx\", index=False)\n",
    "# missing[\"ArticleID\"].dropna().astype(str).to_csv(\"tsargrad_missing_ids_21092025.txt\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Copy manual collected info from .csv to original .csv to update score/ article text google translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- file paths (edit if needed) ----\n",
    "old_path = Path(\"tsargrad_soros_translated_21092025.csv\")\n",
    "new_path = Path(\"tsargrad_soros_translated_21092025_new.csv\")\n",
    "\n",
    "# Columns to overwrite from _new -> old\n",
    "cols_to_copy = [\n",
    "    \"title\", \"translated_title\", \"author\", \"article_text\",\n",
    "    \"ArticleTextEnglish\", \"score\", \"probability\"\n",
    "]\n",
    "\n",
    "# Read CSVs (adjust encoding if needed, e.g., encoding='utf-8-sig')\n",
    "old_df = pd.read_csv(old_path)\n",
    "new_df = pd.read_csv(new_path)\n",
    "\n",
    "# Basic sanity checks\n",
    "assert \"url\" in old_df.columns, \"old CSV missing 'url' column\"\n",
    "assert \"url\" in new_df.columns, \"new CSV missing 'url' column\"\n",
    "\n",
    "# Ensure URL uniqueness (warn but proceed by keeping first)\n",
    "if new_df[\"url\"].duplicated().any():\n",
    "    print(\"WARNING: Duplicated URLs in NEW file; keeping the first occurrence.\")\n",
    "    new_df = new_df.drop_duplicates(subset=\"url\", keep=\"first\")\n",
    "if old_df[\"url\"].duplicated().any():\n",
    "    print(\"WARNING: Duplicated URLs in OLD file; keeping the first occurrence for preview/diff only.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use URL as index to align\n",
    "old_df_idx = old_df.set_index(\"url\", drop=False)\n",
    "new_df_idx = new_df.set_index(\"url\", drop=False)\n",
    "\n",
    "common_urls = old_df_idx.index.intersection(new_df_idx.index)\n",
    "\n",
    "# Columns that exist in both\n",
    "effective_cols = [c for c in cols_to_copy if c in old_df_idx.columns and c in new_df_idx.columns]\n",
    "\n",
    "# Snapshot BEFORE update\n",
    "before = old_df_idx.loc[common_urls, effective_cols].copy()\n",
    "\n",
    "# Perform overwrite\n",
    "old_df_idx.loc[common_urls, effective_cols] = new_df_idx.loc[common_urls, effective_cols].values\n",
    "\n",
    "# Snapshot AFTER update\n",
    "after = old_df_idx.loc[common_urls, effective_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "changed = (before != after) & ~(before.isna() & after.isna())\n",
    "\n",
    "print(\"Number of changed cells:\", changed.sum().sum())\n",
    "\n",
    "# Show first 20 differences\n",
    "diff_df = []\n",
    "for col in effective_cols:\n",
    "    mask = changed[col]\n",
    "    if mask.any():\n",
    "        sub = pd.DataFrame({\n",
    "            \"url\": before.index[mask],\n",
    "            \"field\": col,\n",
    "            \"old\": before.loc[mask, col],\n",
    "            \"new\": after.loc[mask, col],\n",
    "        })\n",
    "        diff_df.append(sub)\n",
    "\n",
    "diff_df = pd.concat(diff_df, ignore_index=True)\n",
    "display(diff_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many rows actually changed in at least one column\n",
    "rows_changed = (before != after).any(axis=1).sum()\n",
    "print(f\"Rows changed: {rows_changed}\")\n",
    "\n",
    "# Count how many columns changed per row\n",
    "per_row_changes = (before != after).sum(axis=1)\n",
    "print(per_row_changes.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "from datetime import datetime\n",
    "\n",
    "# Make a timestamped backup before overwriting\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "backup_path = old_path.with_suffix(f\".bak_{ts}.csv\")\n",
    "\n",
    "# Save the untouched original as backup\n",
    "old_df.to_csv(backup_path, index=False)\n",
    "print(f\" Backup saved → {backup_path}\")\n",
    "\n",
    "# Save the updated dataframe (reset index to drop 'url' as index)\n",
    "old_df_updated = old_df_idx.reset_index(drop=True)\n",
    "old_df_updated.to_csv(old_path, index=False)\n",
    "print(f\" Updated CSV saved → {old_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Project Scope\n",
    "Delete specific ArticleIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load CSV\n",
    "# df = pd.read_csv(\"soros_21092025_eastview.csv\")\n",
    "\n",
    "# # Make sure ArticleID is treated as string (avoids type mismatch issues)\n",
    "# df[\"ArticleID\"] = df[\"ArticleID\"].astype(str).str.strip()\n",
    "\n",
    "# # IDs you want to delete (as strings)\n",
    "# to_remove = [\"285237\", \"4961499\", \"4953787\"]\n",
    "\n",
    "# # Drop them\n",
    "# df_cleaned = df[~df[\"ArticleID\"].isin(to_remove)]\n",
    "\n",
    "# # Verify — should be empty if removal worked\n",
    "# print(df_cleaned[df_cleaned[\"ArticleID\"].isin(to_remove)])\n",
    "\n",
    "# # Check row counts\n",
    "# print(\"Before:\", len(df), \"After:\", len(df_cleaned))\n",
    "\n",
    "# # Save back\n",
    "# df_cleaned.to_csv(\"soros_21092025_eastview_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"soros_21092025_eastview.csv\")\n",
    "\n",
    "to_remove = {285237, 4961499, 4953787}\n",
    "present = set(df[\"ArticleID\"]).intersection(to_remove)\n",
    "\n",
    "if present:\n",
    "    print(\"Found and removing:\", present)\n",
    "    df = df[~df[\"ArticleID\"].isin(present)]\n",
    "    df.to_csv(\"soros_21092025_eastview_cleaned.csv\", index=False)\n",
    "    print(\"New row count:\", len(df))\n",
    "else:\n",
    "    print(\"None of the target ArticleIDs are in this file:\", to_remove)\n",
    "    print(\"Sample of nearby IDs for context:\")\n",
    "    print(df[\"ArticleID\"].head(20).to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the cleaned CSV\n",
    "df = pd.read_csv(\"zavtra_soros_translated_01092025.csv\")\n",
    "\n",
    "# Show the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count total rows vs. non-null rows in 'score'\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Rows with score:\", df['score'].notna().sum())\n",
    "\n",
    "# 2. Check how many are missing\n",
    "missing_scores = df['score'].isna().sum()\n",
    "print(\"Missing score rows:\", missing_scores)\n",
    "\n",
    "# 3. If you want to actually see the missing rows\n",
    "df[df['score'].isna()]\n",
    "\n",
    "# 4. If score could be blank strings instead of NaN\n",
    "blank_scores = (df['score'].astype(str).str.strip() == \"\").sum()\n",
    "print(\"Blank score rows:\", blank_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing per Publication\n",
    "print(missing_df['Publication'].value_counts())\n",
    "\n",
    "# How many missing per Year\n",
    "print(missing_df['Year'].value_counts())\n",
    "\n",
    "# Look at just the ArticleIDs\n",
    "missing_ids = missing_df['ArticleID'].tolist()\n",
    "print(missing_ids[:20])  # print first 20 IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all missing score rows in a table (scrollable in Jupyter)\n",
    "missing_df = df[df['score'].isna()]\n",
    "missing_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with character count of the English text\n",
    "missing_df = df[df['score'].isna()].copy()\n",
    "missing_df['char_len'] = missing_df['ArticleTextEnglish'].astype(str).str.len()\n",
    "\n",
    "# See distribution of lengths\n",
    "print(missing_df['char_len'].describe())\n",
    "\n",
    "# See the top 10 longest texts\n",
    "print(missing_df[['ArticleID', 'Publication', 'Year', 'char_len']].sort_values(by='char_len', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case-insensitive search for 'soros' in ArticleTextEnglish\n",
    "mask = df['ArticleTextEnglish'].str.contains(\"soros\", case=False, na=False)\n",
    "\n",
    "# Get all rows where 'soros' appears\n",
    "soros_df = df[mask]\n",
    "\n",
    "# Show a few examples\n",
    "soros_df[['ArticleID', 'Publication', 'Year', 'ArticleTextEnglish']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = df[df['score'].isna()].copy()\n",
    "mask = missing_df['ArticleTextEnglish'].str.contains(\"soros\", case=False, na=False)\n",
    "\n",
    "soros_missing = missing_df[mask]\n",
    "\n",
    "print(\"Missing rows with 'soros' in text:\", len(soros_missing))\n",
    "soros_missing[['ArticleID', 'Publication', 'Year', 'ArticleTextEnglish']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all rows that mention 'soros'\n",
    "soros_df.to_csv(\"soros_mentions.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Exported\", len(soros_df), \"rows to soros_mentions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary of text lengths\n",
    "df[\"ArticleText\"].str.len().describe()\n",
    "\n",
    "# Filter for a particular ArticleID\n",
    "df[df.ArticleID == \"10012711\"].T\n",
    "\n",
    "# Count how many records per Publication Year\n",
    "df[\"Publication Date\"].dt.year.value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a boolean mask: True if ArticleText is non‑empty after stripping whitespace\n",
    "has_text = df[\"ArticleText\"].fillna(\"\").str.strip() != \"\"\n",
    "\n",
    "# count how many have text vs don’t\n",
    "counts = has_text.value_counts().rename(index={True: \"with_text\", False: \"without_text\"})\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Check PNGS imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your CSV\n",
    "df = pd.read_csv(\"soros_text.csv\", dtype=str).fillna(\"\")\n",
    "\n",
    "# 2. Filter down to just the PNG‑based records\n",
    "png_df = df[df[\"ZipFile\"].str.lower().str.endswith(\".png\")]\n",
    "\n",
    "# 3. Build a boolean mask: True if there’s any non‑whitespace in ArticleText\n",
    "has_text = png_df[\"ArticleText\"].str.strip() != \"\"\n",
    "\n",
    "# 4. Print counts\n",
    "print(f\"Total PNGs processed      : {len(png_df)}\")\n",
    "print(f\"PNGs with extracted text  : {has_text.sum()}\")\n",
    "print(f\"PNGs without extracted text: {len(png_df) - has_text.sum()}\")\n",
    "\n",
    "# Show the first few that failed:\n",
    "print(\"\\nPNGs that produced NO text:\")\n",
    "print(png_df.loc[~has_text, [\"ZipFile\",\"ArticleText\"]].head(), \"\\n\")\n",
    "\n",
    "# And a few that succeeded:\n",
    "print(\"PNGs that DID extract some text:\")\n",
    "print(png_df.loc[has_text, [\"ZipFile\",\"ArticleText\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### Check png ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your cleaned CSV\n",
    "df = pd.read_csv(\"soros_text.csv\", dtype=str).fillna(\"\")\n",
    "\n",
    "# 2) List PNG IDs to check\n",
    "ids = [\n",
    "    \"43450062\",\"45780836\",\"46081842\",\"46155643\",\"46186228\",\"46237045\",\n",
    "    \"46590726\",\"46652831\",\"46679825\",\"46830363\",\"46889551\",\"47040605\",\n",
    "    \"47834519\",\"48128305\",\"48191549\",\"48210782\",\"48280247\",\"48765228\",\n",
    "    \"48871713\",\"49079885\",\"49361014\",\"49563408\",\"49680285\",\"49859217\",\n",
    "    \"50939659\",\"51225798\",\"51337298\",\"51901452\",\"52320070\",\"52818826\",\n",
    "    \"53259695\",\"53518917\",\"53887820\",\"56256564\",\"56710275\",\"59288998\",\n",
    "    \"59459315\"\n",
    "]\n",
    "\n",
    "# 3) Filter\n",
    "subset = df[df[\"ArticleID\"].isin(ids)]\n",
    "\n",
    "# 4) How many have non‑empty ArticleText?\n",
    "with_text    = subset[\"ArticleText\"].str.strip().astype(bool).sum()\n",
    "without_text = len(subset) - with_text\n",
    "print(f\"{len(subset)} total IDs\\n→ with text   : {with_text}\\n→ without text: {without_text}\\n\")\n",
    "\n",
    "# 5) Peek at the ones missing text\n",
    "print(\"IDs with missing text:\")\n",
    "print(subset[subset[\"ArticleText\"].str.strip() == \"\"][\"ArticleID\"].tolist())\n",
    "\n",
    "# 6) And sample of those that did extract:\n",
    "print(\"\\nSample extracted text:\")\n",
    "print(subset[subset[\"ArticleText\"].str.strip() != \"\"][[\"ArticleID\",\"ArticleText\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Check PDFs imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"soros_text.csv\", dtype=str).fillna(\"\")\n",
    "# ids = [\"10012711\", \"10012758\", \"10021857\", \"10060474\", \"10110314\", \"10124118\", \"10130531\", \"10137\", \"10214979\", \"10228407\", \"10491551\", \"10493649\", \"10682772\", \"11147\", \"11204991\", \"11322392\", \"11376316\", \"11396980\", \"11400\", \"11407779\", \"11411652\", \"11614680\", \"1167\", \"11831910\", \"11875889\", \"11877061\", \"11911212\", \"11931073\", \"12075116\", \"12075586\", \"12247814\", \"12261076\", \"12261077\", \"12298431\", \"12723219\", \"12800283\", \"12908500\", \"12929645\", \"12957670\", \"12968446\", \"13068430\", \"13092748\", \"13255401\", \"13255434\", \"13276200\", \"14080180\", \"1447\", \"14746291\", \"14764054\", \"1750044729\", \"1750044756\", \"1750044780\", \"1750044801\", \"1750045011\", \"18530836\", \"18672929\", \"18761394\", \"18770939\", \"18981209\", \"18991913\", \"19032349\", \"19035059\", \"19124361\", \"19287542\", \"19359219\", \"19484195\", \"19612370\", \"19669286\", \"19726145\", \"19771679\", \"19826195\", \"19923542\", \"19953013\", \"19975642\", \"20098411\", \"20166113\", \"20268856\", \"20432991\", \"20460473\", \"20462359\", \"20515198\", \"20574709\", \"20658875\", \"20709184\", \"20785406\", \"20938540\", \"2105770\", \"2107390\", \"2107558\", \"2108346\", \"2109114\", \"2109753\", \"2110094\", \"21188636\", \"21250044\", \"21293652\", \"2130105\", \"2130824\", \"21347366\", \"2135847\", \"2137686\", \"2138649\", \"2138727\", \"21482574\", \"2157603\", \"2158028\", \"2162202\", \"2162539\", \"21625812\", \"21636781\", \"2164830\", \"2165502\", \"2168241\", \"2179890\", \"2187948\", \"2188180\", \"2189962\", \"2190096\", \"2190489\", \"2191171\", \"2191350\", \"2191374\", \"2191510\", \"2191633\", \"2191646\", \"2192593\", \"2192899\", \"2193067\", \"2193124\", \"2193893\", \"2194023\", \"2194037\", \"2194162\", \"2194222\", \"2194332\", \"2194957\", \"2195015\", \"2195112\", \"2195514\", \"2196401\", \"2196666\", \"2196844\", \"22568763\", \"22838932\", \"23255672\", \"2329758\", \"23300130\", \"2330607\", \"2331522\", \"23379764\", \"23457205\", \"2403543\", \"2403905\", \"2405780\", \"2407303\", \"2407832\", \"2409053\", \"2409098\", \"2409978\", \"2411229\", \"2411383\", \"2415134\", \"2415300\", \"2419018\", \"2424121\", \"2424207\", \"2424842\", \"2426314\", \"24278986\", \"2432957\", \"2434299\", \"2435371\", \"2437516\", \"2439774\", \"2440493\", \"2440966\", \"2441032\", \"2441403\", \"2442876\", \"2448810\", \"24861986\", \"25761140\", \"25916073\", \"26191336\", \"26239338\", \"26312899\", \"26345940\", \"26658188\", \"267898\", \"26850258\", \"27622039\", \"276480\", \"276769\", \"27698022\", \"278160\", \"279257\", \"279258\", \"279833\", \"28078435\", \"28108755\", \"28109218\", \"284502\", \"285237\", \"285540\", \"28595762\", \"28710929\", \"28844176\", \"29252278\", \"29454859\", \"29498634\", \"29607948\", \"298477\", \"29867190\", \"29888074\", \"29889084\", \"303065\", \"30373426\", \"303959\", \"304093\", \"305141\", \"305616\", \"305795\", \"305865\", \"306105\", \"306567\", \"307577\", \"308071\", \"308203\", \"309897\", \"309954\", \"310196\", \"310854\", \"312056\", \"315304\", \"316369\", \"317052\", \"317224\", \"317282\", \"318415\", \"319672\", \"320142\", \"320144\", \"321630\", \"322168\", \"322271\", \"322648\", \"323732\", \"325045\", \"327194\", \"327390\", \"327545\", \"332266\", \"3464154\", \"3464176\", \"3464631\", \"3465168\", \"3466536\", \"3466698\", \"3466899\", \"3467628\", \"3468090\", \"3469071\", \"3469674\", \"3469961\", \"3470168\", \"3471430\", \"3473306\", \"3473550\", \"34818350\", \"35244234\", \"35851437\", \"36000606\", \"36021200\", \"36058932\", \"36107825\", \"36107950\", \"36395681\", \"36413999\", \"36506561\", \"37269835\", \"37689266\", \"37689427\", \"37706855\", \"37719250\", \"37740682\", \"37834274\", \"37867818\", \"38623245\", \"3872132\", \"39128008\", \"3917019\", \"39191803\", \"39253550\", \"3953289\", \"4059170\", \"40678772\", \"40846261\", \"4095755\", \"41297084\", \"41467526\", \"41520943\", \"4158492\", \"4158790\", \"4206937\", \"42162547\", \"4217333\", \"42321440\", \"4237588\", \"42426863\", \"4245423\", \"42564330\", \"42580160\", \"42601175\", \"42740408\", \"42798891\", \"42820421\", \"42841428\", \"42844413\", \"42940705\", \"42953765\", \"42973236\", \"4310025\", \"4332384\", \"43335259\", \"4370561\", \"43864581\", \"4410099\", \"4428914\", \"44540655\", \"4457225\", \"44586218\", \"44783215\", \"44783233\", \"44809697\", \"4487\", \"44937489\", \"45085825\", \"45120173\", \"45163176\", \"45235282\", \"4534\", \"45353100\", \"45386197\", \"45489106\", \"45489108\", \"45705034\", \"45812104\", \"4582313\", \"45826120\", \"45975893\", \"45984198\", \"45991332\", \"46002287\", \"46022309\", \"46022310\", \"46024087\", \"46033685\", \"4606\", \"46188048\", \"46222890\", \"46237047\", \"46287407\", \"4637963\", \"46397855\", \"46399554\", \"4642491\", \"46431219\", \"46488755\", \"46497683\", \"46543824\", \"46721285\", \"46742677\", \"46760929\", \"46800290\", \"4681934\", \"46822146\", \"46859511\", \"4685983\", \"46863058\", \"46886399\", \"46889509\", \"4689176\", \"4689502\", \"47042265\", \"47082683\", \"47094466\", \"47174907\", \"47558736\", \"47670617\", \"47687945\", \"47716365\", \"47823928\", \"47885479\", \"48021687\", \"48021743\", \"48137043\", \"48189090\", \"48212888\", \"48259395\", \"4848531\", \"48575326\", \"48594769\", \"48609659\", \"48649178\", \"48650200\", \"48682296\", \"48789319\", \"48844470\", \"4889395\", \"48966866\", \"49297858\", \"49385985\", \"49564054\", \"49593638\", \"4969215\", \"49708713\", \"49807008\", \"49934268\", \"49954494\", \"50031193\", \"5014873\", \"50325487\", \"50412491\", \"5046237\", \"50828249\", \"50857949\", \"5097003\", \"5113870\", \"5117647\", \"5143163\", \"51614643\", \"51616512\", \"51839600\", \"51857978\", \"51946156\", \"51965969\", \"51972322\", \"52040572\", \"52073680\", \"52074383\", \"52289573\", \"52289614\", \"5229215\", \"5241135\", \"52458978\", \"52733125\", \"52757830\", \"53135485\", \"53174198\", \"53276473\", \"5332922\", \"5333984\", \"5359827\", \"53916190\", \"53927202\", \"54001466\", \"54848525\", \"54991897\", \"5569657\", \"5572489\", \"55934740\", \"56147896\", \"5632599\", \"56414434\", \"56604908\", \"5667822\", \"57192503\", \"5772044\", \"5782498\", \"5817081\", \"5826635\", \"5831454\", \"5838108\", \"5842454\", \"5842478\", \"58709113\", \"59267169\", \"5952566\", \"6053775\", \"6063326\", \"6079023\", \"6081302\", \"6081306\", \"60861681\", \"60932377\", \"6105391\", \"61486113\", \"61723954\", \"62051793\", \"6205543\", \"6223037\", \"62506412\", \"6306421\", \"63294008\", \"6507\", \"6521169\", \"6533255\", \"6603029\", \"6611882\", \"6656277\", \"68032659\", \"6909372\", \"6929\", \"7009\", \"7012403\", \"7049276\", \"7070299\", \"7151751\", \"7180777\", \"7210829\", \"7273512\", \"7282622\", \"7305\", \"7576248\", \"7681169\", \"7695433\", \"7747272\", \"7755691\", \"7812272\", \"78706085\", \"7901\", \"7915932\", \"7964028\", \"8161579\", \"8183791\", \"8212732\", \"8240787\", \"8441541\", \"8515751\", \"8588339\", \"8596692\", \"8730296\", \"8744257\", \"8749749\", \"8786106\", \"8860966\", \"9039690\", \"9124723\", \"9147024\", \"9184404\", \"9193869\", \"9208314\", \"9242\", \"9251\", \"9255771\", \"9320963\", \"9378997\", \"9392495\", \"9400\", \"94314565\", \"9523663\", \"9539796\", \"9542459\", \"9564985\", \"9606\", \"9667316\", \"9706592\", \"9726453\", \"9731173\", \"9737074\", \"9740249\", \"9850587\", \"9895356\", ]\n",
    "\n",
    "# subset = df[df[\"ArticleID\"].isin(ids)]\n",
    "\n",
    "# # 4) How many have non‑empty ArticleText?\n",
    "# with_text    = subset[\"ArticleText\"].str.strip().astype(bool).sum()\n",
    "# without_text = len(subset) - with_text\n",
    "# print(f\"{len(subset)} total IDs\\n→ with text   : {with_text}\\n→ without text: {without_text}\\n\")\n",
    "\n",
    "# # 5) Peek at the ones missing text\n",
    "# print(\"IDs with missing text:\")\n",
    "# print(subset[subset[\"ArticleText\"].str.strip() == \"\"][\"ArticleID\"].tolist())\n",
    "\n",
    "# # 6) And sample of those that did extract:\n",
    "# print(\"\\nSample extracted text:\")\n",
    "# print(subset[subset[\"ArticleText\"].str.strip() != \"\"][[\"ArticleID\",\"ArticleText\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your CSV\n",
    "df = pd.read_csv(\"soros_text.csv\", dtype=str).fillna(\"\")\n",
    "\n",
    "# 2. Create a boolean mask for “has any non‑whitespace text”\n",
    "has_text = df[\"ArticleText\"].str.strip() != \"\"\n",
    "\n",
    "# 3. Counts\n",
    "num_with    = has_text.sum()\n",
    "num_without = len(df) - num_with\n",
    "\n",
    "print(f\"Articles WITH text   : {num_with}\")\n",
    "print(f\"Articles WITHOUT text: {num_without}\")\n",
    "\n",
    "# 4. List of ArticleIDs with empty ArticleText\n",
    "empty_ids = df.loc[~has_text, \"ArticleID\"].tolist()\n",
    "print(\"\\nArticleIDs with empty ArticleText:\")\n",
    "print(empty_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Make sure pandas will show every column\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# 2) Pick your ArticleID\n",
    "article_id = \"9728994\"\n",
    "\n",
    "# 3) Display the full row (all columns) for that ID\n",
    "df[df[\"ArticleID\"] == article_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Pick your ArticleID\n",
    "# article_id = \"3023148\"\n",
    "\n",
    "# # 2) Extract that row as a Series\n",
    "# row = df.loc[df[\"ArticleID\"] == article_id].squeeze()\n",
    "\n",
    "# # 3) Print each column on its own line\n",
    "# for col, val in row.items():\n",
    "#     print(f\"{col}:\\n{val}\\n{'-'*40}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Publications filter and check ArticleID\n",
    "Kommersant\n",
    "Literaturnaia gazeta\n",
    "Nezavisimaia gazeta\n",
    "Novaia gazeta\n",
    "Pravda\n",
    "Slovo\n",
    "Sovetskaia Rossiia\n",
    "Trud\n",
    "Vedomosti\n",
    "Время МН\n",
    "Общая газета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your CSV\n",
    "df = pd.read_csv(\"soros_text.csv\", dtype=str).fillna(\"\")\n",
    "\n",
    "# 2) Define the list of publications you care about\n",
    "pubs = [\n",
    "    \"Kommersant\", \"Literaturnaia gazeta\", \"Nezavisimaia gazeta\", \"Novaia gazeta\",\n",
    "    \"Pravda\", \"Slovo\", \"Sovetskaia Rossiia\", \"Trud\", \"Vedomosti\",\n",
    "    \"Время МН\", \"Общая газета\"\n",
    "]\n",
    "\n",
    "# 3) Filter down to just those rows\n",
    "sub = df[df[\"Publication\"].isin(pubs)].copy()\n",
    "\n",
    "# 4) Create a boolean column for “has text”\n",
    "sub[\"has_text\"] = sub[\"ArticleText\"].str.strip() != \"\"\n",
    "\n",
    "# 5) Group and aggregate\n",
    "result = (\n",
    "    sub\n",
    "    .groupby(\"Publication\")[\"has_text\"]\n",
    "    .agg(\n",
    "        with_text = lambda x: x.sum(),\n",
    "        without_text = lambda x: (~x).sum()\n",
    "    )\n",
    "    .reindex(pubs)   # keep the original order\n",
    ")\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get unique, non-null values\n",
    "unique_pubs = df[\"Publication\"].dropna().unique()\n",
    "\n",
    "# 2) Print them as a list\n",
    "# print(list(unique_pubs))\n",
    "\n",
    "for pub in unique_pubs:\n",
    "    print(pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many match list\n",
    "pubs = [\n",
    "    \"Kommersant\", \"Literaturnaia gazeta\", \"Nezavisimaia gazeta\",\n",
    "    \"Novaia gazeta\", \"Pravda\", \"Slovo\", \"Sovetskaia Rossiia\",\n",
    "    \"Trud\", \"Vedomosti\", \"Время МН\", \"Общая газета\"\n",
    "]\n",
    "\n",
    "# get the distinct Publication values in your df\n",
    "unique_pubs = set(df[\"Publication\"].dropna().unique())\n",
    "\n",
    "# find the intersection\n",
    "matches = unique_pubs & set(pubs)\n",
    "\n",
    "print(f\"Matched publications ({len(matches)} of {len(pubs)}):\")\n",
    "for m in sorted(matches):\n",
    "    print(\" •\", m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find publications\n",
    "# Your target publications\n",
    "pubs = [\n",
    "    \"Kommersant\", \"Literaturnaia gazeta\", \"Nezavisimaia gazeta\",\n",
    "    \"Novaia gazeta\", \"Pravda\", \"Slovo\", \"Sovetskaia Rossiia\",\n",
    "    \"Trud\", \"Vedomosti\", \"Время МН\", \"Общая газета\"\n",
    "]\n",
    "\n",
    "# Boolean mask for “has any non‑blank text”\n",
    "has_text = df[\"ArticleText\"].fillna(\"\").str.strip() != \"\"\n",
    "\n",
    "for pub in pubs:\n",
    "    sub = df[df[\"Publication\"] == pub]\n",
    "    non_null_ids = sub.loc[has_text & (df[\"Publication\"] == pub), \"ArticleID\"].tolist()\n",
    "    null_ids     = sub.loc[~has_text & (df[\"Publication\"] == pub), \"ArticleID\"].tolist()\n",
    "\n",
    "    print(f\"=== {pub} ===\")\n",
    "    print(f\"With text    ({len(non_null_ids)}): {non_null_ids}\")\n",
    "    print(f\"Without text ({len(null_ids)    }): {null_ids}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
