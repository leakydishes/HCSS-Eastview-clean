{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication Totals\n",
    "\n",
    "# Kommersant \n",
    "# Weekly Period (1990-1991): 2 years × 52 issues/year = 104 issues\n",
    "\n",
    "# Daily Period (1992-2022): 31 years × ~240 issues/year = ~7,440 issues\n",
    "\n",
    "# = 7,544\n",
    "\n",
    "\n",
    "\n",
    "# Nezavisimaya Gazeta \n",
    "\n",
    "# December 1990: A handful of issues were likely published in the last ten days of the year.\n",
    "\n",
    "# 1991 (Tri-weekly): Approximately 156 issues (3 issues/week x 52 weeks).\n",
    "\n",
    "# 1992 - May 23, 1995 (Daily): Roughly 3.4 years with an estimated 260 issues per year, totaling approximately 884 issues.\n",
    "\n",
    "# May 24, 1995 - August 1995 (Suspension): Zero issues published.\n",
    "\n",
    "# September 1995 - December 2022 (Daily): Approximately 27.3 years with an estimated 260 issues per year, resulting in around 7,098 issues.\n",
    "\n",
    "# = 8,144\n",
    "\n",
    "\n",
    "# Vedomosti\n",
    "\n",
    "# 1999 5 issues a week from September 7: In the last four months of its inaugural year, 84 issues.\n",
    "\n",
    "# 2000 to 2022 (23 full years): With a consistent five-day-a-week schedule (an average of approximately 250 issues per year), results in an estimated 5,750 issues over 23 years.\n",
    "\n",
    "# = 5,834\n",
    "\n",
    "\n",
    "# Sovetskaia Rossiia\n",
    "\n",
    "# Daily Period (2 years): Jan 1990 to the August 1991 (Inc late 1991 to the October 1993 ban). Published roughly six times a week. Approximately 312 issues per year, i.e., 624 issues.\n",
    "\n",
    "# Suspensions: The two suspensions combined would account for a loss of approximately 80-100 issues.\n",
    "\n",
    "# Tri-weekly Period (29 years): The remaining period approximately 156 issues per year (3 issues/week) 4,524 issues.\n",
    "\n",
    "# = 5,148\n",
    "\n",
    "\n",
    "# Novaia Gazeta\n",
    "\n",
    "# 1993-1999: Mix (weekly and bi-weekly).\n",
    "\n",
    "# 2000 - 2022: Tri-weekly schedule.\n",
    "\n",
    "# forced to suspend publication on March 28, 2022.\n",
    "\n",
    "# = (Roughly) 3,800\n",
    "\n",
    "\n",
    "# Время MH (Vremya MN)\n",
    "\n",
    "# Founded in 1997 and ceased publication on February 27, 2004.\n",
    "\n",
    "# Published daily.\n",
    "\n",
    "# = (Roughly) 1,700\n",
    "\n",
    "\n",
    "# Literaturnaia Gazeta\n",
    "\n",
    "# Consistent weekly publication schedule from 1990 to the end of 2022.\n",
    "\n",
    "# = 1,650\n",
    "\n",
    "\n",
    "# Trud (Труд)\n",
    "\n",
    "# Daily publication (1990 – 2007).\n",
    "\n",
    "# Weekly Publication (2008 – 2022).\n",
    "\n",
    "# = 5,250\n",
    "\n",
    "\n",
    "# Pravda\n",
    "\n",
    "# Daily (Jan 1990 - Aug 1991) around 500 issues.\n",
    "\n",
    "# Irregular Period (Late 1991 - 1996) Publication was sporadic, making any figure a rough guess.\n",
    "\n",
    "# Tri-weekly (1997 - 2022) 3 issues/week.\n",
    "\n",
    "# = 4,500 to 5,500.\n",
    "\n",
    "\n",
    "# Slovo\n",
    "\n",
    "# 7 years (from 1990 to end of 1996).\n",
    "\n",
    "# = 350\n",
    "\n",
    "\n",
    "# Obshchaya Gazeta (Общая газета)\n",
    "\n",
    "# 9 years (1993 to 2002)\n",
    "\n",
    "# Weekly Publication\n",
    "\n",
    "# = (Roughly) 450\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the cleaned CSV\n",
    "# df = pd.read_csv('soros_with_text_translated_fixed_all_scores_cleaned_filtered.csv', dtype={'ArticleID': str})\n",
    "\n",
    "# # df columns\n",
    "# Index(['ArticleLink', 'ArticleTitle', 'Publication', 'Information',\n",
    "#        'Date collected', 'ArticleID', 'ArticleText', 'ArticleTextEnglish',\n",
    "#        'score', 'probability'],\n",
    "#       dtype='object')\n",
    "\n",
    "# # Score (Postive) \n",
    "# # probability  (0.756675)\n",
    "# # Publication = Kommersant\n",
    "\n",
    "# # Custom color palette\n",
    "# custom_palette = {'Kommersant': '#', 'literaturnaia_gazeta': '#'}\n",
    "\n",
    "\n",
    "# # Publications\n",
    "# # pubs = [\n",
    "# #     \"Kommersant\",\n",
    "# #     \"Literaturnaia gazeta\",\n",
    "# #     \"Nezavisimaia gazeta\",\n",
    "# #     \"Novaia gazeta\",\n",
    "# #     \"Pravda\",\n",
    "# #     \"Slovo\",\n",
    "# #     \"Sovetskaia Rossiia\",\n",
    "# #     \"Trud\",\n",
    "# #     \"Vedomosti\",\n",
    "# #     \"Время МН\",\n",
    "# #     \"Общая газета\"\n",
    "# # ]\n",
    "\n",
    "# # Tasks\n",
    "# # Find out how many articles (ArticleID) for each (Publication) are positive vs negative vs NaN\n",
    "# # Investigate publications over each year\n",
    "# # Find highest yearly scores \n",
    "# # Investigate publications over each month\n",
    "# # Find highest monthly scores \n",
    "# # Compare increase and decrease soros records line graph\n",
    "# # Find is there an increase a decrease in soros records for periods of time for two or more publications\n",
    "# # Top 50 words by TF-IDF score each publication\n",
    "# # Word Cloud for each\n",
    "# # Heat map \n",
    "# # SVD scatterplot of content and soros sentences for each publication\n",
    "# # SVD scatterplot of content and soros between publications\n",
    "# # KMeans clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned CSV\n",
    "# df = pd.read_csv('soros_with_text_translated_fixed_all_scores_cleaned.csv', dtype={'ArticleID': str})\n",
    "\n",
    "# Load the cleaned CSV\n",
    "df = pd.read_csv('soros_filtered_scores.csv', dtype={'ArticleID': str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Case-insensitive match for 'soros' in ArticleTextEnglish\n",
    "# mask = df['ArticleTextEnglish'].str.contains(r'soros', case=False, na=False)\n",
    "\n",
    "# # View matching rows\n",
    "# matching_rows = df[mask]\n",
    "\n",
    "# # Optionally, view how many matches\n",
    "# print(f\"Found {len(matching_rows)} rows with 'soros' or 'Soros'.\")\n",
    "\n",
    "# # Display a few results\n",
    "# with pd.option_context('display.max_columns', None, 'display.max_colwidth', None):\n",
    "#     print(matching_rows.head())\n",
    "\n",
    "# Filter for the specific ArticleID\n",
    "row = df[df['ArticleID'] == '18922019']  # or 9184404 if dtype is int\n",
    "\n",
    "# Check if the row exists and print ArticleID and ArticleTextEnglish\n",
    "if not row.empty:\n",
    "    article_id = row['ArticleID'].iloc[0]\n",
    "    article_text = row['ArticleTextEnglish'].iloc[0]\n",
    "    \n",
    "    print(f\"ArticleID: {article_id}\\n\")\n",
    "    print(\"ArticleTextEnglish:\\n\")\n",
    "    print(article_text)\n",
    "else:\n",
    "    print(\"✗ ArticleID 9184404 not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned CSV\n",
    "# df = pd.read_csv('soros_with_text_translated_fixed_all_scores_cleaned.csv', dtype={'ArticleID': str})\n",
    "\n",
    "# Load the cleaned CSV\n",
    "df_small = pd.read_csv('soros_filtered_scores_short.csv', dtype={'ArticleID': str})\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create date and time\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# # Example: load your dataframe (already done)\n",
    "# # df = pd.read_csv('soros_with_text_translated_fixed_all_scores_cleaned.csv', dtype={'ArticleID': str})\n",
    "\n",
    "# # Step 1: Extract first date in YYYY-MM-DD format from the 'Information' column\n",
    "# df['ExtractedDate'] = df['Information'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})')\n",
    "\n",
    "# # Step 2: Convert to datetime\n",
    "# df['ExtractedDate'] = pd.to_datetime(df['ExtractedDate'], errors='coerce')\n",
    "\n",
    "# # Step 3: Create Year and Month columns\n",
    "# df['Year'] = df['ExtractedDate'].dt.year\n",
    "# df['Month'] = df['ExtractedDate'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "View data\n",
    "Count how many negative/neutral/positive per publication\n",
    "positive, negative, NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter dataframe for only publications targeted\n",
    "\n",
    "# # 1) define your target pubs\n",
    "# pubs = [\n",
    "#     \"Kommersant\", \"Literaturnaia gazeta\", \"Nezavisimaia gazeta\",\n",
    "#     \"Novaia gazeta\", \"Pravda\", \"Slovo\", \"Sovetskaia Rossiia\",\n",
    "#     \"Trud\", \"Vedomosti\", \"Время МН\", \"Общая газета\"\n",
    "# ]\n",
    "\n",
    "# # 2) filter out all rows whose Publicatidon is NOT in your list\n",
    "# df = df[df['Publication'].isin(pubs)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Move score and probability to end\n",
    "# import pandas as pd\n",
    "\n",
    "# # 1. Load your CSV\n",
    "# df = pd.read_csv('soros_with_text_translated_fixed_all_scores_cleaned_filtered.csv')\n",
    "\n",
    "# # 2. Build a new column order: everything except 'score'/'probability', then those two at the end\n",
    "# cols = [c for c in df.columns if c not in ('score','probability')] + ['score','probability']\n",
    "\n",
    "# # 3. Reorder and overwrite df\n",
    "# df = df[cols]\n",
    "\n",
    "# # 4. (Optional) Preview\n",
    "# df.head()\n",
    "\n",
    "# # 5. Save back out\n",
    "# df.to_csv('soros_with_text_reordered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the filtered DataFrame\n",
    "# df.to_csv('soros_with_text_translated_fixed_all_cleaned_filtered_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#              non_null  null\n",
    "#score             835  2353\n",
    "#probability       835  2353\n",
    "# df = pd.read_csv('soros_with_text_reordered.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#              non_null  null\n",
    "#score            2380   808\n",
    "#probability      2380   808\n",
    "#df = pd.read_csv('soros_filtered_scores.csv', dtype={'ArticleID': str})\n",
    "\n",
    "\n",
    "#             non_null  null\n",
    "#score            2528   660\n",
    "#probability      2528   660\n",
    "df = pd.read_csv('soros_filtered_scores_updated.csv', dtype={'ArticleID': str})\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.read_csv('soros_filtered_scores_updated.csv', dtype={'ArticleID': str})\n",
    "# list out the columns you care about\n",
    "cols = ['score','probability']\n",
    "# cols = ['score','score.1','probability','probability.1']\n",
    "\n",
    "# build a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'non_null': df[cols].notna().sum(),\n",
    "    'null'    : df[cols].isna().sum()\n",
    "})\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#              non_null  null\n",
    "#score            2380   808\n",
    "#probability      2380   808\n",
    "df = pd.read_csv('rows_with_all_nulls.csv', dtype={'ArticleID': str})\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list out the columns you care about\n",
    "cols = ['score','probability']\n",
    "# cols = ['score','score.1','probability','probability.1']\n",
    "\n",
    "# build a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'non_null': df[cols].notna().sum(),\n",
    "    'null'    : df[cols].isna().sum()\n",
    "})\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#              non_null  null\n",
    "df = pd.read_csv('soros_filtered_scores_rerun_updated.csv', dtype={'ArticleID': str})\n",
    "\n",
    "df.columns\n",
    "\n",
    "# list out the columns you care about\n",
    "cols = ['score','probability']\n",
    "\n",
    "# build a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'non_null': df[cols].notna().sum(),\n",
    "    'null'    : df[cols].isna().sum()\n",
    "})\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine both datasets\n",
    "# import pandas as pd\n",
    "\n",
    "# # 1. Load both CSVs\n",
    "# df_master = pd.read_csv('soros_filtered_scores.csv', dtype={'ArticleID': str})\n",
    "# df_nulls  = pd.read_csv('rows_with_all_nulls.csv',   dtype={'ArticleID': str})\n",
    "\n",
    "# # 2. Keep only the rows in df_nulls that actually have a score to give\n",
    "# df_nulls_nonnull = df_nulls[df_nulls['score'].notna()]\n",
    "\n",
    "# # 3. If there happen to be duplicate ArticleIDs in df_nulls, drop extras\n",
    "# df_nulls_unique = df_nulls_nonnull.drop_duplicates(subset='ArticleID', keep='first')\n",
    "\n",
    "# # 4. Build lookup maps\n",
    "# score_map = df_nulls_unique.set_index('ArticleID')['score']\n",
    "# prob_map  = df_nulls_unique.set_index('ArticleID')['probability']\n",
    "\n",
    "# # 5. Fill in missing values in the master DataFrame\n",
    "# df_master['score']       = df_master['score'].fillna(df_master['ArticleID'].map(score_map))\n",
    "# df_master['probability'] = df_master['probability'].fillna(df_master['ArticleID'].map(prob_map))\n",
    "\n",
    "# # 6. Save out the updated master\n",
    "# df_master.to_csv('soros_filtered_scores_updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine both datasets\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load both CSVs\n",
    "df_master = pd.read_csv('soros_filtered_scores_updated_null_score_run.csv', dtype={'ArticleID': str})\n",
    "# df_nulls  = pd.read_csv('soros_filtered_scores_updated_null_score_run.csv',   dtype={'ArticleID': str})\n",
    "df_nulls  = pd.read_csv('soros_filtered_scores_rerun_updated.csv',   dtype={'ArticleID': str})\n",
    "\n",
    "# 2. Keep only the rows in df_nulls that actually have a score to give\n",
    "df_nulls_nonnull = df_nulls[df_nulls['score'].notna()]\n",
    "\n",
    "# 3. If there happen to be duplicate ArticleIDs in df_nulls, drop extras\n",
    "df_nulls_unique = df_nulls_nonnull.drop_duplicates(subset='ArticleID', keep='first')\n",
    "\n",
    "# 4. Build lookup maps\n",
    "score_map = df_nulls_unique.set_index('ArticleID')['score']\n",
    "prob_map  = df_nulls_unique.set_index('ArticleID')['probability']\n",
    "\n",
    "# 5. Fill in missing values in the master DataFrame\n",
    "df_master['score']       = df_master['score'].fillna(df_master['ArticleID'].map(score_map))\n",
    "df_master['probability'] = df_master['probability'].fillna(df_master['ArticleID'].map(prob_map))\n",
    "\n",
    "# 6. Save out the updated master\n",
    "df_master.to_csv('soros_filtered_scores_updated_again.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both datasets\n",
    "import pandas as pd\n",
    "\n",
    "# # 1. Load both CSVs\n",
    "# df_master = pd.read_csv('soros_filtered_scores_updated.csv', dtype={'ArticleID': str})\n",
    "# # df_nulls  = pd.read_csv('soros_filtered_scores_updated_null_score_run.csv',   dtype={'ArticleID': str})\n",
    "# df_nulls  = pd.read_csv('soros_filtered_scores_updated_again.csv',   dtype={'ArticleID': str})\n",
    "\n",
    "# # 2. Keep only the rows in df_nulls that actually have a score to give\n",
    "# df_nulls_nonnull = df_nulls[df_nulls['score'].notna()]\n",
    "\n",
    "# # 3. If there happen to be duplicate ArticleIDs in df_nulls, drop extras\n",
    "# df_nulls_unique = df_nulls_nonnull.drop_duplicates(subset='ArticleID', keep='first')\n",
    "\n",
    "# # 4. Build lookup maps\n",
    "# score_map = df_nulls_unique.set_index('ArticleID')['score']\n",
    "# prob_map  = df_nulls_unique.set_index('ArticleID')['probability']\n",
    "\n",
    "# # 5. Fill in missing values in the master DataFrame\n",
    "# df_master['score']       = df_master['score'].fillna(df_master['ArticleID'].map(score_map))\n",
    "# df_master['probability'] = df_master['probability'].fillna(df_master['ArticleID'].map(prob_map))\n",
    "\n",
    "# 6. Save out the updated master\n",
    "df_master.to_csv('soros_filtered_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#              non_null  null\n",
    "df = pd.read_csv('soros_filtered_scores.csv', dtype={'ArticleID': str})\n",
    "\n",
    "df.columns\n",
    "\n",
    "# list out the columns you care about\n",
    "cols = ['score','probability']\n",
    "\n",
    "# build a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'non_null': df[cols].notna().sum(),\n",
    "    'null'    : df[cols].isna().sum()\n",
    "})\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save nulls\n",
    "# Create .csv with null values 230\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load your master CSV\n",
    "df = pd.read_csv('soros_filtered_scores.csv', dtype={'ArticleID': str})\n",
    "\n",
    "# 2a. If you only want rows where *score* is null:\n",
    "df_null_score = df[df['score'].isna()]\n",
    "\n",
    "# 3. Save to a new CSV\n",
    "df_null_score.to_csv('soros_filtered_no_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list out the columns you care about\n",
    "cols = ['score','probability']\n",
    "# cols = ['score','score.1','probability','probability.1']\n",
    "\n",
    "# build a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'non_null': df_master[cols].notna().sum(),\n",
    "    'null'    : df_master[cols].isna().sum()\n",
    "})\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create .csv with null values 660\n",
    "# import pandas as pd\n",
    "\n",
    "# # 1. Load your master CSV\n",
    "# df = pd.read_csv('soros_filtered_scores.csv', dtype={'ArticleID': str})\n",
    "\n",
    "# # 2a. If you only want rows where *score* is null:\n",
    "# df_null_score = df[df['score'].isna()]\n",
    "\n",
    "# # 3. Save to a new CSV\n",
    "# df_null_score.to_csv('soros_filtered_scores_updated_null_score.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#              non_null  null\n",
    "\n",
    "df = pd.read_csv('soros_filtered_no_scores.csv', dtype={'ArticleID': str})\n",
    "df.columns\n",
    "\n",
    "\n",
    "# list out the columns you care about\n",
    "cols = ['score','probability']\n",
    "# cols = ['score','score.1','probability','probability.1']\n",
    "\n",
    "# build a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'non_null': df[cols].notna().sum(),\n",
    "    'null'    : df[cols].isna().sum()\n",
    "})\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# soros_filtered_scores_updated_null_score_run\n",
    "df = pd.read_csv('soros_filtered_scores_updated_null_score_run.csv', dtype={'ArticleID': str})\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the specific ArticleID\n",
    "row = df[df['ArticleID'] == '24861986']  # ensure dtype is str, or remove quotes if it's int\n",
    "\n",
    "# Print all columns for the row\n",
    "if not row.empty:\n",
    "    with pd.option_context('display.max_columns', None, 'display.max_colwidth', None):\n",
    "        print(row.to_string(index=False))\n",
    "else:\n",
    "    print(\"✗ ArticleID 24861986 not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop \n",
    "# # df.drop(['score.1', 'probability.1'], axis=1, inplace=True)\n",
    "# df.to_csv('soros_filtered_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # list out the columns you care about\n",
    "# cols = ['score','probability']\n",
    "# # cols = ['score','score.1','probability','probability.1']\n",
    "\n",
    "# # build a summary DataFrame\n",
    "# summary = pd.DataFrame({\n",
    "#     'non_null': df[cols].notna().sum(),\n",
    "#     'null'    : df[cols].isna().sum()\n",
    "# })\n",
    "\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export Null to reclassify\n",
    "# cols = ['score','probability']\n",
    "\n",
    "# # grab every row where any of those cols is null, then copy it\n",
    "# null_df = df[df[cols].isna().any(axis=1)].copy()\n",
    "\n",
    "# # now you can safely modify or export null_df without altering df\n",
    "# null_df.to_csv('rows_with_null_scores.csv', index=False)\n",
    "# all_null_df = df[df[cols].isna().all(axis=1)].copy()\n",
    "# all_null_df.to_csv('rows_with_all_nulls.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Check if no_mention has any Soros in ArticleTextEnglish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # 1. Load your master CSV\n",
    "df = pd.read_csv('soros_filtered_scores.csv', dtype={'ArticleID': str})\n",
    "\n",
    "# 1) Select only the rows where score is NaN\n",
    "no_mention = df[df['score'].isna()]\n",
    "\n",
    "# 2) Within those, filter for “soros” in the English text\n",
    "mask = no_mention['ArticleTextEnglish']\\\n",
    "           .str.contains('soros', case=False, na=False)\n",
    "soros_mentions = no_mention[mask]\n",
    "\n",
    "# 3) Print how many slipped through unclassified\n",
    "print(f\"Found {len(soros_mentions)} no_mention articles containing 'soros'.\")\n",
    "\n",
    "# 4) Inspect them\n",
    "soros_mentions[\n",
    "    ['ArticleLink','ArticleTitle', 'ArticleID', 'Publication','ArticleTextEnglish']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    " Publications Over Each Year (Line Plot Ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Top Yearly Scores per Publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Compare Trends Line Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
